<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>&lt; Tensorflow &gt;How dose TensorFlow do Quant Aware Training? - Billy&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Billy&#039;s Blog"><meta name="msapplication-TileImage" content="/img/1.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Billy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Let firstly simplify the Quant process in TFOverview1S_a1(q_a1 + Z_a1) &amp;#x3D; S_w1(q_w1 + Z_w1) * S_a0(q_a0 + Z_a0)  q_a1: Quanted activation value in layer 1 S_a1, Z_a1:  Estimated scale and zero poi"><meta property="og:type" content="blog"><meta property="og:title" content="&lt; Tensorflow &gt;How dose TensorFlow do Quant Aware Training?"><meta property="og:url" content="https://zhengtq.github.io/2019/03/25/tf-quant/"><meta property="og:site_name" content="Billy&#039;s Blog"><meta property="og:description" content="Let firstly simplify the Quant process in TFOverview1S_a1(q_a1 + Z_a1) &amp;#x3D; S_w1(q_w1 + Z_w1) * S_a0(q_a0 + Z_a0)  q_a1: Quanted activation value in layer 1 S_a1, Z_a1:  Estimated scale and zero poi"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhengtq.github.io/img/og_image.png"><meta property="article:published_time" content="2019-03-25T09:22:38.000Z"><meta property="article:modified_time" content="2021-03-12T06:44:17.371Z"><meta property="article:author" content="Billy"><meta property="article:tag" content="Deeplearning"><meta property="article:tag" content="Tensorflow"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhengtq.github.io/2019/03/25/tf-quant/"},"headline":"< Tensorflow >How dose TensorFlow do Quant Aware Training?","image":["https://zhengtq.github.io/img/og_image.png"],"datePublished":"2019-03-25T09:22:38.000Z","dateModified":"2021-03-12T06:44:17.371Z","author":{"@type":"Person","name":"Billy"},"description":"Let firstly simplify the Quant process in TFOverview1S_a1(q_a1 + Z_a1) &#x3D; S_w1(q_w1 + Z_w1) * S_a0(q_a0 + Z_a0)  q_a1: Quanted activation value in layer 1 S_a1, Z_a1:  Estimated scale and zero poi"}</script><link rel="canonical" href="https://zhengtq.github.io/2019/03/25/tf-quant/"><link rel="icon" href="/img/1.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-25T09:22:38.000Z" title="3/25/2019, 5:22:38 PM">2019-03-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-12T06:44:17.371Z" title="3/12/2021, 2:44:17 PM">2021-03-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Deeplearning/">Deeplearning</a></span><span class="level-item">6 分钟 read (About 973 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">&lt; Tensorflow &gt;How dose TensorFlow do Quant Aware Training?</h1><div class="content"><h1 id="Let-firstly-simplify-the-Quant-process-in-TF"><a href="#Let-firstly-simplify-the-Quant-process-in-TF" class="headerlink" title="Let firstly simplify the Quant process in TF"></a>Let firstly simplify the Quant process in TF</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">S_a1(q_a1 + Z_a1) &#x3D; S_w1(q_w1 + Z_w1) * S_a0(q_a0 + Z_a0)</span><br></pre></td></tr></table></figure>
<ul>
<li>q_a1: Quanted activation value in layer 1</li>
<li><p>S_a1, Z_a1:  <strong>Estimated</strong> scale and zero point in layer 1</p>
</li>
<li><p>q_w1: Quanted weight in layer 1</p>
</li>
<li><p>S_w1, Z_w1:  <strong>Statistical</strong> scale and zero point in layer 1</p>
</li>
<li><p>q_a0: Quanted activation value in layer 0</p>
</li>
<li>S_a0, Z_a0:  <strong>Estimated</strong> scale and zero point in layer 0</li>
</ul>
<p>As we can see, in order to compute q_a1(Quanted activation value in layer 1), we have to get S_w1, Z_w1, S_a0, Z_a0, q_a1, Z_a1. To get S_w1/Z_w1 is simple, we can get the Statistical maximum of the weights in each layer we want. The only tricky thing is how to get S_a1/Z_a1/S_a0/Z_a0, which have to be estimated from the training data.<br><span id="more"></span></p>
<h3 id="Why-we-have-to-get-the-estimation-of-S-Z-after-activation-instead-of-before"><a href="#Why-we-have-to-get-the-estimation-of-S-Z-after-activation-instead-of-before" class="headerlink" title="Why we have to get the estimation of S/Z after activation instead of before?"></a>Why we have to get the estimation of S/Z after activation instead of before?</h3><p>Of course we can estimate the S/Z before activation. And then we activate(Relu) the quantized value. However, there is one drawbacks:</p>
<p>Estimate S/Z before activation<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv -&gt;  (before activation)estimate S&#x2F;Z&#x3D;[0-&gt;255] -&gt; truncation（relu）&#x3D; [Z-&gt;255] (if relu6 : [Z-&gt;X])</span><br></pre></td></tr></table></figure></p>
<p>Estimate S/Z after activation<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv -&gt;  (after activation)estimate S&#x2F;Z&#x3D;[-A-&gt;A] -&gt; truncation（relu）&#x3D; [0-&gt;255](if relu6: [0-&gt;255])</span><br></pre></td></tr></table></figure><br>As we can see,  after activation, the range of activation value for estimating S/Z is always 0 to 255. However, when we estimate S/Z before activation, the range of  activation value for estimating S/Z is narrow than 0 to 255. So if we estimate S/Z before activation, the quantized activated is compressed even wore  which could lead to accuracy loss.</p>
<h1 id="Quantization-aware-training-in-Tensorflow"><a href="#Quantization-aware-training-in-Tensorflow" class="headerlink" title="Quantization aware training in Tensorflow"></a>Quantization aware training in Tensorflow</h1><p>You can either train your quantized model by restoring a ever trained floating point model or from scratch. In any cases, you have to create a quantization training graph first.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP)</span><br></pre></td></tr></table></figure><br>The <em>DELAY_STEP</em> means the number of steps that you want your normal floating point training sustain. So after the DELAY_STEP of normal training, the quantization aware training would be started.  </p>
<p>If you use multi-GPU to training your network, you have to create a new quantization graph on every GPU card. Just like these code below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope()):</span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="built_in">len</span>(GPU_NUM_ID)):</span><br><span class="line">          <span class="keyword">with</span> tf.device(<span class="string">&#x27;/gpu:%d&#x27;</span> % GPU_NUM_ID[i]):</span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;%s_%d&#x27;</span> % (<span class="string">&#x27;cnn_mg&#x27;</span>, i)) <span class="keyword">as</span> scope:</span><br><span class="line">                            images, abels = load_batch_images()           </span><br><span class="line">                            logits, out_data = net.inference(images, reuse=tf.AUTO_REUSE,  num_classes=LABEL_NUM)</span><br><span class="line">                            <span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):</span><br><span class="line">                                tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP)</span><br><span class="line">                            loss = conpute_loss(labels, logits)</span><br><span class="line">                            tf.get_variable_scope().reuse_variables()</span><br><span class="line">                            grads = optimizer.compute_gradients(loss_total_sep)</span><br><span class="line">                            tower_grads.append(grads)</span><br></pre></td></tr></table></figure>
<p>One thing I have to mention is that the quantized aware training process is fake training.</p>
<p>Fake training means that during the forward step,  the training graph just simulate the integer multiply by using corresponding floating point multiply.</p>
<p>The word ‘Corresponding’ means that the simulated float weights are the inverse quantization value of the corresponding fixed integer. </p>
<p>So the forward result may be slightly different from the actual quantization computed result.</p>
<h1 id="Save-Frozen-Convert-and-Test"><a href="#Save-Frozen-Convert-and-Test" class="headerlink" title="Save, Frozen, Convert and Test"></a>Save, Frozen, Convert and Test</h1><h2 id="Save"><a href="#Save" class="headerlink" title="Save"></a>Save</h2><p>When finishing quantization aware training, you have to save your trained quantized model.</p>
<p>To save your quantized model, you have to create a quantized evaluation graph by using the following code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">g = tf.get_default_graph()</span><br><span class="line">tf.contrib.quantize.create_eval_graph(input_graph=g)</span><br></pre></td></tr></table></figure>
<p>Then just get the graph and save it.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./your_quantized_graph.pb&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">       f.write(<span class="built_in">str</span>(g.as_graph_def()))\</span><br></pre></td></tr></table></figure></p>
<h2 id="Frozen"><a href="#Frozen" class="headerlink" title="Frozen"></a>Frozen</h2><p>To make your model more compact, you can froze your model. Frozen a model means that getting rid of useless operations and fusing redundant operations. To froze your graph, you can use the standard frozen tool.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bazel build tensorflow/python/tools:freeze_graph &amp;&amp; \</span><br><span class="line">bazel-bin/tensorflow/python/tools/freeze_graph \</span><br><span class="line">--input_graph=some_graph_def.pb \</span><br><span class="line">--input_checkpoint=model.ckpt-8361242 \</span><br><span class="line">--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax</span><br></pre></td></tr></table></figure></p>
<h2 id="Convert"><a href="#Convert" class="headerlink" title="Convert"></a>Convert</h2><p>The next step is to convert your frozen graph to tflite for future deploy.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">path_to_frozen_graphdef_pb = <span class="string">&#x27;./your_frozen_graph.pb&#x27;</span></span><br><span class="line">input_shapes = &#123;<span class="string">&#x27;validate_input/imgs&#x27;</span>:[<span class="number">1</span>,<span class="number">320</span>,<span class="number">320</span>,<span class="number">3</span>]&#125;</span><br><span class="line">(tf_verion&gt;<span class="number">1.11</span>)converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(path_to_frozen_graphdef_pb, [<span class="string">&#x27;validate_input/imgs&#x27;</span>], [<span class="string">&#x27;output_node&#x27;</span>])</span><br><span class="line">(tf_version&lt;=<span class="number">1.11</span>)converter = tf.contrib.lite.TocoConverter.from_frozen_graph(path_to_frozen_graphdef_pb, [<span class="string">&#x27;validate_input/imgs&#x27;</span>], [<span class="string">&#x27;output_node&#x27;</span>])</span><br><span class="line">converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8</span><br><span class="line">converter.quantized_input_stats = &#123;<span class="string">&#x27;validate_input/imgs&#x27;</span>:(<span class="number">0.</span>,<span class="number">1.</span>)&#125;</span><br><span class="line">converter.allow_custom_ops = <span class="literal">True</span></span><br><span class="line">converter.default_ranges_stats = (<span class="number">0</span>,<span class="number">255</span>)</span><br><span class="line">converter.post_training_quantize = <span class="literal">True</span></span><br><span class="line">tflite_model = converter.convert()</span><br><span class="line"><span class="built_in">open</span>(<span class="string">&quot;sfnv2.tflite&quot;</span>, <span class="string">&quot;wb&quot;</span>).write(tflite_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><p>Finally, your can test your converted tflite. By using the following code, you can test your quantized model:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">interpreter = tf.contrib.lite.Interpreter(model_path=<span class="string">&quot;your.tflite&quot;</span>) </span><br><span class="line">interpreter.allocate_tensors() </span><br><span class="line">input_details = interpreter.get_input_details() </span><br><span class="line">output_details = interpreter.get_output_details() </span><br><span class="line">interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>], batch_validate_img)</span><br><span class="line">interpreter.invoke()</span><br><span class="line">score = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">score = score[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">zero_point = xxx</span><br><span class="line">scale = xxx</span><br><span class="line">reverse_socre = scale  * (score - zero_point)</span><br></pre></td></tr></table></figure><br>One thing to mention is that the final score you get is a fixed point integer value. </p>
<p>You have to convert the fixed point integer value to the corresponding float value. </p>
<p>In order to do that, you have to check the corresponding zero point and scale in the corresponding output layer and then you can transfer the fixed point integer value to get the final float value.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>&lt; Tensorflow &gt;How dose TensorFlow do Quant Aware Training?</p><p><a href="https://zhengtq.github.io/2019/03/25/tf-quant/">https://zhengtq.github.io/2019/03/25/tf-quant/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Billy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2019-03-25</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-03-12</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Deeplearning/">Deeplearning</a><a class="link-muted mr-2" rel="tag" href="/tags/Tensorflow/">Tensorflow</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/03/29/network-depthwise/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">&lt; Network &gt; Understanding DepthWiseConv</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/03/16/antispoof-multi-task-learn/"><span class="level-item">&lt; Antispoofing &gt; Multi-Task-Learning in Face Antispoofing</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "BGzxkVpRtr5PoQUppRDqiC1V-gzGzoHsz",
            appKey: "K9tU9mVpjknHh8SNOWrqXqDV",
            
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "zh-cn",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Billy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>