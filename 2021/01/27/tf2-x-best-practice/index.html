<!DOCTYPE html>
<html lang="zh-cn">
<head><meta name="generator" content="Hexo 3.8.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="Billy">



<meta name="description" content="开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．">
<meta name="keywords" content="Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="&lt; Tensorflow &gt;Tensorflow2.4 最佳实践">
<meta property="og:url" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/index.html">
<meta property="og:site_name" content="Billy&#39;s Blog">
<meta property="og:description" content="开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．">
<meta property="og:locale" content="zh-cn">
<meta property="og:image" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/before.png">
<meta property="og:image" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/before1.png">
<meta property="og:updated_time" content="2021-03-11T02:42:09.046Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="&lt; Tensorflow &gt;Tensorflow2.4 最佳实践">
<meta name="twitter:description" content="开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．">
<meta name="twitter:image" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/before.png">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Billy&#39;s Blog" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>&lt; Tensorflow &gt;Tensorflow2.4 最佳实践 | Billy&#39;s Blog</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Billy</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Make It Simple</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Menu</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Antispoofing/">Antispoofing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-C/">C/C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DEMO/">DEMO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deeplearning/">Deeplearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NCNN/">NCNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Other/">Other</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tensorflow/">Tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Billy</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Billy</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Make It Simple</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="Tags" friends="Friends" about="About Me">
</nav>
      <div class="body-wrap"><article id="post-tf2-x-best-practice" class="article article-type-post" itemscope="" itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2021/01/27/tf2-x-best-practice/" class="article-date">
      <time datetime="2021-01-27T00:38:00.000Z" itemprop="datePublished">2021-01-27</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      &lt; Tensorflow &gt;Tensorflow2.4 最佳实践
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/Tensorflow/">Tensorflow</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/">Tensorflow</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><p>最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．</p>
<a id="more"></a>
<p>跑了一个官方给的cifar的例子，效果还行．于是就打算在自己的数据上跑跑看效果，在这个过程中，发现官方给你例子还远远达不到训练速度最优化的程度．于是就把官方例子就改了一下，最终达到了一个满意的训练速度，本篇就是记录一下tf2.4下的训练性能调优全过程．最终调优后的代码会共享出来．</p>
<h1 id="单卡到多卡"><a href="#单卡到多卡" class="headerlink" title="单卡到多卡"></a>单卡到多卡</h1><p>官方给的vit(Vision Transformer)的例子是基于单卡的，但是现在训练大型网络已经离不开多卡，于是把官方例子改成多卡训练的版本（这里主要说明一下单机多卡的设置）．</p>
<p><strong>首先根据你手上卡的数量来建立一个strategy的对象</strong>．</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">physical_devices = tf.config.list_physical_devices(<span class="string">'GPU'</span>)          </span><br><span class="line"><span class="keyword">for</span> ind, item <span class="keyword">in</span> enumerate(physical_devices):                      </span><br><span class="line">    tf.config.experimental.set_memory_growth(item, <span class="keyword">True</span>)           </span><br><span class="line">                                                                   </span><br><span class="line">TRAIN_GPUS = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]                                          </span><br><span class="line">devices = [<span class="string">"/gpu:&#123;&#125;"</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> TRAIN_GPUS]             </span><br><span class="line">strategy = tf.distribute.MirroredStrategy(devices)</span><br></pre></td></tr></table></figure>
<p>这里tf.config.experimental.set_memory_growth的作用是限制显存的使用．</p>
<p><strong>然后我们要把数据做一个副本化的包装：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)   </span><br><span class="line">test_dist_dataset = strategy.experimental_distribute_dataset(test_daset)</span><br></pre></td></tr></table></figure>
<p><strong>然后我们需要对train_step和test_step做一个封装：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope()</span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">distributed_train_step</span><span class="params">(dataset_inputs)</span>:</span>                           </span><br><span class="line">           per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))                           </span><br><span class="line">           <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=<span class="keyword">None</span>)               </span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">distributed_test_step</span><span class="params">(dataset_inputs)</span>:</span>                           </span><br><span class="line">           <span class="keyword">return</span> strategy.run(test_step, args=(dataset_inputs,))</span><br></pre></td></tr></table></figure>
<p><strong>最后在train_step，test_step，和计算loss和accuracy加上strategy的scope</strong>（这里只是拿train_step和test_step举例）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope():                                                       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(inputs)</span>:</span>                                                  </span><br><span class="line">        images, labels = inputs                                              </span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:                                      </span><br><span class="line">            predictions = model(images, training=<span class="keyword">True</span>)                       </span><br><span class="line">            loss = compute_loss(labels, predictions)                         </span><br><span class="line">                                                                             </span><br><span class="line">        gradients = tape.gradient(loss, model.trainable_variables)           </span><br><span class="line">        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) </span><br><span class="line">        compute_acc(labels, predictions, train_accuracy)                     </span><br><span class="line">        <span class="keyword">return</span> loss                                                          </span><br><span class="line">                                                                             </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_step</span><span class="params">(inputs)</span>:</span>                                                   </span><br><span class="line">        images, labels = inputs                                              </span><br><span class="line">        predictions = model(images, training=<span class="keyword">False</span>)                          </span><br><span class="line">        compute_acc(labels, predictions, test_accuracy)</span><br></pre></td></tr></table></figure>
<p>经过了上述的步骤，成功的在tf2.4下，把单卡训练转换到了单机多卡训练．</p>
<h1 id="eager-mode-到-static-graph-mode"><a href="#eager-mode-到-static-graph-mode" class="headerlink" title="eager mode 到 static graph mode"></a>eager mode 到 static graph mode</h1><p>由于tf2.x模式的执行方式是急切执行（eager mode），eager mode的好处在在于方便debug，但是如果拿来训练就不太好了，因为eager mode会托慢速度．所以我们需要在调试好网络之后把执行模式切换为静态图的模式．而这个步骤非常简单，<strong>加上一个tf.function的修饰符就好</strong>．</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope():                                                   </span><br><span class="line"><span class="meta">    @tf.function                                                         </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">distributed_train_step</span><span class="params">(dataset_inputs)</span>:</span>                           </span><br><span class="line">        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))         </span><br><span class="line">        <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=<span class="keyword">None</span>)             </span><br><span class="line"><span class="meta">    @tf.function                                                         </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">distributed_test_step</span><span class="params">(dataset_inputs)</span>:</span>                           </span><br><span class="line">        <span class="keyword">return</span> strategy.run(test_step, args=(dataset_inputs,))</span><br></pre></td></tr></table></figure>
<h1 id="调整tf-data数据流顺序"><a href="#调整tf-data数据流顺序" class="headerlink" title="调整tf.data数据流顺序"></a>调整tf.data数据流顺序</h1><p>tf.data的数据处理顺序非常重要，改变顺序可能会极度的托慢训练速度．</p>
<p>这里，我的数据处理流程如下：</p>
<ol>
<li>读取所有图片的路径和对应的label．</li>
<li>把图片路径给parse成图片．</li>
</ol>
<p>流程其实很简单，但这里要涉及到几点：</p>
<ol>
<li>如何shuffle.</li>
<li>哪里设定epoch</li>
<li>哪里设定batch</li>
<li>哪里设定预取prefetch．</li>
</ol>
<p>先来上code，再解释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  image_roots, labels = generate_fileroots_labels(file_root)                   </span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((image_roots, labels))           </span><br><span class="line">  dataset = dataset.repeat(<span class="number">100</span>).shuffle(buffer_size=<span class="number">2000</span>) </span><br><span class="line"><span class="comment">#  dataset = dataset.map(_parse_data, num_parallel_calls=tf.data.experimental.AUTOTUNE) </span></span><br><span class="line">  dataset = dataset.map(_parse_data, num_parallel_calls=<span class="number">16</span>)                     </span><br><span class="line">  dataset = dataset.batch(batch_size)                                           </span><br><span class="line">  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure>
<p>首先，repeat表示需要把数据重复多少次，也就是设定的epoc，shuffle代表在多少的buffer中打乱数据．<strong>这两者需要放到map前面</strong>，因为在map前，数据流处理的都是图片路径和label的轻量化数据，这对于repeat和shuffle是有利的．</p>
<p>而batch和prefetch就需要放到map的后面．<strong>这里需要注意，要先设定batch，再prefetch（不然会慢）</strong>．</p>
<h1 id="大杀器－－tf-profiler"><a href="#大杀器－－tf-profiler" class="headerlink" title="大杀器－－tf.profiler"></a>大杀器－－tf.profiler</h1><p>当我们觉得已经把加速做到极致了之后，我们需要用tensorflow自带的性能检测工具tf.profiler来检查一些性能还有那些可以榨取的空间．</p>
<p>用tf.profiler之前，你需要先按照官方的教程<a href="https://github.com/tensorflow/profiler" target="_blank" rel="noopener">安装</a>．这里有个小坑，因为tf.profiler需要依赖libcupti这个库，而libcupti这个库不在cuda的主库目录里，而是在extras/CUPTI/lib64里面，这个需要注意．</p>
<p>然后在你的训练代码中，需要添加如下的code：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t_step, x <span class="keyword">in</span> enumerate(train_dist_dataset):</span><br><span class="line">    <span class="keyword">if</span> t_step == <span class="number">500</span>:</span><br><span class="line">          tf.profiler.experimental.start(<span class="string">'/tmp/'</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>))</span><br><span class="line">    <span class="keyword">if</span> t_step == <span class="number">600</span>:</span><br><span class="line">          tf.profiler.experimental.stop()</span><br><span class="line">    <span class="keyword">with</span> tf.profiler.experimental.Trace(<span class="string">'Train'</span>, step_num=t_step, _r=<span class="number">1</span>):</span><br><span class="line">        step_loss = distributed_train_step(x)</span><br></pre></td></tr></table></figure>
<p>这个代码段的意思是，在训练的第200步到第300步需要记录你的训练profile．</p>
<p>这里说明两点：</p>
<p>第一，<strong>我们不需要要整个训练过程都记录profile</strong>，因为记录profile仅仅是为了调优，只需要记录某些步的profile就可以提供你来调优即可．</p>
<p>第二，<strong>不从第０步就开始记录是因为我们需要先让训练达到稳定之后记录才会比较准确</strong>（示例中是从500步到600步开始记录profile）．</p>
<p>好了，我们看一下，都记录了一些什么东西：</p>
<p><img src="/2021/01/27/tf2-x-best-practice/before.png" alt=""></p>
<p>这里我们可以看到耗时主要在3个方面：</p>
<ol>
<li><strong>Kernel Launch Time</strong>  </li>
<li><strong>Host Compute Time</strong></li>
<li><strong>Device Compute Time</strong></li>
</ol>
<p>我们来一个一个解决，首先来解决kernel lanuch time，这个在右边的建议(Recommendation for Next Step)有说明：</p>
<blockquote>
<ul>
<li>14.3 % of the total step time sampled is spent on ‘Kernel Launch’. It could be due to CPU contention with tf.data. In this case, you may try to set the environment variable TF_GPU_THREAD_MODE=gpu_private.</li>
</ul>
</blockquote>
<p>也就是说可以通过设定TF_GPU_THREAD_MODE=gpu_private来解决．也就是说要在之前训练程序的前面加上下面一句命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TF_GPU_THREAD_MODE=gpu_private</span><br></pre></td></tr></table></figure>
<p>然而我这样执行之后发现作用好像不是很大，知道怎么解决的同学可以交流．</p>
<p>我们现在来解决第二个主要耗时的点，也就是Host Compute Time耗时过高．</p>
<p>我们知道，tensorflow的多卡策略是ps-worker的方式，这个ps可以认为是host，主要是负责更新参数和处理数据流，按理说这部分的耗时不应该很高才对．于是我往下看具体的host的耗时的页面：</p>
<p><img src="/2021/01/27/tf2-x-best-practice/before1.png" alt=""></p>
<p>我们看到，host的耗时，很大的程度和２个op有关：</p>
<ol>
<li>stridedSlide</li>
<li>cast</li>
</ol>
<p>其中stridedSlide占了绝对的大头，经过查阅资料发现，stridedSlide耗时比较高主要和tf.distribute.MirroredStrategy这个对象有关．</p>
<p>因为我在我的工程里面用到了tf.data.Dataset.from_tensor_slices这个对象，这个对象用到了stridedSlide这个操作，而tf.distribute.MirroredStrategy对stridedSlide的操作支持的不好．</p>
<p>网上的建议是把tf.distribute.MirroredStrategy换成tf.distribute.experimental.MultiWorkerMirroredStrategy．</p>
<p>也就是如下实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  TRAIN_GPUS = [0,1,2,3]</span></span><br><span class="line"><span class="comment">#  devices = ["/gpu:&#123;&#125;".format(i) for i in TRAIN_GPUS]</span></span><br><span class="line"><span class="comment">#  strategy = tf.distribute.MirroredStrategy(devices)</span></span><br><span class="line">tf.config.set_visible_devices(physical_devices[<span class="number">0</span>:<span class="number">8</span>], <span class="string">'GPU'</span>) </span><br><span class="line">strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()</span><br></pre></td></tr></table></figure>
<p>更换了之后，果然<strong>Host Compute Time</strong> 得到了下降．</p>
<p>关于cast这个操作，是因为我在dataset的parse_function里面用到了tf.cast的操作，我把这个操作放到了网络里面，这部分的耗时也消除了（其实是分给worker了）．</p>
<p>关于<strong>Device Compute Time</strong>这一部分，我发现这个部分主要的耗时用到了矩阵操作Einsum上，这一部分的操作也是MultiHeadAttention的主要操作，于是也就没有修改这个部分．</p>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>当然，说了这么多，不如大家来下面一行代码来的方便…..</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch as tf</span><br></pre></td></tr></table></figure>
<p>不过话说回来，tensorflow和pytorch不是二选一的问题，而是大家最好都会用，这样才能更好在深度学习里探(lian)索(dan)．</p>
<p>最后把示例代码放到<a href="https://github.com/Zhengtq/tensorflow2.x-best-practice" target="_blank" rel="noopener">这里</a>，大家自行取用．</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>Title:</span><a href="/2021/01/27/tf2-x-best-practice/">&lt; Tensorflow &gt;Tensorflow2.4 最佳实践</a></p>
        <p><span>Author:</span><a href="/" title="Back to Homepage">Billy</a></p>
        <p><span>Created:</span>2021-01-27, 08:38:00</p>
        <p><span>Updated:</span>2021-03-11, 10:42:09</p>
        <p>
            <span>Full URL:</span><a class="post-url" href="/2021/01/27/tf2-x-best-practice/" title="&lt; Tensorflow &gt;Tensorflow2.4 最佳实践">http://yoursite.com/2021/01/27/tf2-x-best-practice/</a>
            <span class="copy-path" data-clipboard-text="From http://yoursite.com/2021/01/27/tf2-x-best-practice/　　By Billy" title="Copy Article&#39;s Link &amp; Author"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>License:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"CC BY-NC-SA 4.0"</a> Keep Link &amp; Author if Distribute.
        </p>
    </div>



    <nav id="article-nav">
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2020/12/28/ncnn-lesson-start/">
                    &lt; NCNN-Lession-Start &gt;　Start
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#开始"><span class="toc-number">1.</span> <span class="toc-text">开始</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#单卡到多卡"><span class="toc-number">2.</span> <span class="toc-text">单卡到多卡</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#eager-mode-到-static-graph-mode"><span class="toc-number">3.</span> <span class="toc-text">eager mode 到 static graph mode</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#调整tf-data数据流顺序"><span class="toc-number">4.</span> <span class="toc-text">调整tf.data数据流顺序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#大杀器－－tf-profiler"><span class="toc-number">5.</span> <span class="toc-text">大杀器－－tf.profiler</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#最后"><span class="toc-number">6.</span> <span class="toc-text">最后</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="Hide" title="Show or Hide Table of Contents">

    <script>
        yiliaConfig.toc = ["Hide", "Show", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"< Tensorflow >Tensorflow2.4 最佳实践　| Billy's Blog　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/" title="Back to Homepage"><i class="fa fa-home"></i></a>
        

        <a title="Mini Archives"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2020/12/28/ncnn-lesson-start/" title="Next: &lt; NCNN-Lession-Start &gt;　Start">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/01/27/tf2-x-best-practice/">< Tensorflow >Tensorflow2.4 最佳实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/28/ncnn-lesson-start/">< NCNN-Lession-Start >　Start</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/21/ncnn-lesson-10/">< NCNN-Lession-10 >　Forward Net</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/21/ncnn-lesson-9/">< NCNN-Lession-9 >　Load Image</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/18/ncnn-lesson-8/">< NCNN-Lession-8 >　读取网络的权重信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/15/ncnn-lesson-7/">< NCNN-Lession-7 >　Mat类的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/11/ncnn-lesson6/">< NCNN-Lession-6 >　内存管理，allocator的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/10/ncnn-lession-5/">< NCNN-Lession-5 >　create_layer的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/10/ncnn-lession-4/">< NCNN-Lession-４ >　创建layer子类</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/09/ncnn-lession-3/">< NCNN-Lession-3 >　读取网络的proto信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/09/ncnn-lession-2/">< NCNN-Lession-2 >　Net/Layer/Blob</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/08/ncnn-lession-1/">< NCNN-Lession-1 >　数据读取类DataReader</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/11/13/A-new-FaceQnet/">< Deeplearning > 博采众长，一个更加全面的人脸质量评价库</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/07/30/pri-knowledge-1/">< Deeplearning > 给模型加入先验知识</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/20/advnoise/">< Deeplearning > TF实操Game of Noise</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/17/advAS/">< Antispoofing >　如何科学的攻破活体识别系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/19/saferm/">< Linux > A Safe Way to Use Deletion Command In Linux.</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/27/linux-multi-thread/">< Linux > A simple way to run program with mult-thread</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/DEMO/">< DEMO > My work and DEMO</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/15/gru-bp1/">< Deeplarning > Understand Backpropagation of RNN/GRU and Implement It in Pure Python---1</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/24/op-summary/"><deeplarning> A simple way to distinguish different optimizers in DeepLearning</deeplarning></a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/19/motion-blink-rnn/">< Deeplearning > Use CNN and RNN to detect blink in a video</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/19/linux-install-config/">< Linux > As a AI-practitioner, what I install in my linux system</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/network-depthwise/">< Network > Understanding DepthWiseConv</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/25/tf-quant/">< Tensorflow >How dose TensorFlow do Quant Aware Training?</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/16/antispoof-multi-task-learn/">< Antispoofing > Multi-Task-Learning in Face Antispoofing</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/06/dataset-cut/">< DeepLearning > The Support Vectors in DeepLearning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/01/face-as-1/">< Antispoofing > Data Augmentation in Face Antispoofing</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/30/face-as/">< Antispoofing > Does we should align in Face Antispoofing</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/09/tf-read-data/">< Tensorflow > Data flow in Tensorflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/05/tf-rnn-gru/">< Tensorflow > What is the implementation of GRU in tensorflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/03/softmax-sigmoid-cross-entropy/">< Tensorflow > Softmax cross entropy & Sigmoid cross entropy</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/02/dl-question/"><deeplarning> Some DeepLearning Questions to Think About</deeplarning></a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/30/tf-lsoftmax/">< Tensorflow > How to implement the larget margin softmax loss in tensorflow.</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/tf-rgb-disturb/">< Tensorflow >Tensorflow Data Augmentation RGB distortation</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/tf-tur-perspective-transform/">< Tensorflow >Tensorflow Data Augmentation affine transformation</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/14/newwork/">< Network > Understanding the activation style in residual block</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/11/LINUX-COMMOND/">< Linux > Commond</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/09/bp-simple/">< Deeplearning > Break up backpropagation</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/05/python-tricks-1/">< Python > Some python tricks you may never use But you should know</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/02/tensorflow-disturb-yuv/">< Tensorflow >Tensorflow Data Augmentation YUV distortation</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/30/c-c-1/">< C\C++ > Compile on linux</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/28/network-shufflenet-v2/">< Network > Understanding Shufflenet_v2</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/26/algorithm-hash-table/">< Algorithm > Hash table and decision tree</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/04/py-check/">< Python > To check elements in Python</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2021 Billy
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="A fast, simple &amp; powerful blog framework">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="Another simple and elegant theme for Hexo  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style="display:none">
                        <span id="site-visit" title="Site Visitors"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style="display:none">
                        <span id="page-visit" title="Page Hits"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="Back to Top"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="Comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="Go to Bottom"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>