{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"&lt; DEMO &gt; My work and DEMO","text":"Blinking DEMO The blue digits on the right top of the video is the time of blinks since detection started. For more details, you can check this page. FaceAntispoofing, RNN Smooth DEMO The number behind the string “CNN” is the output of CNN model which is used to predict a face whether is fake or alive. The number behind the string “RNN” is the output of RNN model which aims to smoth the final result fed by CNN model based on sequence. The number behind the string “SE_NUM” is the number of sequence which is used by RNN model to smooth the final result. For details of RNN, you can check this page . Face Antispoofing, RNN online Training DEMO When white string “DO” is displayed on the top of the video, it meams that the RNN model is inferencing without any training and the number behind the string “RNN” is the confidence output of RNN model which is used to predict a face is whether fake or alive. When white string “TRAIN_LIVE” is displayed on the top of the video, it meams that RNN model are trained online only based on liveness samples captured by camera. When white string “TRAIN_ATTACK” is displayed on the top of the video, it meams that RNN model are trained online only based on spoofing samples captrued by camera. The number behind the string “LOSS” is the current loss while training model online. For details of RNN online training, you can check this page . RNN based lane line offset detection DEMO The number on the bottom left of the video indicates the probability whether a car is about to changing a lane or not.","link":"/2019/05/27/DEMO/"},{"title":"&lt; Tensorflow &gt;Tensorflow2.4 最佳实践","text":"开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）． 跑了一个官方给的cifar的例子，效果还行．于是就打算在自己的数据上跑跑看效果，在这个过程中，发现官方给你例子还远远达不到训练速度最优化的程度．于是就把官方例子就改了一下，最终达到了一个满意的训练速度，本篇就是记录一下tf2.4下的训练性能调优全过程．最终调优后的代码会共享出来． 单卡到多卡官方给的vit(Vision Transformer)的例子是基于单卡的，但是现在训练大型网络已经离不开多卡，于是把官方例子改成多卡训练的版本（这里主要说明一下单机多卡的设置）． 首先根据你手上卡的数量来建立一个strategy的对象． 1234567physical_devices = tf.config.list_physical_devices('GPU') for ind, item in enumerate(physical_devices): tf.config.experimental.set_memory_growth(item, True) TRAIN_GPUS = [0,1,2,3] devices = [&quot;/gpu:{}&quot;.format(i) for i in TRAIN_GPUS] strategy = tf.distribute.MirroredStrategy(devices) 这里tf.config.experimental.set_memory_growth的作用是限制显存的使用． 然后我们要把数据做一个副本化的包装： 12train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset) test_dist_dataset = strategy.experimental_distribute_dataset(test_daset) 然后我们需要对train_step和test_step做一个封装： 123456with strategy.scope() def distributed_train_step(dataset_inputs): per_replica_losses = strategy.run(train_step, args=(dataset_inputs,)) return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) def distributed_test_step(dataset_inputs): return strategy.run(test_step, args=(dataset_inputs,)) 最后在train_step，test_step，和计算loss和accuracy加上strategy的scope（这里只是拿train_step和test_step举例）： 1234567891011121314151617with strategy.scope(): def train_step(inputs): images, labels = inputs with tf.GradientTape() as tape: predictions = model(images, training=True) loss = compute_loss(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) compute_acc(labels, predictions, train_accuracy) return loss def test_step(inputs): images, labels = inputs predictions = model(images, training=False) compute_acc(labels, predictions, test_accuracy) 经过了上述的步骤，成功的在tf2.4下，把单卡训练转换到了单机多卡训练． eager mode 到 static graph mode由于tf2.x模式的执行方式是急切执行（eager mode），eager mode的好处在在于方便debug，但是如果拿来训练就不太好了，因为eager mode会托慢速度．所以我们需要在调试好网络之后把执行模式切换为静态图的模式．而这个步骤非常简单，加上一个tf.function的修饰符就好． 123456789with strategy.scope(): @tf.function def distributed_train_step(dataset_inputs): per_replica_losses = strategy.run(train_step, args=(dataset_inputs,)) return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) @tf.function def distributed_test_step(dataset_inputs): return strategy.run(test_step, args=(dataset_inputs,)) 调整tf.data数据流顺序tf.data的数据处理顺序非常重要，改变顺序可能会极度的托慢训练速度． 这里，我的数据处理流程如下： 读取所有图片的路径和对应的label． 把图片路径给parse成图片． 流程其实很简单，但这里要涉及到几点： 如何shuffle. 哪里设定epoch 哪里设定batch 哪里设定预取prefetch． 先来上code，再解释： 1234567 image_roots, labels = generate_fileroots_labels(file_root) dataset = tf.data.Dataset.from_tensor_slices((image_roots, labels)) dataset = dataset.repeat(100).shuffle(buffer_size=2000) # dataset = dataset.map(_parse_data, num_parallel_calls=tf.data.experimental.AUTOTUNE) dataset = dataset.map(_parse_data, num_parallel_calls=16) dataset = dataset.batch(batch_size) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) 首先，repeat表示需要把数据重复多少次，也就是设定的epoc，shuffle代表在多少的buffer中打乱数据．这两者需要放到map前面，因为在map前，数据流处理的都是图片路径和label的轻量化数据，这对于repeat和shuffle是有利的． 而batch和prefetch就需要放到map的后面．这里需要注意，要先设定batch，再prefetch（不然会慢）． 大杀器－－tf.profiler当我们觉得已经把加速做到极致了之后，我们需要用tensorflow自带的性能检测工具tf.profiler来检查一些性能还有那些可以榨取的空间． 用tf.profiler之前，你需要先按照官方的教程安装．这里有个小坑，因为tf.profiler需要依赖libcupti这个库，而libcupti这个库不在cuda的主库目录里，而是在extras/CUPTI/lib64里面，这个需要注意． 然后在你的训练代码中，需要添加如下的code： 1234567for t_step, x in enumerate(train_dist_dataset): if t_step == 500: tf.profiler.experimental.start('/tmp/' + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)) if t_step == 600: tf.profiler.experimental.stop() with tf.profiler.experimental.Trace('Train', step_num=t_step, _r=1): step_loss = distributed_train_step(x) 这个代码段的意思是，在训练的第200步到第300步需要记录你的训练profile． 这里说明两点： 第一，我们不需要要整个训练过程都记录profile，因为记录profile仅仅是为了调优，只需要记录某些步的profile就可以提供你来调优即可． 第二，不从第０步就开始记录是因为我们需要先让训练达到稳定之后记录才会比较准确（示例中是从500步到600步开始记录profile）． 好了，我们看一下，都记录了一些什么东西： 这里我们可以看到耗时主要在3个方面： Kernel Launch Time Host Compute Time Device Compute Time 我们来一个一个解决，首先来解决kernel lanuch time，这个在右边的建议(Recommendation for Next Step)有说明： 14.3 % of the total step time sampled is spent on ‘Kernel Launch’. It could be due to CPU contention with tf.data. In this case, you may try to set the environment variable TF_GPU_THREAD_MODE=gpu_private. 也就是说可以通过设定TF_GPU_THREAD_MODE=gpu_private来解决．也就是说要在之前训练程序的前面加上下面一句命令： 1export TF_GPU_THREAD_MODE=gpu_private 然而我这样执行之后发现作用好像不是很大，知道怎么解决的同学可以交流． 我们现在来解决第二个主要耗时的点，也就是Host Compute Time耗时过高． 我们知道，tensorflow的多卡策略是ps-worker的方式，这个ps可以认为是host，主要是负责更新参数和处理数据流，按理说这部分的耗时不应该很高才对．于是我往下看具体的host的耗时的页面： 我们看到，host的耗时，很大的程度和２个op有关： stridedSlide cast 其中stridedSlide占了绝对的大头，经过查阅资料发现，stridedSlide耗时比较高主要和tf.distribute.MirroredStrategy这个对象有关． 因为我在我的工程里面用到了tf.data.Dataset.from_tensor_slices这个对象，这个对象用到了stridedSlide这个操作，而tf.distribute.MirroredStrategy对stridedSlide的操作支持的不好． 网上的建议是把tf.distribute.MirroredStrategy换成tf.distribute.experimental.MultiWorkerMirroredStrategy． 也就是如下实现： 12345# TRAIN_GPUS = [0,1,2,3]# devices = [&quot;/gpu:{}&quot;.format(i) for i in TRAIN_GPUS]# strategy = tf.distribute.MirroredStrategy(devices)tf.config.set_visible_devices(physical_devices[0:8], 'GPU') strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() 更换了之后，果然Host Compute Time 得到了下降． 关于cast这个操作，是因为我在dataset的parse_function里面用到了tf.cast的操作，我把这个操作放到了网络里面，这部分的耗时也消除了（其实是分给worker了）． 关于Device Compute Time这一部分，我发现这个部分主要的耗时用到了矩阵操作Einsum上，这一部分的操作也是MultiHeadAttention的主要操作，于是也就没有修改这个部分． 最后当然，说了这么多，不如大家来下面一行代码来的方便….. 1import torch as tf 不过话说回来，tensorflow和pytorch不是二选一的问题，而是大家最好都会用，这样才能更好在深度学习里探(lian)索(dan)． 最后把示例代码放到这里，大家自行取用．","link":"/2021/01/27/tf2-x-best-practice/"},{"title":"&lt; NCNN-Lession-9 &gt;　Load Image","text":"开始由于马上学习网络forward的部分，这一节先学习一下ncnn如何读取外部图片的． 我们再插一个小红旗： 作用通常我们读取读片的时候会用到Opencv的Mat类，大家应该发现了，Opencv中的Mat类和ncnn中的Mat类的名字其实是一样的．为了区分方便，我们把Opencv中的Mat类记错cv::Mat，我们把ncnn中的Mat类记做ncnn::Mat. 本节的作用就是用Opencv读取图片，然后把cv::Mat转化为ncnn::Mat． 实现由于Opencv读一幅彩色图片的格式是bgr的格式，我们今天只考虑bgr的cv::Mat转化到ncnn:Mat．暂时不考虑其他格式的转化． 我们首先考虑一下Opencv::Mat的图片数据存放方式． Opencv::Mat的索引方式是这样的： 先索引列h 再索引宽w 再索引通道c 可以用下面的表格来看一下Opencv的数据存放方式: b g r b g r … pad? b g r b g r … pad? 对上面的表格做一个说明： 元素分布是按照bgrbgrbgr….的顺序排列的． 每一行代表一个行（一个w）数据． pad?代表是否需要pad操作．Opencv默认内存是连续的，也就是没有pad操作． 上面是用表格的形式展示的，在内存上，上面的所有行是连在一起的． 我们先考虑Opencv内存是连续的情况． 由于我们之前学过，ncnn::Mat中有ncnn::channel的函数，它可以返回一个对应channel的新的ncnn::Mat对象．所以我们可以针对bgr这三个通道各自建立一个channel对象，并每隔3个元素分别读入即可． 123456789101112131415161718192021222324static int from_rgb(const unsigned char* rgb, int w, int h, int stride, Mat&amp; m, Allocator* allocator) { m.create(w, h, 3, 4u, allocator); if (m.empty()) return -100; float* ptr0 = m.channel(0); float* ptr1 = m.channel(1); float* ptr2 = m.channel(2); int remain = w * h; for (; remain &gt; 0; remain--) { *ptr0 = rgb[0]; *ptr1 = rgb[1]; *ptr2 = rgb[2]; rgb += 3; ptr0++; ptr1++; ptr2++; } return 0;} 现在我们考虑cv::Mat中每一行的内存需要pad，也就是内存不连续的情况． 这个时候我们需要知道Ｍat的stride，也就是加上pad之后一行有多少个元素．所以pad的个数就是： 1const int wgap = stride - w * 3; 在求得wgap之后，每一行读完之后，指针需要移动wgap的数目． 所以，具体的实现方式如下： 12345678910111213141516171819202122232425262728static int from_rgb(const unsigned char* rgb, int w, int h, int stride, Mat&amp; m, Allocator* allocator) { m.create(w, h, 3, 4u, allocator); if (m.empty()) return -100; const int wgap = stride - w * 3; float* ptr0 = m.channel(0); float* ptr1 = m.channel(1); float* ptr2 = m.channel(2); for (int y = 0; y &lt; h; y++) { int remain = w; for (; remain &gt; 0; remain--) { *ptr0 = rgb[0]; *ptr1 = rgb[1]; *ptr2 = rgb[2]; rgb += 3; ptr0++; ptr1++; ptr2++; } rgb += wgap; } return 0;} 如果我们把上述两种情况都考虑在一起，它的实现方式如下： 1234567891011121314151617181920212223242526272829303132static int from_rgb(const unsigned char* rgb, int w, int h, int stride, Mat&amp; m, Allocator* allocator) { m.create(w, h, 3, 4u, allocator); if (m.empty()) return -100; const int wgap = stride - w * 3; if (wgap == 0) { w = w * h; h = 1; } float* ptr0 = m.channel(0); float* ptr1 = m.channel(1); float* ptr2 = m.channel(2); for (int y = 0; y &lt; h; y++) { int remain = w; for (; remain &gt; 0; remain--) { *ptr0 = rgb[0]; *ptr1 = rgb[1]; *ptr2 = rgb[2]; rgb += 3; ptr0++; ptr1++; ptr2++; } rgb += wgap; } return 0;} 代码示例测试程序在这里． 代码结构如下：","link":"/2020/12/21/ncnn-lesson-9/"},{"title":"&lt; NCNN-Lession-10 &gt;　Forward Net","text":"开始今天我们学一下ncnn怎么进行网络前向的，学习了之后，会发现ncnn的网络前向流程的设计是多么的优雅，废话不多说，先插一个小红旗压压惊： 作用网络的前向运算流程在一个神经网络前向框架里面中是处于核心地位的．我们把输入图片送到ncnn的第一个blob，然后通过调用每一个层的前向forward运算函数，来计算每一个层的输出结果，最终得到你想要的结果． 实现ncnn为了更好的实现forward网络前向，定义了一个新的类：Extractor．这个类的主要作用是提供输入数据的接口和输出数据的接口，同时它还保存着每一个网络层计算的结果．我们先来看看这个类的概况图： 这里对Extractor的友元函数Net::create_extractor()做一个说明． 它其实首先是Net类的一个成员函数，它的作用是用一个Net类的对象，调用自己的成员函数，去创建一个Extractor类的对象． 由于它也是Extractor的友元函数，所以它可以访问Extractor的所有成员变量和成员函数，所以它也就可以调用Extractor类的构造函数，所以它就可以实现用一个Net类的对象，调用Extractor的成员函数，去创建一个Extractor类的对象． 我们现在来看一下它是怎么把输入图片（已经转化为ncnn::Mat）通过input函数，给放到对应的blob里面的： 123456789101112int Extractor::input(const char* blob_name, const Mat&amp; in) { int blob_index = net-&gt;find_blob_index_by_name(blob_name); if (blob_index == -1) return -1; return input(blob_index, in); } int Extractor::input(int blob_index, const Mat&amp; in) { if (blob_index &lt; 0 || blob_index &gt;= (int)blob_mats.size()) return -1; blob_mats[blob_index] = in; return 0; } 我们可以看到，它其实是先通过输入blob_name去找对应的blob_index，然后把blob_index作为blob_mats的id给blob_mats赋值． 我们再来看它是如何得到我们想要的输出的： 12345678910111213141516171819int Extractor::extract(const char* blob_name, Mat&amp; feat) { int blob_index = net-&gt;find_blob_index_by_name(blob_name); if (blob_index == -1) return -1; return extract(blob_index, feat); } int Extractor::extract(int blob_index, Mat&amp; feat) { if (blob_index &lt; 0 || blob_index &gt;= (int)blob_mats.size()) return -1; int ret = 0; if (blob_mats[blob_index].dims == 0) { int layer_index = net-&gt;blobs[blob_index].producer; ret = net-&gt;forward_layer(layer_index, blob_mats, opt); } return 10; } 首先它也是通过blob_name去得到我们想要的输出的blob的id（blob_index）．然后检查这个blob_index对应的blob_mat[blob_index]是否是有值的，如果有值则直接返回对应的值． 如果这个blob_mats[blob_index]没有值，则再基于这个blob_index去通过net-&gt;blobs这个变量去找它对应的输入层的id（layer_index）． 然后再调用net的forward函数去计算该层的输出top_blob． forward函数是Net类的成员函数，它的作用是检查某一个层id所对应的bottom_blobs是否有值（之前已经被运算出来）． 如果这个bottom_blob有值（已经被运算出来），那么它就用该层的bottom_blobs去调用对应Layer子类的forward函数来运算得到这个top_blobs． 如果这个bottom_blob没有值，则递归上述过程，直到某一层的bottom_blobs是有值的（不可能无限递归，至少input_blobs肯定有值），是不是很优雅． 我们来看一下它的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142//参数是当前层的layer_id和所有的blob_mats, 其中blob_mats的输入层已经填入数据,其他层可能没有填入数据 int Net::forward_layer(int layer_index, std::vector&lt;Mat&gt;&amp; blob_mats, const Option&amp; opt) const { const Layer* layer = layers[layer_index]; if (layer-&gt;one_blob_only) { //由于是one_blob_only,所有只会取第一个值 int bottom_blob_index = layer-&gt;bottoms[0]; int top_blob_index = layer-&gt;tops[0]; if (blob_mats[bottom_blob_index].dims == 0) { int ret = forward_layer(blobs[bottom_blob_index].producer, blob_mats, opt); if (ret != 0) return ret; } //这个时候经过了递归之后bottom_blob肯定是有值的 Mat bottom_blob = blob_mats[bottom_blob_index]; Mat top_blob; int ret = layer-&gt;forward(bottom_blob, top_blob, opt); if (ret != 0) return ret; blob_mats[top_blob_index] = top_blob; } else { std::vector&lt;Mat&gt; bottom_blobs(layer-&gt;bottoms.size()); for (size_t i = 0; i &lt; layer-&gt;bottoms.size(); i++) { int bottom_blob_index = layer-&gt;bottoms[i]; if (blob_mats[bottom_blob_index].dims == 0) { int ret = forward_layer(blobs[bottom_blob_index].producer, blob_mats, opt); if (ret != 0) return ret; } bottom_blobs[i] = blob_mats[bottom_blob_index]; } std::vector&lt;Mat&gt; top_blobs(layer-&gt;tops.size()); //调用对应Layer子类的forward函数 int ret = layer-&gt;forward(bottom_blobs, top_blobs, opt); if (ret != 0) return ret; for (size_t i = 0; i &lt; layer-&gt;tops.size(); i++) { int top_blob_index = layer-&gt;tops[i]; blob_mats[top_blob_index] = top_blobs[i]; } } return 0; } 关于每一个Layer子类的成员函数前向forward．我们这里先用统一用Ｍat::clone这个接口来实现，也就是说每一个Layer子类的forward函数的作用都是对它的输入blob的复制操作．至于具体的每一类的实现，我们后面能再详细的学习．这里用convolution的forward函数来举个例子： 12345`int Convolution::forward(const Mat&amp; bottom_blob, Mat&amp; top_blob, const Option&amp; opt) const { cout &lt;&lt; &quot;CONV FORWARD&quot; &lt;&lt; endl; top_blob = bottom_blob.clone(opt.blob_allocator); return 0;} 代码示例测试程序在这里． 代码结构下如：","link":"/2020/12/21/ncnn-lesson-10/"},{"title":"&lt; NCNN-Lession-8 &gt;　读取网络的权重信息","text":"开始今天我们要学些ncnn怎么load权重信息，咱们再插一个小红旗： 作用作用当然是给含有权重的层把权重（与层的参数区分开）给load到对应的内存中． 包含有权重的层最常用也就两种： 卷积层 bn层 我们今天就以卷积层为例子，来说一下ncnn中Load 权重的实现． 实现实现Load 权重的功能其实很简单，因为我们在第一节就学习了数据读取类DataReader，我们读取网络proto的时候是就是用的这个类，在读取权重的时候，也是使用的这个类．我们重新把DataReader的框架图放到下面来再看一下: 我们在读取proto信息的时候，主要是用了scan这个成员函数，因为proto信息中结构化的信息比较多． 在我们读取网络权重的时候，主要需要用到read这个函数，因为权重是以二进制的方式存储的，我们在知道elemsize和权重个数的情况下，就可以方便的通过read这个成员函数读取． ncnn中用ModelBinFromDataReader这个类来实现权重的读取，它其实是对于DataReader这个类的一种包装，它的框架图如下： 成员变量就不用多说，我们看一下它的成员函数的实现： 12345678910111213141516171819202122232425262728293031323334Mat ModelBinFromDataReader::load(int w, int type) const { if (type == 0) { size_t nread; union { struct { unsigned char f0; unsigned char f1; unsigned char f2; unsigned char f3; }; unsigned int tag; } flag_struct; nread = dr.read(&amp;flag_struct, sizeof(flag_struct)); unsigned int flag = flag_struct.f0 + flag_struct.f1 + flag_struct.f2 + flag_struct.f3; Mat m(w); if (m.empty()) return m; if (flag != 0) { return m; } else if (flag_struct.f0 == 0) { nread = dr.read(m, w * sizeof(float)); } return m; } else if (type == 1) { Mat m(w); if (m.empty()) return m; size_t nread = dr.read(m, w * sizeof(float)); return m; } return Mat(); } load这个函数，它有两个参数： w 它表示需要读取的元素个数 type 是否去要按照不同的精度读取 这里，对type做一下说明，当type为０的时候，需要读取数据的头部４个字节，然后由头部四个字节的数据来决定按照什么精度读取．这里我对代码做了简化，按照默认的float精度(4字节)的去读． 当type为1的时候，则不读取头部的四个字节，直接按照默认的float精度(4字节)去读． 有了ModelBinFromDataReader这个类，我们就可以在需要load权重的类里面实现一个load_model的函数去调用ModelBinFromDataReader这个类的对象． 在不需要load权重的类里面就不用实现load_model的函数，只需要继承来自Layer父类的load_model函数就行． 最终在net这个类里面，也实现一个load_model的函数，在这个函数里，按顺序遍历每一个层的load_model成员函数即可． 代码示例测试程序在这里． 代码结构如下：","link":"/2020/12/18/ncnn-lesson-8/"},{"title":"&lt; NCNN-Lession-7 &gt;　Mat类的实现","text":"开始这一节我们终于要学习Mat类．大家可以看到，这个类的名字”Mat”其实和Opencv中常用的Mat类是一样的名字，但二者不是同一个东西，一个是ncnn的Mat类，一个是Opencv的Mat类，大家要注意．好，废话不多说，我们再插上一个小红旗（压压惊）： 作用Mat类的作用其实存放神经网络需要处理的数据，这些数据一般包含一下几种(不限于)： 输入图片 模型的weight和bias 模型处理的top_blob和bottom_blob Mat好处在于如下(不限于)： 方便的构建数据，如根据长宽高构建不同维度的数据． 方便的索引数据，如按照channel索引数据． 可以很方便的获取数据信息，如维度，长，宽等等． 可以很方便的拷贝数据，如clone方法 实现首先不废话，先上个Mat的概况图： 上图中，右半部分是Mat类所包含的必要的成员变量，左半部分是Mat所包含的必要的成员函数． 成员函数很简单，名字和类型都在上图中有写到，这里就不再做过多的说明．下面我们着重说一下Mat类的成员函数． 我们从开辟内存的函数Ｍat::create来说起．我看先来看一下它的实现： 123456789101112131415161718192021222324252627inline void Mat::create(int _w, int _h, int _c, size_t _elemsize, int _elempack, Allocator* _allocator) { if (dims == 3 &amp;&amp; w == _w &amp;&amp; h == _h &amp;&amp; c == _c &amp;&amp; elemsize == _elemsize &amp;&amp; elempack == _elempack &amp;&amp; allocator == _allocator) return; release(); elemsize = _elemsize; elempack = _elempack; allocator = _allocator; dims = 3; w = _w; h = _h; c = _c; //pad_y cstep = alignSize((size_t)w * h * elemsize, 16) / elemsize; if (total() &gt; 0) { //pad_z size_t totalsize = alignSize(total() * elemsize, 4); //pad_x data = fastMalloc(totalsize + (int)sizeof(*refcount)); refcount = (int*)(((unsigned char*)data) + totalsize); *refcount = 1; } } 上述函数针对一个三维的数组开辟空间，三个维度分别为长h，宽w和通道数c(channel)． 其中cstep使用了AlignSize()函数，它的含义是计算出一个大于步长数字且该数可以刚好被对齐数(16)整除，这个数字其实就是每一个channel所包含的字节空间数． 计算这个数字的目的是为了进行另外一个维度的对齐，也就是空间尺寸上的对齐．我们在上一节中讲到，alllocator中的alignPtr函数做到了首地址上的对齐． 由于每一个元素的字节数是elemsize，所以一共需要申请的空间如下： 12size_t tmp_size = c* cstep * elemsizesize_t totalsize = alignSize(tmp_size, 4); 关于具体的空间分配情况，我做了一个如下的表格来说明，假如说我们申请了一个(2x2x4)的Mat,具体的空间分配方式如下所示 head pad_x (head_use)elemsize elemsize elemsize elemsize pad_y pad_y… (x) (x) (x) (channel2)elemsize elemsize elemsize elemsize pad_y pad_y… (x) (x) (x) (channel3)elemsize elemsize elemsize elemsize pad_y pad_y… (x) (x) (x) (channel4)elemsize elemsize elemsize elemsize pad_y pad_y… pad_z… 下面对上面的表格做一个说明： 其中head代表总的申请空间的地址． pad_x是为了让使用的空间的首地址能够对齐来进行的把head指针向前推移的操作， elemsize是每一个元素占用的空间． pad_y是为了使得申请的cstep得到空间上的对齐而额外pad的空间，pay_z是为了使的申请的total_size对对齐而额外pad的空间． (x)没有任何意义，请跳过 上个分为4行表示主要是因为我们的channel共有４个，每一行代表一个channel，在实际的内存中，这四行其实是连续的． 下面我们说一下channel这个函数，这个函数其实是构造了一个Mat对象，只不过把Mat对象的数据指针给移动到了对应的位置上: 123inline Mat Mat::channel(int _c) { return Mat(w, h, (unsigned char*)data + cstep * _c * elemsize, elemsize, elempack, allocator); } 其他的函数都是大同小异，大家可以看今天的示例代码． 最后还要说一下，Mat类中定义了一个重载类型转化符，当Mat类被某个指针强制转化的时候，其实是返回了它的成员函数data的指针被强制转化的结果： 1234template &lt;typename T&gt; inline Mat::operator T*() { return (T*)data; } 有的同学可能注意到Mat这个类还有两个的参数没有说，那就是elempack和refcount，我们这一节暂时不讲． 代码示例测试程序在这里． 代码结构如下：","link":"/2020/12/15/ncnn-lesson-7/"},{"title":"&lt; NCNN-Lession-6 &gt;　内存管理，allocator的实现","text":"开始由于在下一节我们要学习最重要的Mat类的实现，所以我们这节要为下节做一下准备．我们这一节学习实现Mat类的一个关键，那就是内存分配方面的实现：allocator． 我们再插一个小红旗（压压惊）： 作用allocator的作用主要是给后面需要用到的mat类中的数据空间分配内存． 其次，出于平台移植安全性和性能的原因，需要对分配的内存进行人工指定字节的内存对齐． 好，下面我们就来看看allocator的实现． 实现假如说我们不按照主动对齐的方式来实现一个内存分配函数(尽管编译器可能会为我们做内存对齐的事情)，我们可以用如下来实现： 1234static inline void* fastMalloc(size_t size) { unsigned char* adata = (unsigned char*)malloc(size); return adata;} 但是，如果我们要自定义内存对齐的话（自定义对齐内存的位数），我们就需要换一种方式来实现． 首先，一段内存n位对齐可以这样理解：这个内存的首地址可以被n整除．那么为什么要内存的首地址被n整除呢？因为CPU就是按照这样来存取内存里面的数据．具体n为多少合适，这个和cpu支持多少位有关． 所以，假如说我们申请的内存首地址不能被n整除，我们需要向前移动这段内存的首地址，直到能够被n整除．所以首先，我们需要申请一段移动n之后还能够有足够大小的内存： 12#define MALLOC_ALIGN 16 unsigned char* udata = (unsigned char*)malloc(size + sizeof(void*) + MALLOC_ALIGN); 这里的MALLOC_ALIGN就是对齐的位数n． 小伙伴可能不理解，都加上了MALLOC_ALIGN了，为什么还要加上sizeof(void*)？这是因为在移动首地址之后，就我们就把本来申请的内存的首地址给丢了，这样在释放的时候就会出现问题，所以我们要在一个地方记录这次这次申请的内存的首地址，如何记住呢，我们往下看． 123456unsigned char** tmpdata = (unsigned char**)udata + 1;unsigned char** adata = alignPtr(tmpdata, MALLOC_ALIGN);adata[-1] = udata;//内存释放方式unsigned char* udata = ((unsigned char**)ptr)[-1];free(udata); 我们可以从tmpdata开始移动内存，因为现在tmpdata就是udata移动了一个地址长度(8Byte)的地址．而我们可以把刚开始申请的内存首地址放到移动后的首地址的前一个buffer里面．这样我们就记住了刚开始申请的内存的首地址． 下面就要看这个移动内存首地址的操作是怎么实现的了： 1234template &lt;typename _Tp&gt; static inline _Tp* alignPtr(_Tp* ptr, int n = (int)sizeof(_Tp)) { return (_Tp*)(((size_t)ptr + n - 1) &amp; -n); } 我们可以看到， (size_t)ptr + n - 1的目的是使得你的内存指针向前移动足够用的距离来对齐n位(被n整除)，然后和-n进行逻辑与的操作． 由于当n是2的y次幂的时候，-n的二进制表示是前面x位是1后面y位0(如n=16的时候的-n的二进制就是111110000)，所以一个数与-n进行与的操作就是这个数的后面的y位被截断了．所以这样的数必对2的y次幂整除，所以也就达到了n为对齐的目的． 其实上述手动n位内存对齐的方法有个替代操作，就是当你的系统是unix的时候，假如你的代码符合POSIX的C标准，你可以通过如下方式来自动对齐内存： 123posix_memalign(&amp;ptr, MALLOC_ALIGN, size);//内存释放方式free(ptr); 代码示例测试程序放到了这里， 代码结构如下：","link":"/2020/12/11/ncnn-lesson6/"},{"title":"&lt; NCNN-Lession-5 &gt;　create_layer的实现","text":"开始在第三节我们实现了读取网络proto的基本流程，但是有一个功能没有实现，就是我们在实例化Layer的时候，其实是实例化了父类的Layer，并没有实例化子类的Layer，这样后面就没法调用子类Layer的方法．在上一节，我们实现了squeezenet中用到的所有子类Layer，所以在这一节，我们就实现如何对在读取网络proto的时候，智能的实例化不同的子类Layer. 我们继续插上新的小红旗： 作用其实父类Layer的目的，就是为了能够利用C++中的多态和重载机制，然后智能化的实例化不同的子类Layer． 在上一节中，我们实现的子类Layer，有些自己的方法，又会继承父类Layer的方法．这样就可以优雅的用同一个接口去调用不同子类Layer的不同功能． 实现由于在load_proto中，代表该Layer属于哪个子类的标识只有layer_type这个char*的标识，所以为了自动的把layer_type和开辟各个子空间的函数应起来． 第一步，我们需要一个layer_type和返回对应子类指针的函数(给子类开辟空间)的结构体： 123456789//定义返回Layer*的函数指针的别名，layer_creator_func，//用于开辟各个子类的空间typedef Layer* (*layer_creator_func)(); struct layer_registry_entry { const char* name; //layer_type layer_creator_func creator; //函数指针，返回Layer*,}; 第二步，我们需要实现给每一个子类开辟空间的函数： 123456789101112131415161718#include &quot;layer/input.h&quot;namespace ncnn{ Layer* Input_final_layer_creator() {return new Input();}}#include &quot;layer/convolution.h&quot;namespace ncnn{ Layer* Convolution_final_layer_creator() {return new Convolution();}}#include &quot;layer/relu.h&quot;namespace ncnn{ Layer* Relu_final_layer_creator() {return new Relu();}}...... 不过上面挨个需要定义每一个Layer子类的开辟空间的函数，较为繁琐，我们发现每一个Layer子类的开辟空间的函数都较为类似，唯一不同的是Layer名字的变化．所以我们可以定义一个宏来做这些重复的操作： 12345678910111213141516#define DEFINE_LAYER_CREATOR(name) \\ ncnn::Layer* name##_final_layer_creator() { return new name; }}#include &quot;layer/input.h&quot;namespace ncnn {DEFINE_LAYER_CREATOR(Input)}#include &quot;layer/convolution.h&quot;namespace ncnn {DEFINE_LAYER_CREATOR(Convolution)}#include &quot;layer/relu.h&quot;namespace ncnn {DEFINE_LAYER_CREATOR(ReLU)} 第三步，接下来，我们实现一个类似于＂字典＂的功能，该＂字典＂的目的是使得从proto读入的layer_type和每一个Layer子类的开辟空间的函数对应． 由于我们上面定义了layer_registry_entry这样一个结构体，这个结构体本质上的目的就是使得layer_type的char*变量和给Layer子类的开辟空间的函数对应．于是，我们基于layer_registry_entry来构造一个数组，它包含了所有用到的对应关系： 12345678910static const layer_registry_entry layer_registry[] = { {&quot;ReLU&quot;, ReLU_final_layer_creator}, {&quot;Input&quot;, Input_final_layer_creator}, {&quot;Pooling&quot;, Pooling_final_layer_creator}, {&quot;Convolution&quot;, Convolution_final_layer_creator}, {&quot;Split&quot;, Split_final_layer_creator}, {&quot;Concat&quot;, Concat_final_layer_creator}, {&quot;Dropout&quot;, Dropout_final_layer_creator}, {&quot;Softmax&quot;, Softmax_final_layer_creator}, }; 第四步，有了这样的一个数组，我们只需要当layer_type传进来的时候，找到对应的结构体就好了，于是我们先确定应该输出上述数组的哪个元素，即layer_type对应的layer_registry_entry的数组id： 12345678910//确定一共有多少个layer static const int layer_registry_entry_count = sizeof(layer_registry) / sizeof(layer_registry_entry);int layer_to_index(const char* type) { for (int i = 0; i &lt; layer_registry_entry_count; i++) { if (strcmp(type, layer_registry[i].name) == 0) return i; } return -1; } 第五步，找到了数组id，我们就可以构建我们的函数了： 1234567891011121314//基于id在数组里面找constructor Layer* create_layer(int index) { if (index &lt; 0 || index &gt;= layer_registry_entry_count) return 0; layer_creator_func layer_creator = 0; { layer_creator = layer_registry[index].creator; } if (!layer_creator) return 0; Layer* layer = layer_creator(); layer-&gt;typeindex = index; return layer; } 我们最后总结一些create_layer的流程： 构建返回Layer*的函数指针的别名：layer_creator_func 构建layer_type和layer_creator_func对应的结构体：layer_registry_entry 实现给每一个子类开辟空间的函数 基于layer_registry_entry来构造一个数组，它包含了所有layer_type和create函数用到的对应关系 基于layer_type找到该layer_type在layer_registry_entry中的id 基于上述id，找到layer_type对应开辟空间的函数． 开辟子类空间，返回子类指针． 代码示例测试程序放到了这里． 代码结构如下：","link":"/2020/12/10/ncnn-lession-5/"},{"title":"&lt; NCNN-Lession-４ &gt;　创建layer子类","text":"开始今天我们开始下一节，那就是我们要开始创建Layer子类，因为有了Layer子类，我们才可以针对不同的op进行不同的操作．所以我们再次插上一个小红旗． 作用我们把之前第二节讲的父类Layer记做Layer(Father)，把今天这一节讲的子类Layer记做Layer(Child)． Layer(Child)的作用主要是用来针对不同的op来提供一些不同的forward前向操作，同时Layer(Child)拥有自己的参数parameter和权重weight，它们也继承了来自父类的一些特性，比如name, type, one_blob_only等等． 我们今天就先实现一下squeezenet中的Layer(Child)．我们不打算把Layer(Child)的所有成员都实现，今天仅仅实现他们的一些与网络proto有关的东西，比如成员变量等等． 实现在squeezenet中，所有的layer op如有： input Convolution Relu Pooling Split Concat Dropout Softmax 大家可能对dropout有疑问，这里因为官方的ncnn的proto中包含了这个op，于是这里也把这个op加了上去． 先来看一下Layer(Child)都包含些什么成员： 由于我们本节主要实现的是Layer(Child)的参数部分，暂时先不实现权重部分和前向部分，所以Layer(Child)的成员函数只有一个，那就是： 1virtual int load_param(const ParamDict&amp; pd); 这里要说明一下，Layer(Child)中重写Layer(Father)的load_param函数前面的virtual可加可不加，因为只要Layer(Father)加了virtual的关键字，Layer(Child)默认也会加的． 所以我们需要在Layer(Father)中也实现一个load_param的虚函数． 1int Layer::load_param(const ParamDict&amp;) { return 0; } 关于不同的Layer(Child)的参数params，由于每一个不同的层，有不同的参数，具体需要什么参数与该Layer前向的时候需要用到什么有关，这里就不多说了． 代码示例这节其实没什么好说的，其实就是简单的实现了几个层．测试程序在这里． 代码结构如下： 这里增加了一个文件夹layer，里面存放着Layer(Child)子类的实现．","link":"/2020/12/10/ncnn-lession-4/"},{"title":"&lt; NCNN-Lession-3 &gt;　读取网络的proto信息","text":"开始今天我们开始第三课，来说一些如何读取网络的proto信息，所以我们又要插上一个小红旗： ![](ncnn-lession-3/lession_3.png) 今天的讲解和之前的不太一样, 之前的都是讲类的实现．今天更加侧重讲解函数处理流程．让我们开始吧。 作用当我们要把训练好的网络部署到移动端的时候，网络结构的表达就非常重要．因为部署框架需要把你的网络结构读取它的自己的数据结构中． 我们上一节讲的Net/Layer/Blob就是ncnn自己的网络数据结构，所以我们要把自己的模型的网络结构load到ncnn的Net/Layer/Blob中． 我们先用一个例子来看一下ncnn读取的是怎样的网络结构（把它称为proto）： 这是一个经典的squeezenet的网络结构(非全部)，我们可以看到，第一行只有一个数字，它没有任何意义，只是一个标记，然后第二行有两个数字，分别代表layer的总数和blob的总数。从第三行开始的每一行代表一个网络的layer op操作和对应的layer信息。我们以卷积层为例子，来说明每一列代表什么含义： layer_type layer_name bottom_count top_bount bottom_name top_name 参数1 参数2 … Convolution data 1 1 data conv1 0=227 1=225 … Relu relu_conv1 1 1 data1 conv1_relu_conv1 0=0.00000 其中 layer_type:这个layer op是哪种op layer_name: 这个layer op的名字 bottom_count: 输入blob的个数 top_bount: 输出blob的个数 bottom_name:输入blob的名字 top_name:输出blob的名字 参数: 该layer op的参数 这样的一个proto可以完备的表达一个网络的数据流向， 所以我们可以用它来描述自己的网络结构．我们现在先不讲知道这个proto是怎样来的，今天主要要研究一下ncnn是怎样把这个proto给读到自己的数据结构中． 实现要把网络的proto读到ncnn自己定义的Net/Layer/Blob中，就要用到我们之前第一课中学习到的datareader类，因为网络proto中都是一些固定格式的信息，所以我们主要用到datareader类中的scan函数，这个其实就是fscan的一种包装，它比较适合读一些结构化的数据。 我们可以把proto中的信息分成3类： 字符串如layer_type, layer_name, blob_name 数字如layer_num, blob_num 带有”=”的参数信息 我们要通过不同的format格式去读取上述三种信息，对应的方式如下： 对于字符串的话，我们需要用如下的方法去读：12345FILE* fp;DataReaderFromStdio dr(fp);......char buf[256];dr.scan(&quot;%255&quot;, buf); // fscan(fp, &quot;%255&quot;, buf) 注意，我们规定读取字符串的最大长度为256，如果你的字符的长度大于256，则会出现错误．同时，由于函数fscanf遇到空格对停止读取操作，所以不必担心256过长． 对于数字的话，由于不确定数字的写法（比如是否是用科学计数法），我们还是需要用读字符串的方式去读，然后再转化为数字，如下所示：1234567891011FILE* fp;DataReaderFromStdio dr(fp);......char vstr[16];dr.scan(&quot;%15s&quot;, vstr);bool is_float = vstr_is_float(vstr);if(is_float) float now_param = vstr_to_float(vstr);else int now_param = 0; sscanf(vstr, &quot;%d&quot;, &amp;now_param); 如上所示，我们需要先判断读取的字符串是不是float，如果是float，我们需要先将其转化为float，如果不是float，我们直接调用sscanf去读取字符串的数字． 对于带有”=”的参数信息，我们需要用如下的方式去读：12345FILE* fp;DataReaderFromStdio dr(fp);......int id = 0;dr.scan(&quot;%d=&quot;, id); //fscan(fp, &quot;%d=&quot;, id) 给layer和blob分配id由于后面在前向推理的时候，我们要通过id去找对应的layer和blob，所以给layer和blob分配id就是一件非常重要的事情． 对于layer的id是非常自然的，因为在你的proto中，从第三行开始的每一行都是一个layer op的操作，所以我们就根据行数，给每一个layer顺序的分配自己的id. 给blob分配id的思想类似，在前面我们介绍过，每一层layer信息中，有一个参数代表着这个layer的top个数，我们可以根据layer的id和top的个数给blob分配id信息． layer类还有两个重要的参数，那就是layer-&gt;tops和layer-&gt;bottoms，前者是layer的top blob的id的集合，后者是layer的bottom blob的id集合． 我们在赋值layer-&gt;tops的时候可以直接用blob的id．我们在赋值layer-&gt;bottoms的时候，就需要从之前的blob中去找对应的名字的blob的id，这也就是我们需要blob的name的原因． 具体的代码可以如下所示： 1234567891011121314151617int blob_index = 0;for(int i = 0; i &lt; layer_count; i++){ int layer_id = i;//layer_id for(int j = 0; j &lt; bottom_count; j++) { int bottom_index = find_blob_index_by_name(bottom_name); layer-&gt;bottoms[j] = bottom_index; //layer-&gt;bottoms } for(int j = 0; j &lt; top_count; j++) { Blob&amp; blob = blobs[blob_index]; // 获取这时候的blob_id int now_blob_index = blob_index;//blob id layer-&gt;tops[j] = blob_index; //layer-&gt;tops blob_index++; }} 有一个小trick需要说一下： 由于我们需要频繁的调用如下的函数接口： 1dr.scan(format, buf); 我们可以通过宏函数来调用这个函数，这样就会比较简单明了： 1#define SCAN_VALUE(fmt, v) dr.scan(fmt, &amp;v); 代码示例关于这一节的测试程序在这里，代码结构如下： 大家可以看到，我们这里多了一个paramdict的实现，这是由于在读取数字的时候，涉及到判断是否是float，和char转float等等操作，会比较繁琐，所以ncnn把这部分的实现放到了paramdict这个类去实现．","link":"/2020/12/09/ncnn-lession-3/"},{"title":"&lt; NCNN-Lession-2 &gt;　Net&#x2F;Layer&#x2F;Blob","text":"开始今天我们开始第二课,也就是Net/Layer/Blob的讲解. ![](ncnn-lession-2/lession_2.png) Net/Layer/Blob这三个类是组成网络的基础.我们今天就学习和实现一下它们的基础功能. 作用我们来分别说明它们三个类的作用. Net类是共外部调用的一个类,它提供了两个重要的外部调用接口: load_param() load_model() 这两个接口可以调用用户自己网络的结构和权重信息,不过这个不是我们今天的重点. Layer这个类包含了网络层的信息,比如层的名字(name), 类型(type)信息等等. 同时它是一个父类,它的子类包含了具体的网络层实现,比如convolution, relu等等. Blob这个类比较简单,它主要包含了网络每一个blob的名字(name),产生该blob的层id(producer),使用该blob的层的id(consumers). 实现关于Net的实现,我们先看图 我们今天主要讲它包含的两个变量: layers blobs layers是一个包含了所有layer*的vector数组，它存放了所有layer的信．blobs是一个包含了所有blob的数组，它存放了所有blob的信息，关于blob我们下面会讲到． 关于layer的实现，我们先看图 它包含了4个重要的变量 type name bottoms tops 这四个变量分别是layer的类型(type)，名字(name)，输入blob的id(bottoms)， 输出blob的id(tops)． 之所以要在Layer后面加上Father， 是因为它是具体子Layer实现类的父类，而这些具体的Layer实现类我们后面再做介绍． 关于blob的实现，我们先看图 它包含了3个重要变量 name producer consumers name当然是该blob的名字． producer是一个int类型的变量，它存放着产生该blob的layer_id． consumers 是一个vector容器，里面存放着需要该blob作为输入的层的id，也就是字面意思的需要“消费”该blob的layer_id，因为可能不止一个层需要用到该blob作为输入，所以这个layer_id可能有多个，它需要一个容器来保存． 代码示例关于Net/Layer/Blob的测试程序在这里 ．代码结构如下： 代码中主要把Net/Layer/Blob的构造函数测试了一下．","link":"/2020/12/09/ncnn-lession-2/"},{"title":"&lt; NCNN-Lession-1 &gt;　数据读取类DataReader","text":"开始从本篇开始，我们来拆解NCNN的代码，共同学习一下NCNN的相关知识．如果有错误，希望大佬指正，大家共同学习． 初步的大计划程如下： ![](ncnn-lession-1/lession_before.png) 漏斗显示我们计划还未开始． 由于今天开始DataReader的讲解，所以我们会把第一个漏斗变成一个小红旗： 后面的计划可能所有变化，以尽量合理为准．我们今天就开始第一课，那就是DataReader类的学习． 作用DataReader的作用主要有两个： 读取深度模型的proto信息 读取深度模型的权重信息 模型的proto信息如下，主要是写了一些模型结构的信息，下面是官方的squeezenet的网络proto： 模型的权重信息则写到了一个二进制文件中． 实现先来一张图来看一下类DataReader的实现方式： DataReaderFromStdio是DataReader的子类，我们主要用它来实现DataReader类的主要功能． 它包含一个成员变量： fp 它其实是一个FILE的指针，也就是保存了当前读取的数据流的状态． 它包含两个成员函数： scan read 这两个成员函数分别是对fscan和fread的一个包装． 函数scan是按照format格式读取信息的，所以它非常适合读取模型的proto信息． 函数read是安装buffer长度读取信息的，所以它非常适合读取读取模型的权重信息． 具体的实现方式如下: 123456789101112//scan的读取方式是按照format去读取int DataReaderFromStdio::scan(const char* format, void* p) const{ return fscanf(fp, format, p);}//read的读取方式是按照size去读取size_t DataReaderFromStdio::read(void* buf, size_t size) const{ return fread(buf, 1, size, fp);} 代码示例关于DataReader的测试代码在这里 ，代码结构如下： 代码中分别对scan和read做了测试．主要是学习一些基于format和基于buf_size去读取文件的差别．","link":"/2020/12/08/ncnn-lession-1/"},{"title":"&lt; NCNN-Lession-Start &gt;　Start","text":"一这个tutorial是笔者在自己学习ncnn的过程的一个记录．等于把自己的理解写了出来，如果有差错，还请大家指正． 这个tutorial还没有写完，后面会随着学习的深入，会不定期的更新． 二这个tutorial的每一节，会分为4个section来讲述，分别是： 开始 作用 实现 代码示例 开始：每一节的开始，说一下当前学习计划． 作用：说一下本节要讲的内容的作用． 实现：说一下这一节的内容ncnn是如何实现的． 代码示例：把这一节的内容中ncnn相关的代码拿出来，写了个例子． 三每一节，为了方便展示，我都会用一些图表来展示，如： head pad_x (head_use)elemsize elemsize elemsize elemsize pad_y pad_y… (x) (x) (x) (channel2)elemsize elemsize elemsize elemsize pad_y pad_y… (x) (x) (x) (channel3)elemsize elemsize elemsize elemsize pad_y pad_y… (x) (x) (x) (channel4)elemsize elemsize elemsize elemsize pad_y pad_y… pad_z… 四在每一节的最后，我都会附上对应的示例代码．在使用每一节的示例代码时候，需要自己clone下来，然后自己编译．一个常用的过程如下： 123456git clone git@github.com:Zhengtq/ncnn_breakdown.gitcd ncnn_breakdown &amp;&amp; cd lesson &amp;&amp; cd lesson_1mkdir build &amp;&amp; cd buildcmake ..make -j8./examples/lession_1 五Have Fun !","link":"/2020/12/02/ncnn-lesson-start/"},{"title":"&lt; Deeplearning &gt; 博采众长，一个更加全面的人脸质量评价库","text":"开始今天给大家推荐一个效果很好速度又快(逃))的人脸质量评价库，这个人脸质量评价库是我自己训的，为了方便表示，我给它起个名字，就BFQ(很像BBQ). 其实网上有很多开源的人脸质量算法，其中的很多甚至开源了模型出来，这给需要用人脸质量评价算法来过滤人脸的人们提供了很好的工具，但是很多脸质量算法的效果并不好，或是不能满足自己的需要，或是只能满足一部分的需求，所以我们往往需要多个质量算法库联合去判断一个人脸的质量。 选择基础算法我在网上找了一下几个觉得还不错的(人脸)质量评估算算法： 拉布拉斯二阶梯度 一阶梯度 MotionBlurNet(mbn) FaceQualityNet(fqn) PaQ-2-PiQ 这5中算法各有所长，比如fqn是针对人脸的一种质量评价，对于人脸本身的性质，如遮挡和角度具有一定的敏感性；mbn是针对运动模型的一个质量评价算法；PaQ-2-PiQ是针对全场景的一种质评价；而基于一阶/二阶的梯度算法则是用传统算法去求解梯度去衡量一张图片的清晰度。 假如我们想基于分类去训练一个人脸质量评价的网络，我们的目的是训练一个输出为0/1的二分类器，0代表质量不好，1代表质量好，我们可以是先通过上面五种质量评价算法去给我们的每一个人脸样本打上置信度，然后基于这五个置信度分别设定不同的阈值，联合去判断一个样本的质量标签(0或者1)，然后基于这个标签去训练一个质量评价的分类网络。 当然上面的做法是有明显的缺点的，那就是没有给不同质量的人脸划分程度，质量最好的和质量次好的都往相同的目标去优化，同样的，质量最差的和质量次差的也是都向同样的目标去优化，这样显然是不合理的。 更加合理的做法是把这个任务当做回归的问题来看待，我们的目标是训练一个人脸质量评价归回网络，而不是一个分类网络。 标签设置所以现在问题来了，怎么给每一张图打上对应的标签。有一个办法，就是通过上面的5个图像(人脸)质量评价算法给每一张图片打的置信度综合去给每一张图片的质量标签，这样下来，图片的质量标签就集合了上面5个图像(人脸)质量算法的特点，更加的全面。 通过给我们的训练图片打上标签，我们发现上面每一个质量评价算法的输出值的范围的差别很大。统计在训练集上的每一个质量评价库的输出值范围如下： 拉布拉斯二阶梯度 (0-1427) 一阶梯度 (0-23) MotionBlurNet(mbn) (0-36) FaceQualityNet(fqn) (0.3-0.81) PaQ-2-PiQ (25-85) 具体的分布如下图所示： 调整分布大家可以看到，不同算法的输出值的范围的差异还是很大的，我们我们要做的就是统计出每一个算法输出的最大值max和最小值min，然后用对应的最大值和最小值归一化每一个算法的输出。具体的方法如下： 1val = (val - min)/(max - min) 这里需要说明一点，那就是这个min和max不是所有样本的统计值，从上面的直方图我们也可以看到，很多算法的min,max值是一种奇异值的存在，它们绝大部分的输出分布和min, max相差的较远，于是从我从上面的直方图大致估计了一下它们新的min,max，这个min,max所代表的的范围比实际的范围要小，所以也用这样的min,max去归一化输出值也会更加的精准。 归一化之后直方图如下： 好，把各个算法的输出都归一化到同一个尺度后，我们就要想想怎么去组合每一个样本的5个质量评价的结果，然后去生成它自己的质量标签。可以直接通过给不同输出结果加权重的方式来生成一个新的回归标签，如下： 1bfq_score = w1 * lp_blur_socre + w2 * g_sharp + w3 * MotionNet_score + w4 * FaceQnet_score + w5 * Paq-P2Q w1到w5的这些权重就需要自己手动设置了，你可以以算法的重要性来设定权值，把比较符合自己task的算法输出的权值增大。 好，经过了上面的操作，我来看一下训练集的bfq_score的归一化分布情况： 通过上面的样本label分布图中，我们可以看到，数据呈现了明显长尾分布。所以我对分布较少的区域的做了不同程度的曾广，从而让不同label的分布的差异不至于这么大。 后面就开始训练了，这里我用了普通的L2_LOSS做归回，然后训练了大概50个epoc。然后用tensorflow自带的工具计算了一个模型的计算量，如下图： 计算量大概有13M(乘法加法算两次)，应该还有降低的空间，这个后面再优化。 NCNN封装为了更加灵活的使用，我打算用NCNN把bfq模型封装一下．这里，我在原始NCNN的基础上，拆分出了一个极简的实现，对bfq的ncnn封装都是基于这个极简的ncnn实现，代码会在最后放出来． 首先，由于我是使用tensorflow训练的，需要把TF使用的网络结构转换成NCNN的格式． 在网上找了一下，发现没有合适又好用的转换程序，于是自己写了一套，最后转换的结果如下所示（展示部分）： 1234567891011121314151617181920212223242526272829303132337767517182 195Input layer_0 0 1 blob_0 0=80 1=80 2=3Convolution layer_2 1 1 blob_0 blob_1 0=24 1=3 2=1 3=2 4=0 14=0 15=1 16=1 7=1 5=0 6=648BatchNorm layer_7 1 1 blob_1 blob_2 0=24 1=1e-3ReLU layer_8 1 1 blob_2 blob_3 0=0.0Pooling layer_9 1 1 blob_3 blob_4 0=0 1=3 2=2 3=0 13=0 14=1 15=1 4=0 5=0Convolution layer_11 1 1 blob_4 blob_5 0=24 1=1 2=1 3=1 4=0 14=0 15=0 16=0 7=1 5=0 6=576BatchNorm layer_16 1 1 blob_5 blob_6 0=24 1=1e-3ReLU layer_17 1 1 blob_6 blob_7 0=0.0ConvolutionDepthWise layer_19 1 1 blob_7 blob_8 0=24 1=3 2=1 3=2 4=0 14=0 15=1 16=1 7=24 5=0 6=216BatchNorm layer_24 1 1 blob_8 blob_9 0=24 1=1e-3Convolution layer_26 1 1 blob_9 blob_10 0=24 1=1 2=1 3=1 4=0 14=0 15=0 16=0 7=1 5=0 6=576BatchNorm layer_31 1 1 blob_10 blob_11 0=24 1=1e-3ReLU layer_32 1 1 blob_11 blob_12 0=0.0ConvolutionDepthWise layer_34 1 1 blob_4 blob_13 0=24 1=3 2=1 3=2 4=0 14=0 15=1 16=1 7=24 5=0 6=216BatchNorm layer_39 1 1 blob_13 blob_14 0=24 1=1e-3Convolution layer_41 1 1 blob_14 blob_15 0=24 1=1 2=1 3=1 4=0 14=0 15=0 16=0 7=1 5=0 6=576BatchNorm layer_46 1 1 blob_15 blob_16 0=24 1=1e-3ReLU layer_47 1 1 blob_16 blob_17 0=0.0ShuffleTwo layer_48 2 1 blob_17 blob_12 blob_18Split layer_52 1 2 blob_18 blob_19 blob_20Convolution layer_54 1 1 blob_19 blob_21 0=24 1=1 2=1 3=1 4=0 14=0 15=0 16=0 7=1 5=0 6=576BatchNorm layer_59 1 1 blob_21 blob_22 0=24 1=1e-3ReLU layer_60 1 1 blob_22 blob_23 0=0.0ConvolutionDepthWise layer_62 1 1 blob_23 blob_24 0=24 1=3 2=1 3=1 4=1 14=1 15=1 16=1 7=24 5=0 6=216BatchNorm layer_67 1 1 blob_24 blob_25 0=24 1=1e-3Convolution layer_69 1 1 blob_25 blob_26 0=24 1=1 2=1 3=1 4=0 14=0 15=0 16=0 7=1 5=0 6=576BatchNorm layer_74 1 1 blob_26 blob_27 0=24 1=1e-3ReLU layer_75 1 1 blob_27 blob_28 0=0.0ShuffleTwo layer_76 2 1 blob_28 blob_20 blob_29Split layer_80 1 2 blob_29 blob_30 blob_31...... 这里需要说明一下，上面的ncnn结构中有个ShuffleTwo的操作，这是在原始的ncnn中是没有的，这里我为了简化操作，自己实现了一个ShuffleTwo的操作． 其次，我需要把TF的模型权重转换成ncnn能够使用的二进制格式．这里需要注意一下，ncnn使用的默认权重格式是(out_c,in_c,h,w)，而TF的默认权重格式是(h, w, in_c, out_c)，所以需要在转换权重的时候，对权重加上transpose的操作． Pybind11 封装为了更方便的调用，决定用Pybind11把c++代码封装成python的库． 由于要编译python接口，所以在CMakeList中添加了一个按钮PY_WRAP，用来选择是编译python接口还是普通的c++接口． 123456789set(PY_WRAP ON)set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -std=c++11&quot;)add_subdirectory(src)if(PY_WRAP) add_subdirectory(python)else() add_subdirectory(examples)endif() 最后我把代码开源到了github上，暂时没有放二进制的网络权重，后面会放出来． 代码地址： BFQ","link":"/2020/11/13/A-new-FaceQnet/"},{"title":"&lt; Deeplearning &gt; 给模型加入先验知识","text":"模型加入先验知识的必要性端到端的深度神经网络是个黑盒子，虽然能够自动学习到一些可区分度好的特征，但是往往会拟合到一些非重要特征，导致模型会局部坍塌到一些不好的特征上面。常常一些人们想让模型去学习的特征模型反而没有学习到。为了解决这个问题，给模型加入人为设计的先验信息会让模型学习到一些关键的特征。下面就从几个方面来谈谈如何给模型加入先验信息。 为了方便展示，我这边用一个简单的分类案例来展示如何把先验知识加入到一个具体的task中。我们的task是在所有的鸟类中识别出一种萌萌的鹦鹉，这中鹦鹉叫鸮鹦鹉，它长成下面的样子：这种鸟有个特点，就是它可能出现在任何地方，但就是不可能在天上，因为它是世界上唯一一种不会飞的鹦鹉(不是唯一一种不会飞的鸟)。好，介绍完task的背景，咱们就可以分分钟搭建一个端到端的分类神经网络，可以选择的网络结构可以有很多，如resnet, mobilenet等等，loss往往是一个常用的分类Loss，如交叉熵，高级一点的用个focal loss等等。确定好了最优的数据(扰动方式)，网络结构，优化器，学习率等等这些之后，往往模型的精度也就达到了一个上限。然后你测试模型发现，有些困难样本始终分不开，或者是一些简单的样本也容易分错。这个时候如果你还想提升网络的精度，可以通过给模型加入先验的方式来进一步提升模型的精度。 基于pretrain模型给模型加入先验给模型加入先验，大家最容易想到的是把网络的weight替换成一个在另外一个任务上pretrain好的模型weight。经过的预训练的模型(如ImageNet预训练)往往已经具备的识别到一些基本的图片pattern的能力，如边缘，纹理，颜色等等，而识别这些信息的能力是识别一副图片的基础。如下图所示：但这些先验信息都是一些比较general的信息，我们是否可以加入一些更加high level的先验信息呢。 基于输入给模型加入先验假如你有这样的一个先验，你觉得鸮鹦鹉的头是一个区别其他它和鸟类的重要部分，也就是说相比于身体，它的头部更能区分它和其他鸟类。这时怎么让网络更加关注鸮鹦鹉的头部呢。这时你可以这样做，把整个鸮鹦鹉和它的头部作为一个网络的两路输入，在网咯的后端再把两路输入的信息融合。以达到既关注局域，又关注整体的目的。一个简单的示意图如下所示。 基于模型重现给模型加入先验接着上面的设定来，假如说你觉得给模型两路输入太麻烦，而且增加的计算量让你感觉很不爽。这时，你可以尝试让模型自己发现你设定的先验知识。假如说你的模型可以自己输出鸟类头部的位置，虽然这个鸟类头部的位置信息是你不需要的，但是输出这样的信息代表着你的网络能够locate鸟类头部的位置，也就给鸟类的头部更加多的attention，也就相当于给把鸟类头部这个先验信息给加上去了。当然直接模仿detection那样去回归出位置来这个任务太heavy了，你可以通过一个生成网络的支路来生成一个鸟类头部位置的Mask，一个简单的示意图如下： 基于CAM图激活限制给模型加入先验针对鸮鹦鹉的分类，我在上面的提到一个非常有意思的先验信息，那就是鸮鹦鹉是世界上唯一一种不会飞的鹦鹉，这个信息从侧面来说就是，鸮鹦鹉所有地方都可能出现，就是不可能出现在天空中(当然也不可能出现在水中)。也就是说不但鸮鹦鹉本身是一个分类的重点，鸮鹦鹉出现的背景也是分类的一个重要参考。假如说背景是天空，那么就一定不是鸮鹦鹉，同样的，假如说背景是海水，那么也一定不是鸮鹦鹉，假如说背景是北极，那么也一定不是鸮鹦鹉，等等。也就是说，你不能通过背景来判断一只未知的鸟是鸮鹦鹉，但是你能通过背景来判断一只未知的鸟肯定不是鸮鹦鹉(是其他的鸟类)。所以假如说获取了一张输入图片的激活图(包含背景的)，那么这张激活图的鸟类身体部分肯定包含了鸮鹦鹉和其他鸟类的激活，但是鸟类身体外的背景部分只可能包含其他鸟类的激活。所以具体的做法是基于激活图，通过限制激活图的激活区域，加入目标先验。激活图是基于分类网络的倒数第二层卷积层的输出的 feature_map 的线性加权，权重就是最后一层分类层的权重，由于分类层的权重编码了类别的信息，所以加权后的响应图就有了基于不同类别的区域相应。(具体的介绍可以看 https://zhuanlan.zhihu.com/p/51631163)，具体的激活图生成方式可以如下表示：说了这么多，下面就展示展示激活图的样子：大家可以看到，上面一张是一只鸮鹦鹉的激活图，下面是一只在天空飞翔的大雁的激活图。因为鸮鹦鹉的Label是0，其他鸟类的Label是1，所以在激活图上，只要是负值的激活区域都是鸮鹦鹉的激活，也就是Label为0的激活，只要是正值的激活都是其他鸟类的激活，也就是Label为1的激活，为了方便展示，我把负值的激活用冷色调来显示，把正值的激活用暖色调来显示，所以就是变成了上面两幅激活图的样子。而右边的数字是具体的激活矩阵(把激活矩阵进行GAP就可以变成最终输出的Logits)。到这里不知道大家有没有发现一个问题，就是无论对于鸮鹦鹉还是大雁的图片，它们的激活图除了分布在鸟类本身，也会有一部分分布在背景上。对于大雁我们好理解，因为大雁是飞在天空中的，而鸮鹦鹉是不可能在天空中的，所以天空的正激活是非常合理的，但是对于鸮鹦鹉来说，其在除了鸟类身体以外的负激活就不是太合理，因为，大雁或者是其他的鸟类，也可能在鸮鹦鹉的地面栖息环境中(但是鸮鹦鹉却不可能在天空中)。所以环境不能提供任何证据来证明这一次鸟类是一只鸮鹦鹉，所以鸮鹦鹉的负激活只是在鸟类的身体上是合理的。而其他鸟类的正激活却可以同时在鸟类身体上又可能在鸟类的背景上(如天空或者海洋)。所以我们需要这样建模这个问题，就是在除鸟类身体的背景上，不能出现鸮鹦鹉的激活，也就是说不能出现负激活(Label为0的激活）。所以下面的激活才是合理的：从上面来看，在除鸟类身体外的背景部分是不存在负激活的，虽然上面的背景部分有一些正的激活(其他鸟类的激活)，但是从右边的激活矩阵来看，负激活的scale是占据绝对优势的，所以完全不会干扰对于鸮鹦鹉的判断。所以问题来了，怎么从网络设计方面来达到这个目的呢？其实可以从Loss设计方面来达到这个效果。我们假设每一个鸟都有个对应的mask，mask内是鸟类的身体部分，mask外是鸟类的背景部分。那么我们需要做的就是抑制mask外的背景部分激活矩阵的负值，把那一部分负值给抑制到0即可。鸟类的激活矩阵和mask的关系如下图(红色的曲线代表鸟的边界mask)：我们的Loss设计可以用下面的公式表示： 1Loss_cam = -sum(where(bird_mask_outside&lt;0)) 基于辅助学习给模型加入先验知识到现在为止，咱们还只是把我们的鸟类分类的task当成一个二分类来处理，即鸮鹦鹉是一类，其他的鸟类是一类。但是我们知道，世界的鸟类可不仅仅是两类，除了鸮鹦鹉之外还有很多种类的鸟类。而不同鸟类的特征或许有很大的差别，比如鸵鸟的特征就是脖子很长，大雁的特征就是翅膀很大。假如只是把鸮鹦鹉当做一类，把其他的鸟类当做一类来学习的话，那么模型很可能不能学到可以利用的区分非鸮鹦鹉的特征，或者是会坍塌到一些区分度不强的特征上面，从而没有学到能够很好的区分不同其他鸟类的特征，而那些特征对去区别鸮鹦鹉和其他鸟类或许是重要的。所以我们有必要加入其他鸟类存在不同类别的先验知识。而这里，我主要介绍基于辅助学习的方式去学习类似的先验知识。首先我要解释一下什么是辅助学习，以及辅助学习和多任务学习的区别：上图的左侧是多任务学习的例子，右侧是辅助学习的例子。左侧是个典型的face attribute的task，意思是输入一张人脸，通过多个branch来输出这一张人脸的年龄，性别，发型等等信息，各个branch的任务是独立的，同时又共享同一个backbone。右边是一个典型的辅助学习的task，意思是出入一张人脸，判断这一张人脸的性别，同时另外开一个(或几个)branch，通过这个branch来让网络学一些辅助信息，比如发型，皮肤等等，来帮助网络主任务(分男女)的判别。好，回到我们的鸮鹦鹉分类的task，我们可能首先会想到下面的Pipeline:这样虽然可以把不同类别的鸟类的特征都学到，但是却削弱了网络对于鸮鹦鹉和其他鸟类特征的分别。经过实验发现，这种网络架构不能很好的增加主任务的分类精度。为了充分的学到鸮鹦鹉和其他鸟类特征的分别，同时又能带入不同种类鸟类类别的先验，我们引入辅助任务：在上面的Pipeline中，辅助任务相比如主任务，把其他鸟类做更加细致的分类。这样网络就学到了区分不同其他鸟类的能力。但是从实验效果来看这个Pipeline的精度并不高。经过分析原因，发现在主任务和辅助任务里面都有鸮鹦鹉这一类，这样当回传梯度的时候，相当于把区分鸮鹦鹉和其他鸟类的特征回传了两次梯度，而回传两次梯度明显是没用的，而且会干扰辅助任务学习不同其他鸟类的特征。所以我们可以把辅助任务的鸮鹦鹉类去除，于是便形成了下面的pipeline:经过实验发现，这种pipeline是有利于主任务精度提升的，网络对于特征明显的其他鸟类的分类能力得到了一定程度的提升，同时对于困难类别的分类能力也有一定程度的提升。这时候你会想，上面的pipeline好是好，但是我没有那么多的label啊。是的，上面的pipeline除了主任务的label标注，它还同时需要很多的辅助任务的label标注，而标注label是深度学习任务里面最让人头疼的问题(之一)。别怕，我下面介绍一个work，它基于meta-learning的方法，让你不再为给辅助任务标注label而烦恼，它的framework如下：这个framework采用基于maxl的方案（https://github.com/lorenmt/maxl），辅助任务的数据和label不是由人为手工划分，而是由一个label generator来产生，label generator的优化目标是让主网络在主任务的task上的loss降低，主网络的目标是在主任务和辅助任务上的loss同时降低。但是这个framework有个缺点，就是训练时间会上升一个数量级，同时label generator会比较难优化。感兴趣的同学可以自己尝试。但是不得不说，这篇文章有两个结论倒是很有意思： 假设 primary 和 auxiliary task 是在同一个 domain，那么 primary task 的 performance 会提高当且仅当 auxiliary task 的 complexity 高于 primary task。 假设 primary 和 auxiliary task 是在同一个 domain，那么 primary task 的最终 performance 只依赖于 complexity 最高的 auxiliary task。 结语给神经网络的黑盒子里面加入一些人为设定的先验知识，这样往往能给你的task带来一定程度的提升，不过具体的task需要加入什么样的先验知识，需要如何加入先验知识还需要自己探索。","link":"/2020/07/30/pri-knowledge-1/"},{"title":"About","text":"Billy A survey on Billy Abstract This paper detailedly shows who Billy is. Keywords: hardworking, passion, deeplearning, face-antispoofing, c/c++, python Introduction ​ Billy has updated himself in the fields of DeepLearning, Compute Vision, Face-Antispoofing through these years, and he will keep on going this way. Method ​ We can check this [website](https://zhengtq.github.io/) to observe his work. Conclusion ​ In this paper, we make a survey on who Billy is. And we believe that he will do better in future. Acknowledgement ​ Thanks to Billy’s family and friends along the way for their genuine love and support. Reference ［１］https://zhengtq.github.io/ ［２］https://github.com/Zhengtq","link":"/2020/04/13/aboutme/"},{"title":"&lt; Deeplearning &gt; TF实操Game of Noise","text":"前言 来自于2020年1月份的一篇arxiv文章.文章的主要思想是通过给CNN网络(以分类模型举例)的输入图加入噪声来使得模型更加的鲁棒． 与之前手动加入噪声不同的是，该文章采用对抗网络的思想，通过一个噪声生成器来生成噪声,并尽量使你的分类模型(判别模型)做出错误的分类． 而你的分类模型的目的是尽量能够不被加入的图片噪声干扰，依然能做做出正确的输出.最终经过数轮的迭代训练,达到使得你的分类模型能够抵抗各类噪声干扰的目的． 网络泛化与图像增强神经网络需要落地的最大困难往往在于训练数据和实际应用场景有较大的gap．经常是你的模型在训练集上拟合的很好，甚至在你的验证集/测试集上都拟合的很好，但是一部署到实际场景往往会出现各种各样的问题，如误检太多,召回太低等等．这会让你一度觉得你的人工只能简直是人工智障．以上统称模型的泛化能力差. 当然，这里面最大的问题在你的训练\\测试数据不够丰富，说就是你的训练集只是你训练素材真实分布的一个小小的sample．你在训练的时候只是用真实分布的一个sample，但你在线下应用的时候却要让模型面对整个真实分布(更加泛的sample)，这个时候模型如果遇到未知的数据，往往会作出错误判断． 当然我们不太可能获得真实分布那种海量的数据．于是人们就通过各种各样的方式来提升模型的泛化能力．有人通过修改模型的结构，比如Hinton老爷子发明的胶囊网络或dropout等等．有人通过调整优化方式或增加正则项来使得模型的最优值尽量落在平滑的极值点．有人通过调整训练方式，如增加多任务，或给网络添加各种先验等等．除了这些方式之外，更多的人是通过数据增强来达到让模型增加泛化能力的目的． 最初的图像增强是通过给图像加入各种手动的扰动来达到的．如随机裁剪，色彩变换等等．但这样的扰动方式有个很大的弊端，就是你必须手动调整各个增加的参数(如裁剪大小，色彩强度等等)．这是一个很主观且繁琐的事情．万一调试不好，反而会使得模型的准确率下降． 于是Google后来提出了AutoAugment，也就是自己定义了一个扰动的空间，通过强化学习的方式自动选择最优的扰动和参数．而本篇文章要说的Game of Noise，则是另外一种思路． 我们可以这样理解Game of Noise，假如说把扰动分类为两种，一种扰动是改变图像像素的位置，但是不改变图像像素的值(我们姑且成为方位扰动)，如速记裁剪，图像旋转，图像flip，仿射变换等等．另外一种扰动是在图像原始的像素值增加后减去某一个值，但是不改变像素的位置(我们姑且成为像素扰动)，如亮度增强，对比度增强，以及加入噪声等等，今天我们讨论的Game of Noise本质上是属于一种像素扰动．而Game of Noise可以通过对抗学习的方式去学习出最优的像素扰动,从而取代手动设置的像素扰动． Game of NoiseGame of Noise 这篇文章通过实验发现了几个有趣的现象。比如如果给图像加入高斯扰动，一个精心选择的噪声方差可以对模型效果有很好的提升（相对于随意噪声方差来说），比如通过对抗学习的方法来给图像加入噪声可以在免去手动调参的同时给模型加入最适合的扰动。而本文主要讨论和实验的是后者，也就是通多对抗学习的方式给模型加入噪声扰动。具体的实验方式与普通的对抗思想差异不大，由生成器生成对抗噪声，把噪声加入到训练图片上，再把加入噪声的图片送到你的判别器去判别和训练，这里的判别器就是你自己的需要扰动的模型，通常是一个已经训练好的模型，在对抗噪声图片上进行fintue．生成器的目的是尽量让加入噪声的图片让判别器误判，判别器的目的是尽量对加入噪声的图片正确区分。通过这样的对抗训练，最终让你的判别模型学习到抗未知噪声干扰的能力。 TF实现首先我们需要定义一个生成器: 123456789def noise_generator(x): with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, activation_fn=tf.nn.relu): x = slim.conv2d(x, 32, (1, 1), stride=1, scope='Conv1_advnoise') x = slim.conv2d(x, 64, (1, 1), stride=1, scope='Conv2_advnoise') x = slim.conv2d(x, 3, (1, 1), stride=1, activation_fn=None,normalizer_fn=None,scope='Conv3_advnoise') return x 原文中没加batch_norm,我后来实验发现加入bn还是效果会好一些,原文中前两层的channel都为20,我把channel数变大之后发现效果会好一些.下一步生成noise图像: 1234567g_noise_input = tf.random_normal(shape=(BATCH_SIZE_SEP, SIZE, SIZE, CHANNEL), mean=0.0, stddev=0.5, dtype=tf.float32)out_noise = noise_generator(g_noise_input)ran_sel = tf.random_uniform((BATCH_SIZE_SEP, 1, 1, 1), minval=0,maxval=2,dtype=tf.int32)ran_sel = tf.cast(ran_sel, tf.float32)out_noise = out_noise * ran_selnoise_image = images + out_noisenoise_image = tf.clip_by_value(noise_image, 0.0, 255.0) generator的输入是和输入图同样大小的高斯噪声(为了后面的相加操作)．给噪声加入ran_sel随机选择参数，目的是随机选择需要扰动的图片．获得图片的输出： 1out_logits = model.inference(noise_image, num_classes=LABEL_NUM) 定义生成器的loss： 123456reverse_label = tf.cast((1-labels), tf.float32)loss_softmax_sep_fake = LOSS_FUN(reverse_label, out_logits, ran_sel)tmp_noise = tf.reshape(out_noise, (BATCH_SIZE_SEP, 280*280*3))ord_loss = tf.norm(tmp_noise, ord=2, axis=-1)ord_loss = tf.sqrt(tf.pow(ord_loss - 2000, 2))/200ord_loss = tf.reduce_mean(ord_loss) 按照普通Gan中生成器的训练方式，需要把label取反，然后和输出logtis求交叉熵loss(最后要乘以ran_sel)．另外如果你要控制噪声的强度(防止noise的值过大)，需要对噪声加上一个2范数的限制，上面的ord_loss就是起到这样的一个作用.定义判别器的Loss： 1loss_softmax_sep = LOSS_FUN(label, out_logits, ran_sel) 判别器的loss很简单，就是没有reverse的label和输出logits的交叉熵．最后定义训练过程： 123456if step % N ==0: sess.run(tf.variables_initializer(G_variable))if ord_loss_out &gt; 0.1: _, ord_loss_out = sess.run([train_op_ord, ord_loss])_, G_loss_out = sess.run([train_op_G, train_loss_G])_, D_loss_out = sess.run([train_op_D, train_loss_D]) 训练就是普通的Gan的训练方式，也就是G和D交替优化．这里有个需要说明的trick是，最好在一定的迭代次数之后再次初始化G模型的变量，这样做是为了方式G生成的noise过于单一．","link":"/2020/03/20/advnoise/"},{"title":"&lt; Antispoofing &gt;　如何科学的攻破活体识别系统","text":"前言故事(事故)起源于我老大最近跟我说的一个脑洞，他说他最近看了Advhat 的相关介绍，说能不能在活体识别上面尝试一下Advhat, 仿照Advhat做一个特殊贴纸，贴到一张假脸的某个部位，然后就用这样的一张贴了特殊纸张的假脸攻破活体识别系统．我当时觉得应该不太可能，毕竟活体任务和人脸识别任务不同，活体识别任务学的就是真人和非真人介质的不同，无论生成什么样的纸张，毕竟逃离不了纸张透过摄像头的后的独特介质信息，况且即使生成了特殊的贴纸，然后再打印出来，然后再通过摄像头去识别，这样这张生成的’数字’贴纸就经过了两次失真，应该是没啥效果的．我就把我的观点和老大一说，老大说还是让我尝试一下．于是我就做个实验验证一下，没想到我被实验结果深深的打脸．．． 简单回顾AdvhatAdvhat 主要是基于Fast Gradient-Sign Method (FGSM)通过设计好的Loss来对输入进行梯度回传，然后把输入以一定的方式加上输入对应的梯度来实现梯度上升，从达到使之前的Loss上升，模型输出错误的目的. Advhat为了使得生成的贴纸在实际贴到脸上之后中更加的鲁棒，在合成贴纸人脸的时候做了三个个小的trick,第一个trick就是给矩形的贴纸做形态的变换，使得其达到在贴到额头上的弯曲程度．如下图所示: 第二个trick是使用了stn(Spatial Transformer NetworkSpatial Transformer Network),给贴到的人脸图像做位置和形态的细微扰动，使得其生成的贴纸到实际贴到额头上能够更加的鲁棒．第三个trick是作者使用了Total Variation Loss(TV_loss)让图像看起来更加的平滑自然．最终的流程图如下图所示: 其中A是原始的人脸，B是生成的贴纸，C是合成的贴纸人脸．cos_loss的目的是增大真脸的embedding和贴纸的脸的embedding的距离，从而达到模型误识别．grad1是cos_loss对图像C的梯度，grad2是图像C对贴纸B的梯度．grad3是tvloss对贴纸B的梯度，整个迭代过程中不会改变任何的’CLS _model’的参数，’CLS_model’只是为了计算loss,回传梯度．所以你可以把需改变的贴纸B当成weight，把’CLS_model’的所有weight当成常量参数．所以最终用于更新贴纸B的梯度为: 1grad_update = grad2*grad1 + alpha * grad3 其中alpha是控制两个梯度的比例银子． 活体实验过程所以活体的实验就是把上面示意图中的图像变成某一个攻击图片的图像，然后生成一个额头上mask，使得把这个mask贴到假体额头上可以把该假体攻击照片识别为真人．与advhat不一样的是，这个mask不需要为了适应额头的弯曲程度而矩形的形态变换，因为很多攻击都是平面的，我的初步实验也是在一个二维的裁剪纸张上面做的．为了表示方便，我下面会把额头戴mask的假体叫做mask假体，把对应的真实的假体叫做原始假体．总体示意图如下： 同样梯度更新的方式是一样的．只不过grad2不需要经过projection了．为了直观观察迭代情况，我在迭代的时候把mask经过活体模型的loss和置信度(Confidence)的值同样做了输出(在活体分类模型中，真人的标签是1，攻击的标签是0): 从上图可以看到，刚开始mask假体经过判别模型的置信度是一个很低的数值，也就是说迭代刚开始活体模型可以很好的判定mask假体就是一个假体．但随着迭代的增加，mask假体的置信度在fgsm迭代的作用下上升的很快，迭代到100步就已经可以达到99%置信度的水平了，换言之，该人脸(假脸)有以很大的置信度被活体模型识别为了真人．于是我打算看一下下生成的mask到底是啥样的，是不是类似于advhat那样的类似于人的额头的mask.于是我把不同迭代次数生成的mask保存下来，并看到了下面的东西(为了安全起见，我对图片做了黑白处理): 虽然到现在为止的实验结果显示额头贴纸可以很好的把一个攻击假体让神经网络识别为真人．但我还是很疑虑的觉得如果把贴纸打印出来然后贴到假体额头上可能会不起作用，毕竟模拟的mask和现实的mask还是有很大的不同的．现实的mask还要经过两次失真，一次是打印机打印的失真，一次是摄像头拍摄的失真．而活体模型学到的或许就是跟这两次失真有关的特征．于是我就去实际打印出来验证了一下，结果就像开头说的，我被深深的打脸，给攻击人脸贴纸确实是有攻入活体识别模型的可能性.放一下视频看一下效果(隐私起见，对眼睛做了马赛克处理，同时做了黑白处理)： (实在不知道怎么调整视频的长宽大小，建议放大网页看视频) 最后当然，虽然发现贴纸攻击对于活体识别模型是有效的，现在也大可不必担心活体识别的有效性．对于活体识别来说，贴纸攻击的有效性跟纸张方式选择，摄像头，打印机和光线角度都有关．我亲自试验过，同一张贴纸，我换一个环境或者是换一张人脸它就失效了. 而且，今天说的攻击方式本质是白盒攻击，必须要拿到活体模型的内部结构和参数才可以生成对应的贴纸，而想要从手机里面拿到完整的活体模型也是一件很困难的事情，所以大家大可不必过于担心现今FaceID的安全性．","link":"/2020/01/17/advAS/"},{"title":"&lt; Linux &gt; A Safe Way to Use Deletion Command In Linux.","text":"What is saferm?A safe deletion script to prevent you from wrongly deleting important files in your Linux system. How it works?Everytime when you delete a file, it actually move the file to a hidden folder named .saferm_xxx_yyy in the same location where xxx means the timestamp when you delete, yyy is a random number. This hidden folder will keep for some time(which is depended on your appointment) before it is truly deleted. So you can restore your deleted files within the keeping time of the file. How to use it?After you clone this project, there are two ways to use it: First way(If you are a root user)To put this two scripts in your /usr/bin/ folder, then to alias rm=’saferm’ to use it. Second wayTo put the two scripts in wherever you want, then to add the path to your PATH system environment in your .bashrc file. Finally to alias rm=’saferm’ to use it. The useful commandTo delete a file.1rm xxxThe file xxx will keep for a relatively long time(which is depended on your appointment) . To delete a file right now.1rm -now yyyThe file yyy will keep in a relatively short time(Which depends on your appointment). For safety concerns, to use to delete is restricted. If you want to use ‘rm ‘ to delete everything in your folder, you can use the following command.1rm -all * Personal CustomizationYou can decide how long you want to the deleted file to be kept. You can edit the ‘saferm’ file, then modify the ‘KEEP_TIME_LONG’ variable to the time(by second) you want to set.If you want to delete the file right now, you can set the variable ‘KEEP_TIME_INSTANT’ in the script ‘saferm’ to determine the short time you want to keep. Check the deletion historyA ‘.my_deletion.txt’ text will be established once you firstly use saferm. And the location of the text is in your home directory, so you can locate this text by ‘ls ~/.my_deletion.txt’. Everytime when you delete a file, a recored would be written in this text. And once that file is finally being deleted, the corresponded history will be removed also. Source CodeGithub","link":"/2019/10/19/saferm/"},{"title":"&lt; Linux &gt; A simple way to run program with mult-thread","text":"Applicable conditionsIf you want to deal with thousands of cases in a same way, you can use this method to accelerate the processing speed.For example, when you want to load millions of images and process them independently, you can apply this method which may accelerate your processing speed hugely, Firstly, just write a bash file12345678910111213141516171819start_num=0thread_num=8 #Number of threads you want applyall_num=100000 #Numbers of samples you want to deal withstep_num=$[all_num/thread_num]end_num=$step_numall_num=$[all_num+step_num]gpu_num=0current_thread=0echo $all_numcd ${your_directory}; # Locate your codewhile (($current_thread &lt; $thread_num)) do python demo.py $start_num $end_num &amp; #Your interface with parametersecho $start_num $end_num $gpu_numstart_num=$[start_num+step_num] end_num=$[start_num+step_num]current_thread=$[current_thread + 1]done In this code snippet above, I take running python script as an example. You can modify the ‘all_num’ parameter according to the number of samples you want to deal with. One important thing to mention is that ‘&amp;’ could make your program running parallely which is the most important keyword in this code snippet. Modify your codeYou should modify your code in order to fit the bash code above. One thing to note is that your main function should take at least two input variables. The below code snippet is a python version.: 1234567891011121314def main(): if len(sys.argv) == 3: begin_num = int(sys.argv[1]) end_num = int(sys.argv[2]) else: begin_num = 0 end_num = float(&quot;inf&quot;) for i, item in enumerate(all_items): if i &lt; begin_num: continue if i &gt;= end_num: break your_process_fun().... The below code snippet is a C/C++ version: 1234567891011121314int main(int argc, char** argv) { int begin_num = 0, end_num = INT_MAX; if (argc == 3) { begin_num = atoi(argv[1]); end_num = atoi(argv[2]); } int count_num = 0; while (Condition) { count_num++; if (count_num &lt; begin_num) continue; if (count_num &gt; end_num) break; your_process_fun().... } }","link":"/2019/07/27/linux-multi-thread/"},{"title":"&lt; Deeplarning &gt; Understand Backpropagation of RNN&#x2F;GRU and Implement It in Pure Python---1","text":"Understanding GRUAs we know, RNN has the disadvantage of gradient vanishing(and gradient exploding). GRU/LSTM is invented to prevent gradient vanishing, because more early information could be encoded in the late steps. Just similarly to residual structure in CNN, GRU/LSTM could be treated as a RNN model with residual block. That is to say that former hidden states is identically added to the newly computed hidden state(with a gate).For more details about GRU structure, you could check my former blog. Forward Propagation of GRUIn this section, I will implement my pure python version of GRU forward propagation. Talk is cheap, show you the code: 12345678910111213141516171819def forward(self, x, prev_s, W,Wb,C,Cb, V, Vb): self.x_prev_s = np.concatenate((x, prev_s), axis = 0) self.hidden_mum = len(prev_s) self.mulw = mulGate.forward(W, Wb,self.x_prev_s) self.mulwsig = sig_act.forward(self.mulw) self.r = self.mulwsig[:len(self.mulwsig)/2] self.u = self.mulwsig[len(self.mulwsig)/2:] self.r_state = eltwise_mul.forward(self.r, prev_s) self.x_r_state = np.concatenate((x, self.r_state), axis = 0) self.x_r_state_mulc =mulGate.forward(C,Cb, self.x_r_state) self.x_r_state_mulc_tan = tan_act.forward(self.x_r_state_mulc) self.tmpadd1 = eltwise_mul.forward(self.u, prev_s) self.u_fu = 1 - self.u self.tmpadd2 = eltwise_mul.forward(self.u_fu, self.x_r_state_mulc_tan) self.s = addGate.forward(self.tmpadd1, self.tmpadd2) self.mulv = mulGate.forward(V,Vb, self.s) As you can see, we firstly concatenate ‘hpre’ and ‘x’ together, and then multiply with weight ‘W’(Equal to ‘hpre’ and ‘x’ separately multiply with its weight and then add together). Variable ‘r’ and ‘u’ are two gates. Gate ‘r’ controls how much hidden state could be mixed with ‘x’. Gate ‘u’ controls how much hidden state information could be directly flow to the next state. Back Propagation of GRUAs we know, RNN uses back propagation through time(PBTT) to compute gradients for each time step. PBTT means the gradients should flow reversely through time. The bellow code snippet shows how the gradients flow inside the GRU structure at each time step: 12345678910111213141516171819202122def backward(self, x, prev_s, W, Wb, C, Cb, V, Vb,diff_s, dmulv): dV,dVb, dsv = mulGate.backward(V,Vb, self.s, dmulv) ds = dsv + diff_s dtmpadd1, dtmpadd2 = addGate.backward(self.tmpadd1, self.tmpadd2, ds) du_fu, dx_r_state_mulc_tan = eltwise_mul.backward(self.u_fu, self.x_r_state_mulc_tan, dtmpadd2) du1 = -du_fu du2, dprev_s0 = eltwise_mul.backward(self.u, prev_s, dtmpadd1) du = du1 + du2 dx_r_state_mulc = tan_act.backward(self.x_r_state_mulc, dx_r_state_mulc_tan) dC,dCb, dx_x_r_state = mulGate.backward(C,Cb, self.x_r_state, dx_r_state_mulc) dx = dx_x_r_state[:len(x)] dr_state = dx_x_r_state[len(x):] dr, dprev_s1 = eltwise_mul.backward(self.r, prev_s, dr_state) dmulwsig = np.concatenate((dr, du), axis = 0) dmulw = sig_act.backward(self.mulw, dmulwsig) dW,dWb, dx_prev_s = mulGate.backward(W, Wb, self.x_prev_s, dmulw) dprev_s = dx_prev_s[-self.hidden_mum:] + dprev_s1 + dprev_s0 return (dprev_s, dW,dWb, dC,dCb, dV,dVb) Here I have admit that to write a back propagation algorithm is a fussy thing, for you should carefully compute the gradients flowing through each operations without any error. There are some notes for you. Firstly, while hidden state is used three times in the forward propagation(First branch: compute gate ‘r’ and gate ‘u’. Second branch: to mix information with input ‘x’. Third branch, identically add to the final hidden state), so you should add each gradients of hidden state computed from each branch**(A way to prevent gradient vanishing)**. Second thing to remember, feed your computed final gradients of hidden state to your last time step(or last layer of your model).","link":"/2019/05/15/gru-bp1/"},{"title":"&lt;Deeplarning&gt; A simple way to distinguish different optimizers in DeepLearning","text":"The optimizersLet’s simply list the optimizers that may or may not be used in your project. SGDOptimizer MomentumOptimizer NesterovOptimizer AdagradOptimizer AdadeltaOptimizer RMSPropOptimizer AdamOptimizer NadamOptimizer Four type of these optimizers We can categorize these optimizers into Four types: Type1: Base type: SGDOptimizer Type2: Add Momentum to gradient(Base on the historical gradient): MomentumOptimizer, AdamOptimizer, NadamOptimizer Type3: Change LearningRate accordingly(According to the changing speed(second derivative of the gradient) of each variable): AdagradOptimizer, Adadelta, RMSPropOptimizer, NadamOptimizer Type4: Change direction of the gradient(Weight firstly update by the historical gradient, and then compute the new gradient): NesterovOptimizer, NadamOptimizer Let make some equations SGDOptimizer = T1 MomentumOptimizer = T1 + T2 NesterovOptimizer = T1 + T4 AdagradOptimizer = T1 + T3 Adadelta = T1 + T3(Add window on the squared gradient) RMSPropOptimizer = T1 + T3(Add window on the squared gradient) AdamOptimizer = T1 + T3(Add window on the squared gradient. And add window on the gradient(momentum) itself) NadamOptimizer = T1 + T3 + T4","link":"/2019/04/24/op-summary/"},{"title":"&lt; Deeplearning &gt; Use CNN and RNN to detect blink in a video","text":"Let’s do it!If you use your face to pay a sum of money by ALIPAY, you may find that it sometimes requires you to do some facial movement to check whether you are a real person or not. So as you see, facial motion detection is actually being used in many circumstances. You may want to figure out how to detect facial motion. In this post, I will take motion blink as a example to demonstrate how to do realize it. So Let’s do it. How to ‘make’ data if you do not haveOf course we can use thousands of blinking video sequences to train a deep learning classifier. But it may be difficult and inefficient to collect enough blink videos. So Can we simulate blinking data by using single eye image. As we know, a blink sequence contains three eye movements: opening eyes, closing eyes and opening eyes again. We can modeling blink motion as a sequence of eyes opening and eyes closing. Even you have just 100 eye opening images and 100 eye closing images, you can also create millions of eye blinking sequences. Quite surprising! Build your blinking detection system ?It seems is over. The last step is to use a CNN model(may be a 3D CNN model) to train a blink detection model by using the above fake training sequences. If you have read my former post before, you may understand how GRU works in Tensorflow. So we will start from here to demonstrate how to detect blink based by using GRU. Of course you can use Conv-Gru to simply use raw eye image sequences to get a end-to-end blink detection system. However if you want to make this task simpler, you should firstly train a CNN binary classifier to classify if eye close or not. And next you should use the classifier as a feature extractor to extract feature of eye and then form a sequence as the input of your RNN network. To make the system more accurate.In order to make your blink system more accurate, to give the input features to RNN more diversity is very important. So you have to firstly make sure you have trained a good eye feature extractor. You can use facial landmarks(from any open source code) to get the landmarks of the face image and extract the eye part of the face image. One thing to be mentioned, you could use landmark points itself as parts features(combined with the feature extracted from you CNN classifier) to make the feature more robust. Next, you have to figure out how to simulate a good feature sequence to train your RNN model. You can create many hard cases and should not neglect any blinking circumstances. One thing to be remembered, the more diverse your training sequences are, the more accurate your blink system is. Is not over?So now you have a simple eye blinking detection system. But there are many other questions should be solved. There first question is how many frames could be used to detection blink. The answer is quite simple: as many as you like. So in order to do this, you can train a RNN model based on flexible length sequence(You have to define the max sequence length). Another question is how to design your actual system logic when you apply your model on real time video stream. And you can find it by yourself.","link":"/2019/04/19/motion-blink-rnn/"},{"title":"&lt; Network &gt; Understanding DepthWiseConv","text":"Some knowledge about DepthWiseConvAs we all know, people invent DepthWiseConv for more efficient computation cost without accuracy loss. DepthwiseConv coupled with PointWiseConv could be equal to normal convlution. DepthWiseConv means that we assign a single kernel to each feature channel. PointWiseConv is a normal convolution kernel with kernel size 1 and stride 1. DepthWiseConv aims to extract the spatial feature of a feature map, while PointWiseConv aims to extracte the correlation of different feature maps. What pre-feature should be fed to DeptWiseConvIn this section, we would discuss what pre-feature should be fed to DeptWiseConv. Of course we can feed normal feature from normal convolution layers. But people find it may be more efficient if we firstly expand the channels of the pre-feature and then feed them to the DepthWiseConv. Actually this is what mobilenet_v2 just did in their network structure which is called ‘inverted bottleneck’. We can treat the expansion of the pre-feature as a process of feature uncompression. Why we should uncompress feature before depthwise? From my point of view, if we want to extract more useful spatial information using DepthWiseConv, we should firstly extract more correlated information among pre-features. So there is reason why we want to expand the channels of the pre-feature.","link":"/2019/03/29/network-depthwise/"},{"title":"&lt; Tensorflow &gt;How dose TensorFlow do Quant Aware Training?","text":"Let firstly simplify the Quant process in TFOverview1S_a1(q_a1 + Z_a1) = S_w1(q_w1 + Z_w1) * S_a0(q_a0 + Z_a0) q_a1: Quanted activation value in layer 1 S_a1, Z_a1: Estimated scale and zero point in layer 1 q_w1: Quanted weight in layer 1 S_w1, Z_w1: Statistical scale and zero point in layer 1 q_a0: Quanted activation value in layer 0 S_a0, Z_a0: Estimated scale and zero point in layer 0 As we can see, in order to compute q_a1(Quanted activation value in layer 1), we have to get S_w1, Z_w1, S_a0, Z_a0, q_a1, Z_a1. To get S_w1/Z_w1 is simple, we can get the Statistical maximum of the weights in each layer we want. The only tricky thing is how to get S_a1/Z_a1/S_a0/Z_a0, which have to be estimated from the training data. Why we have to get the estimation of S/Z after activation instead of before?Of course we can estimate the S/Z before activation. And then we activate(Relu) the quantized value. However, there is one drawbacks: Estimate S/Z before activation 1conv -&gt; (before activation)estimate S/Z=[0-&gt;255] -&gt; truncation（relu）= [Z-&gt;255] (if relu6 : [Z-&gt;X]) Estimate S/Z after activation 1conv -&gt; (after activation)estimate S/Z=[-A-&gt;A] -&gt; truncation（relu）= [0-&gt;255](if relu6: [0-&gt;255]) As we can see, after activation, the range of activation value for estimating S/Z is always 0 to 255. However, when we estimate S/Z before activation, the range of activation value for estimating S/Z is narrow than 0 to 255. So if we estimate S/Z before activation, the quantized activated is compressed even wore which could lead to accuracy loss. Quantization aware training in TensorflowYou can either train your quantized model by restoring a ever trained floating point model or from scratch. In any cases, you have to create a quantization training graph first. 1tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP) The DELAY_STEP means the number of steps that you want your normal floating point training sustain. So after the DELAY_STEP of normal training, the quantization aware training would be started. If you use multi-GPU to training your network, you have to create a new quantization graph on every GPU card. Just like these code below: 123456789101112with tf.variable_scope(tf.get_variable_scope()): for i in xrange(len(GPU_NUM_ID)): with tf.device('/gpu:%d' % GPU_NUM_ID[i]): with tf.name_scope('%s_%d' % ('cnn_mg', i)) as scope: images, abels = load_batch_images() logits, out_data = net.inference(images, reuse=tf.AUTO_REUSE, num_classes=LABEL_NUM) with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE): tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP) loss = conpute_loss(labels, logits) tf.get_variable_scope().reuse_variables() grads = optimizer.compute_gradients(loss_total_sep) tower_grads.append(grads) One thing I have to mention is that the quantized aware training process is fake training. Fake training means that during the forward step, the training graph just simulate the integer multiply by using corresponding floating point multiply. The word ‘Corresponding’ means that the simulated float weights are the inverse quantization value of the corresponding fixed integer. So the forward result may be slightly different from the actual quantization computed result. Save, Frozen, Convert and TestSaveWhen finishing quantization aware training, you have to save your trained quantized model. To save your quantized model, you have to create a quantized evaluation graph by using the following code: 12g = tf.get_default_graph()tf.contrib.quantize.create_eval_graph(input_graph=g) Then just get the graph and save it. 12with open('./your_quantized_graph.pb', 'w') as f: f.write(str(g.as_graph_def()))\\ FrozenTo make your model more compact, you can froze your model. Frozen a model means that getting rid of useless operations and fusing redundant operations. To froze your graph, you can use the standard frozen tool. 12345bazel build tensorflow/python/tools:freeze_graph &amp;&amp; \\bazel-bin/tensorflow/python/tools/freeze_graph \\--input_graph=some_graph_def.pb \\--input_checkpoint=model.ckpt-8361242 \\--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax ConvertThe next step is to convert your frozen graph to tflite for future deploy. 123456789101112path_to_frozen_graphdef_pb = './your_frozen_graph.pb'input_shapes = {'validate_input/imgs':[1,320,320,3]}(tf_verion&gt;1.11)converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(path_to_frozen_graphdef_pb, ['validate_input/imgs'], ['output_node'])(tf_version&lt;=1.11)converter = tf.contrib.lite.TocoConverter.from_frozen_graph(path_to_frozen_graphdef_pb, ['validate_input/imgs'], ['output_node'])converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8converter.quantized_input_stats = {'validate_input/imgs':(0.,1.)}converter.allow_custom_ops = Trueconverter.default_ranges_stats = (0,255)converter.post_training_quantize = Truetflite_model = converter.convert()open(&quot;sfnv2.tflite&quot;, &quot;wb&quot;).write(tflite_model) TestFinally, your can test your converted tflite. By using the following code, you can test your quantized model: 1234567891011interpreter = tf.contrib.lite.Interpreter(model_path=&quot;your.tflite&quot;) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() interpreter.set_tensor(input_details[0]['index'], batch_validate_img)interpreter.invoke()score = interpreter.get_tensor(output_details[0]['index'])score = score[0][0]zero_point = xxxscale = xxxreverse_socre = scale * (score - zero_point) One thing to mention is that the final score you get is a fixed point integer value. You have to convert the fixed point integer value to the corresponding float value. In order to do that, you have to check the corresponding zero point and scale in the corresponding output layer and then you can transfer the fixed point integer value to get the final float value.","link":"/2019/03/25/tf-quant/"},{"title":"&lt; Antispoofing &gt; Multi-Task-Learning in Face Antispoofing","text":"Is Face Antispoofing only a binary classification task?Of course, we can consider face antispoofing as a binary classification task. We can train a classifier to distinguish a face image between liveness and fake. It may work well but it can not fully use all information of the input face image. In order to push the limit of our trained data and trained classifier, we have to cultivate other information that could help us to better discriminate an attack face. So what information could cultivated from an image beyond its label? The depth information of a face maybe helpful. The fake images which normally are paper, screen do not contain any depth information while the real face contains depth information. So we can use this extra depth information to separate the real face from fake face. Multi-Task-Learning in Face antispoofingMuti-Task-Learning means that a model could achieve its aim by learning other task simultaneously. Specially in deep learning, we could combine multiple loss together to train a deep model. More specially in face antispoofing task, we could combine the depth loss and classification loss together to get a more generalized model. To picture below shows the main process: As for how to combine the classification loss and depth loss, you could try many ways(to add together or to optimize separately).May this could help bring you some insight in your task.","link":"/2019/03/16/antispoof-multi-task-learn/"},{"title":"&lt; DeepLearning &gt; The Support Vectors in DeepLearning","text":"Dose DeepLearning has ‘Support Vectors’?As we all know, support vectors is a notation in SVM(Support Vector Machine). Support vectors means that the data points in the decision boundary(the Maximum Margin in SVM) are very important in a classification algorithm. We can train a SVM only with the ‘Support Vectors’ and can achieve the same accuracy with models trained with more data. So does the ‘Support Vectors’ exists in DeepLearning? Yes! It dose! Many experiments have be made to testify that there may be many redundant data in your training dataset. Deep models could reach to similar accuracy with a few ‘IMPORTANT’ data points which could be seen as the ‘Support Vectors’. So, the only question is how to determine which training data point is important and how to define the importance of a data points? How to determine which training data points is more IMPORTANT?**In a word, the sample(with a right label) that is hard to be classified by your model is more important. **So we have to make sure how hard it is. There are three ways which from three papers: An Empirical Study of Example Forgetting during Deep Neural Network Learning Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J, 2018 This paper finds that the data points that are easily forgotten during the process of training is more likely to be important. One sample to be forgotten during training means that the sample once be to right classified would be falsely classified in future training iterations. One sample to that is unforgotten during training means that the once the sample is right classificed would never be falsely classified during the training iteration. This paper demonstrates that by removing the unforgotten data points, your model could maintain a similar testing accuracy compare to be trained with whole training data. Dataset Culling: Towards Efficient Training Of Distillation-Based Domain Specific Models Yoshioka, Kentaro and Lee, Edward and Wong, Simon and Horowitz, Mark, 2019 To determain which training sample is more importan, this paper designs a loss. To evaluate how difficult an image is to predict, we develop a confidence loss metric. This loss (shown below) uses the model’s output confidence levels to determine whether data samples are 1) difficult-to-predict and kept or 2) easy and culled away. Lconf = −x logx * Q + (1 − x) *expx/( expx + 1) + b Input x is the prediction confidence, b is a constant to set theintercept to zero, and Q sets the weighting of low-confidencepredictions. Are All Training Examples Created Equal? An Empirical Study Yoshioka, Kentaro and Lee, Edward and Wong, Simon and Horowitz, Mark, 2019 This paper demonstrates that the importance of a training sample could be measured by computing gradient of the sample during back propagation. The sample with large gradients means that the sample is more important.","link":"/2019/03/06/dataset-cut/"},{"title":"&lt; Antispoofing &gt; Data Augmentation in Face Antispoofing","text":"How dose people spoof a digital device with face recognitionTo attack a image recognition system is easy. Even to change a single pixel could successfully let the recognition system lose efficiency. So people would use the loophole to simulate a set of fake images to spoof the face recognition system. So in order to defense the spoofing, we should simulate all the possible attacking conditions in the real life with limited training dataset. Ways to simulate the spoofing conditions through data augmentationThe following data augmentation manner could simulate parts of the attacking condition. light condition image color face location image noise background proportion","link":"/2019/02/01/face-as-1/"},{"title":"&lt; Antispoofing &gt; Does we should align face in Face Antispoofing","text":"Align or not, Texture or shapeRelationship with Face Recognition(FR)The aim of Face Recognition is to shorten the distance of one person from different scene, even from different medium. For example, FR should shorten the distance between the person in real life and the person’s picture in paper or tv or phone. However, Face Antispoofing(FA) just did the opposite thing. FA should shorten the person from the same medium. FA should shroten the distance between the two different real persons. And FA should widen the distance of the people from different medium, even the two persons are the same.So as we can see, to differentiate medium is the key of FA. What does the convlutional neural networks learn?As we all know, FR should grab the features from two persons’ faces and differentiate them. The face texture(skin) a person can not be treated as a key feature because one person’s face texture may be different under different conditions. So FR should differentiate two persons’ face by their faces’ shape(My opinion). Sadly, convlutional neural networks are good at grabbing the feature of image textrue, and do very poorly at grabbing the feature of shape. So strictly aligment of the face is very import in FR. For FR, the aim of face alignment is to let the deep network to learn the difference of the face shape regardless of the face texture. On the opposite, FA mainly learns the feature of texture of the medium regardless of the shape of ons’s face, which could take the advantage of learning capcity of the convlutional neural networks. So alignment is not important in FA.","link":"/2019/01/30/face-as/"},{"title":"&lt; Tensorflow &gt; Data flow in Tensorflow","text":"Using data queue to read data(abandoned, not support in future Tensorflow version)If you store you data in traditional hard disk, I suggest you read data through the data format of tfrecord. However, if you store your data in solid state drives , I suggest that you read by FIFO Queue to directly read image by its root.Firstly, you have to set up a FIFO Queue: 12345input_queue = data_flow_ops.FIFOQueue(capacity=3000000, dtypes=[tf.string, tf.int64], shapes=[(1,), (1,)], shared_name=None, name=None) The queue above contain two values. The first is image root, and the second is image label.To load image roots and labels to the FIFO Queue, you have to use the a enqueue_op. 1enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder], name='enqueue_op') Next, you should open up a session to feed your image roots and labels to respective placeholder. 1sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array}) During training, you have to dequeue the FIFO Queue and use multi-thread to accelerate the process. 1234567891011images_and_labels_list = []for _ in range(preprocess_threads): filenames, label= input_queue.dequeue() images = [] image_depths = [] for filename, single_label in zip(tf.unstack(filenames), tf.unstack(label)): file_contents = tf.read_file(filename) image = tf.image.decode_bmp(file_contents, channels=3) image = tf.reshape(image, original_img_shape) images.append(image) images_and_labels_list.append([images, label]) Finally, to create batches of examples, you should use “tf.train.batch_join” api. 12345img_batch, img_depth_batch,label_batch = tf.train.batch_join( images_and_labels_list, batch_size=batch_size, shapes=[[to_height, to_width, channels], ()], enqueue_many=True, capacity=4 * preprocess_threads * 100, allow_smaller_final_batch=True) Using tf.data APITensorflow officially promote using tf.data API for data processing. Using tf.data API is relatively easy to coding. 1234567891011121314151617 def _parse_data(line): file_contents = tf.read_file(image_filepath)image = tf.image.decode_bmp(file_contents, channels=3) image = tf.reshape(image, original_img_shape) return image, single_label dataset = tf.data.TextLineDataset([file_root]) dataset = dataset.map(map_func=_parse_data, num_parallel_calls=4) dataset = dataset.shuffle(buffer_size=batch_size * 3) dataset = dataset.batch(batch_size) dataset = dataset.repeat(epoc) dataset = dataset.prefetch(2000) data_iterator = dataset.make_one_shot_iterator() return data_iterator To accelerate the reading process, do not forget to use dataset.prefetch and num_parallel_calls.","link":"/2019/01/09/tf-read-data/"},{"title":"&lt; Tensorflow &gt; What is the implementation of GRU in tensorflow","text":"What’s the implementation of GRU cell in tensorflow?We can use a chart to demonstrate the GRU cell implementation in Tensorflow, and let’s take a two cells GRU for example: The chart above shows how a two-cells GRU network process sequences at time t and time t+1 in Tensorflow. x_t is the input sequence at time t. h(0)_t-1 is the hidden state of cell zero at time t-1, while h(1)_t is the hidden state of cell one at time t. y_t is the final output of the GRU network at time t. So as you can see, we have to maintain two variables h(0)_t-1 and h(1)_h-1 in order to proceed the two cells gru network.So at the end of each time step, we should copy the memory of h_now to h_prev for every cell. The inner structure of a Gru cell can be demonstrate as the picture below: In this chart, “c” is concatenation. “w/b” demonstrate for the inner product with learned weights and add bias. “sig” means sigmoid operation.","link":"/2019/01/05/tf-rnn-gru/"},{"title":"&lt; Tensorflow &gt; Softmax cross entropy &amp; Sigmoid cross entropy","text":"How differenceFor multi-class classification, if you want optimize only one category during training, you should use SOFTMAX cross entropy. Otherwise, if you want to optimize more than one category, you should use SIGMOID cross entropy. The code below could demonstrate 123456789101112131415161718192021222324252627282930313233from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport tensorflow as tfimport numpy as npnp.set_printoptions(precision=4, suppress=True)def fun(): x = tf.constant([7, 6, -4], tf.float64) x_sig = tf.nn.sigmoid(x) z = tf.constant([1,1,0], tf.float64) loss_1 = tf.reduce_sum(z * -tf.log(x_sig) + (1 - z) * -tf.log(1 - x_sig)) loss_2 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=z)) logits = tf.constant([[3, -4], [4, -2], [-2, 2]], tf.float64) y = tf.nn.sigmoid(logits) y_ = tf.constant([[1, 0], [1, 0], [0, 1]], tf.float64) loss_3 = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = y_)) softmax = tf.nn.softmax(logits) loss_4 = -tf.reduce_sum(y_ * tf.log(softmax)) loss_5 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)) with tf.Session() as sess: print('loss1:', sess.run(loss_1)) print('loss2:', sess.run(loss_2)) print('loss3:', sess.run(loss_3)) print('loss4:', sess.run(loss_4)) print('loss5:', sess.run(loss_5))fun() The results are as follows: 123456loss1: 0.021537079509314265loss2: 0.02153707950931443loss3: 0.465671240538279loss4: 0.021537079509314265loss5: 0.021537079509314338 loss1 is the result which is computed through naive implementation of sigmoid cross entropy. loss2 is the result which is computed through the standard sigmoid cross entropy in tensorflow API. loss4 is the result which is computed through naive implementation of softmax cross entropy. loss5 is the result which is computed through the standard sigmoid cross entropy in tensorflow API. As you can see, the result of sigmoid cross entropy and softmax cross entropy are the same. This is mainly because sigmoid could be seen a special case of sofmax. To sigmoid one number could equal to softmax two number which could sum to that number.","link":"/2019/01/03/softmax-sigmoid-cross-entropy/"},{"title":"&lt; Tensorflow &gt; How to implement the larget margin softmax loss in tensorflow.","text":"There is my implementation of the Large Margin Softmax Loss in TF. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081MARGIN = 4c_m_n = lambda m, n: math.factorial(n) / math.factorial(m) / math.factorial(n-m)c_map = []for i in range(MARGIN+1): c_map.append(c_m_n(i, MARGIN))def find_k_multi(cos_t): cos_t = tf.cast(cos_t, dtype = tf.float64) i = tf.constant(0, dtype = tf.float64) def l_cond(i): tf_pi = tf.constant(math.pi ,tf.float64) left_range = tf.cast(tf.cos(tf_pi*(i+1)/MARGIN), dtype = tf.float64) right_range = tf.cast(tf.cos(tf_pi*i/MARGIN), dtype = tf.float64) logic = tf.logical_or(tf.greater(cos_t, right_range), tf.less(cos_t, left_range)) return logic def l_body(i): return tf.add(i, 1) k = tf.while_loop(l_cond, l_body, [i]) k = tf.cast(k, dtype = tf.int32) return k, k def L_softmax(feature, labels, softmax_w, softmax_b , batch_size, labmda = 100): f_feature = [] l_softmax_loss = [] c_v = [] o_v = [] for sample_index in range(batch_size): sample_feature = tf.slice(feature, [sample_index, 0], [1, 512]) label = tf.squeeze(tf.slice(labels, [sample_index],[1])) label = tf.cast(label, tf.int32) w_label = tf.slice(softmax_w, [0, label], [512, 1]) b_label = tf.squeeze(tf.slice(softmax_b, [label], [1])) wx = tf.squeeze(tf.matmul(sample_feature, w_label)) w_label_mod = tf.sqrt(2 * tf.nn.l2_loss(w_label)) b_label_mod = tf.sqrt(2 * tf.nn.l2_loss(tf.squeeze(b_label))) sample_feature_mod = tf.sqrt(2 * tf.nn.l2_loss(tf.squeeze(sample_feature))) cos_theta = wx/(w_label_mod * sample_feature_mod) sin_theta_t = 1 - cos_theta * cos_theta cos_m_theta = 0 flag = -1 for loop in range(int(MARGIN/2) + 1): flag *= -1 cos_m_theta += flag * c_map[2 * loop] * tf.pow(cos_theta, MARGIN - 2 *loop) * tf.pow(sin_theta_t, loop) k, log = find_k_multi(cos_theta) phi_theta = tf.to_float(tf.pow(tf.constant(-1), k)) * cos_m_theta - 2 * tf.to_float(k) label_value = (labmda * (w_label_mod * sample_feature_mod * cos_theta + b_label_mod) + \\ (w_label_mod * sample_feature_mod * phi_theta + b_label_mod))/(1.0 + labmda) # label_value = tf.to_float(tf.pow(-1, k)) * w_label_mod * sample_feature_mod * cos_m_theta - \\ # 2 * tf.to_float(k) * w_label_mod * sample_feature_mod other_value = tf.squeeze(tf.matmul(sample_feature, softmax_w) + softmax_b) old_value = tf.slice(other_value, [label], [1]) to_substract_1 = tf.sparse_to_dense(label, other_value.get_shape(), old_value, default_value=0.) to_substract_2 = tf.sparse_to_dense(label, other_value.get_shape(), label_value, default_value=0.) l_feature = other_value - to_substract_1 + to_substract_2 f_feature.append(l_feature) c_v.append(cos_theta) o_v.append(k) out_value = [c_v, o_v] all_batch_feature = tf.stack(f_feature) return all_batch_feature, out_value","link":"/2018/12/30/tf-lsoftmax/"},{"title":"&lt; Tensorflow &gt;Tensorflow Data Augmentation RGB distortation","text":"This method could augment images by disturbing the RGB value of a image randomly. 12345678910111213141516171819202122232425 def all_color_channel_tur(image): color_range = 30 def add_color_mask_test(image): img_shape = image.get_shape().as_list() img_shape = [img_shape[0], img_shape[1], 1] ran_color_range_0 = tf.random_uniform([], minval=-color_range, maxval=color_range,dtype=tf.float32) color_mask_0 = tf.ones(img_shape, dtype = tf.float32) * ran_color_range_0 ran_color_range_1 = tf.random_uniform([], minval=-color_range, maxval=color_range,dtype=tf.float32) color_mask_1 = tf.ones(img_shape, dtype = tf.float32) * ran_color_range_1 ran_color_range_2 = tf.random_uniform([], minval=-color_range, maxval=color_range,dtype=tf.float32) color_mask_2 = tf.ones(img_shape, dtype = tf.float32) * ran_color_range_2 sep_channels = tf.split(image, 3, 2) sep_channels[0] = tf.add(sep_channels[0], color_mask_0) sep_channels[1] = tf.add(sep_channels[1], color_mask_1) sep_channels[2] = tf.add(sep_channels[2], color_mask_2) image = tf.concat(sep_channels,2) return image# ran = tf.random_uniform([]) # image = tf.cond(ran&lt;0.5, lambda:add_color_mask_test(image), lambda: image) image = add_color_mask_test(image) return image The images below are original and transfered images:","link":"/2018/12/22/tf-rgb-disturb/"},{"title":"&lt; Tensorflow &gt;Tensorflow Data Augmentation affine transformation","text":"MethodThis method augment image by changing image through affine transformation. 123456789101112131415161718192021222324252627282930def transform_perspective(image): def x_y_1(): x = tf.random_uniform([], minval=-0.3, maxval=-0.15) y = tf.random_uniform([], minval=-0.3, maxval=-0.15) return x, y def x_y_2(): x = tf.random_uniform([], minval=0.15, maxval=0.3) y = tf.random_uniform([], minval=0.15, maxval=0.3) return x, y def trans(image): ran = tf.random_uniform([]) x = tf.random_uniform([], minval=-0.3, maxval=0.3) x_com = tf.random_uniform([], minval=1-x-0.1, maxval=1-x+0.1) y = tf.random_uniform([], minval=-0.3, maxval=0.3) y_com = tf.random_uniform([], minval=1-y-0.1, maxval=1-y+0.1) transforms = [x_com, x,0,y,y_com,0,0.00,0] ran = tf.random_uniform([]) image = tf.cond(ran&lt;0.5, lambda:tf.contrib.image.transform(image,transforms,interpolation='NEAREST', name=None), lambda:tf.contrib.image.transform(image,transforms,interpolation='BILINEAR', name=None)) return image ran = tf.random_uniform([]) image = tf.cond(ran&lt;1, lambda: trans(image), lambda:image) return image This images below are origin image and augmented images:","link":"/2018/12/20/tf-tur-perspective-transform/"},{"title":"&lt; Network &gt; Understanding the activation style in residual block","text":"The evolution of the Activation Style in Residual BlocksThe Activation Style is different between different residual structures. Resnet_v1After shortcut: ActivationAfter residual : No ActivationAfter add: Activation Resnet_v2After shortcut: No ActivationAfter residual: No ActivationAfter add: Activation Mobilenet_v2After shortcut: No ActivationAfter residual: No ActivationAfter add: No Activation The thought behind the changing of Activation Style in Residual BlocksThe main novelty of the residual structure is to add the shortcut and the residual together. In the original version of Resnet，Activation is placed before and after the additive action for nonlinear transformation. However, the additive action is also a special kind of nonlinear transformation for both shortcut and residual. So there is no need to use activation before and after additive action. On the other hand, if we use the common “Relu” for activation after both residual and shortcut, then more useful information could be lost because the “Relu” function is a truncation function. So I think it is wisely to avoid using the activation before and after the additive action in the residual structure.","link":"/2018/12/14/newwork/"},{"title":"&lt; Deeplearning &gt; Break up backpropagation","text":"Let’s see a very simple handwriting formula derivationDefineFirstly, let define some variables and operations Gradient of the variable in layer L(last layer)dWL = dLoss * aL Gradient of the variable in layer L-1dW(L-1) = dLoss * WL * dF(L-1) * a(L-1) Gradient of the variable in layer L-2 dW(L-2) = dLoss * WL * dF(L-1) * a(L-1) * W(L-1) * dF(L-2) * a(L-2) SummarySo, as we can see, the gradients of any trained variables only depends on the following three items: The trained variable itself. The derivative of the activation value from this layer. The activated value from the front layer. Relations with gradient vanishing or explodingThe following cases may cause Gradient Exploding Training variables are larger than 1 The derivative of activation function are larger than 1 The the activated value are larger than 1. The following cases may cause Gradient Vanishing Training variables are smaller than 1 The derivative of activation function are smaller than 1, The the activated value are smaller than 1. To prevent graident vanishing or explodingFrom the view of training variablesTo limit the trained variables into a proper range. We should use a proper variable initialization method, such as xavier initialization. From the view of derivative of activation functionTo limit derivative of activation function to a proper range, we should use non-saturated activation function such as Relu, instead of sigmoid From the view of activated valueTo limit the activation value in to proper range, we should use BatchNorm to make the activated value into a zero centered and variance to one. From the view of model structureTo futher enhance the gradient to the deeper layer, we should use residual block to construct our network.","link":"/2018/12/09/bp-simple/"},{"title":"&lt; Python &gt; Some python tricks you may never use But you should know","text":"namedtupleIf you are too lazy to create a class but you still want to use a variable that can act as a class object, then you should use namedtuple: 1234from collections import namedtupleuser = namedtuple('u', [1, 2, 3])ztq = user(a='a', b='b',c='c')print(ztq.a, ztq.b, ztq.c) result: 1(1,2,3) decoratorIf you are too lazy to change a function from inner code but you still want to add some new features to a function, you should use decorator. For example, if you want to print “hello” when you add two numbers: 12345678910111213def hello(fn): def wrapper(*args, **kwargs): aa = fn(*args, **kwargs) print('hello') return aa return wrapper def my_add(a,b): return a+bmy_add = hello(my_add) aa = my_add(10,10)print(aa) result:12hello20or you can use operator “@” to implement your function elegantly:12345678910111213def hello(fn): def wrapper(*args, **kwargs): aa = fn(*args, **kwargs) print('hello') return aa return wrapper @hellodef my_add(a,b): return a+b aa = my_add(10,10)print(aa) enumerateIf you are too lazy to count the index of a list in a loop but you still need the index, then you should use enumerate: 123a = ['a', 'b', 'c']for ind, item in a: print(ind, item) result:123(0, 'a')(1, 'b')(2, 'c') iteratorIf you are too lazy to get the every elements in a list by index but you still want to get every element in order, then you should use iterator: 12345678910111213def inc(): for i in range(10): yield i x = inc() print(next(x))print(next(x))print(next(x))print(next(x))print(next(x))print(next(x))print(next(x))print(next(x)) result:1234567801234567 Do not easily copy object in pythonIf you are too lazy to get deepcopy a other object but you still want get a copy of a exists one, then you many creating a bug in you code!In python, all things are objects, even class is a kind of object. If you want to copy a object, then the ID of these two objects are the same which means the two objects are are stored in the same place. If you modify one object, then another object are also modified. 12345678910111213 class sample: def __init__(self, items = []): self.items = items def append(self, value): self.items.append(value) s1 = sample(items=['1'])s2 = s1s1.append('a')s2.append('b') print(s1.items, s2.items) result:1(['1', 'a', 'b'], ['1', 'a', 'b'])another example:12345a = [1,2,3,4]b = ab[0] = 100print(a)print(b)result:12[100,2,3,4][100,2,3,4]Instead, you should use “deepcopy” to copy two objects12345a = [1,2,3,4]b = copy.deepcopy(a)b[0] = 100print(a)print(b)result:12[1,2,3,4][100,2,3,4] itertoolsIf you are too lazy to write a infinite loop in an iterator but you still want to reset to loop when iteration finished, then you should use itertools: 1234567891011121314151617import itertools def inc(): a = [1,2,3,4,5] cs = itertools.cycle(a) for i in cs: yield i x = inc() print(next(x))print(next(x))print(next(x))print(next(x))print(next(x))print(next(x))print(next(x))print(next(x)) result:1234567812345123 concurrent.futuresIf you are too lazy to write a multi-thread to process all the elements in a list which may cost lots of time, then you should use a python build-in multi-thread tools: 1234567import concurrent.futuresdef print_num(num): print(num) a = range(5)with concurrent.futures.ProcessPoolExecutor() as executor: executor.map(load_and_resize, a) result:1234501234","link":"/2018/12/05/python-tricks-1/"},{"title":"&lt; Tensorflow &gt;Tensorflow Data Augmentation YUV distortation","text":"MethodThis method augment image by disturbing the YUV field of the target image and then transform back to RGB field. 123456789101112131415161718192021222324252627282930313233343536373839404142434445def all_yuv_tur(image_in, ratio=1.0): def yuv_tur(image , case): image = image / 255 image = tf.image.rgb_to_yuv(image) yuv_channels = tf.split(image, 3, 2) y = yuv_channels[0] uv = tf.concat(yuv_channels[1:],2) shape1 = uv.get_shape().as_list() level=int(8*ratio) d1 = tf.random_uniform([], maxval=level, dtype=tf.int32) d2 = tf.random_uniform([], maxval=level, dtype=tf.int32) if case == 0: uv = tf.slice(uv, [0,0,0], [shape1[0]-d1, shape1[1]-d2, shape1[2]]) uv = tf.pad(uv, [[d1,0], [d2, 0], [0,0]]) elif case == 1: uv = tf.slice(uv, [d1,d2,0], [shape1[0]-d1, shape1[1]-d2, shape1[2]]) uv = tf.pad(uv, [[0, d1], [0, d2], [0,0]]) elif case == 2: uv = tf.slice(uv, [d1,0,0], [shape1[0]- d1, shape1[1]-d2, shape1[2]]) uv = tf.pad(uv, [[0, d1], [d2, 0], [0,0]]) elif case == 3: uv = tf.slice(uv, [0,d2,0], [shape1[0]-d1, shape1[1]-d2, shape1[2]]) uv = tf.pad(uv, [[d1, 0], [0, d2], [0,0]]) yuv_image = tf.concat([y, uv], 2) image = tf.image.yuv_to_rgb(yuv_image) image = image * 255 return image def yuv_tur_sel(image): image = apply_with_random_selector(image, lambda x, ordering: yuv_tur(x, ordering), num_cases=4) return image ran = tf.random_uniform([]) image = tf.cond(ran&lt;0.1, lambda: yuv_tur_sel(image_in), lambda: image_in) return image The images below are the origin image and transfered image:","link":"/2018/12/02/tensorflow-disturb-yuv/"},{"title":"&lt; Network &gt; Understanding Shufflenet_v2","text":"The relationship with DensenetSince I use shufflent_V2 on the task of Face Antispoofing for quit a time, I found that shufflenet_V2 is efficient yet also could provide high accuracy. I found that for each block of shufflent_V2, the block itself could be treated as special case of densenet. Different from densenet, each block of shufflent_V2 gets more information from the recent conv feature and get less information from the far-away conv feature. As we know, densely connected structure could be a tool to reuse feature from different layer, thus could be treated as a multi scale feature pyramid for deep learning. The shufflenet_V2 structure put more emphasis on the feature near-by and put less emphasis on the feature far away. It is probably because the feature from the layer near-by is more correlated.","link":"/2018/11/28/network-shufflenet-v2/"}],"tags":[{"name":"DEMO","slug":"DEMO","link":"/tags/DEMO/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Deeplearning","slug":"Deeplearning","link":"/tags/Deeplearning/"},{"name":"C&#x2F;C++","slug":"C-C","link":"/tags/C-C/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Face","slug":"Face","link":"/tags/Face/"},{"name":"Data","slug":"Data","link":"/tags/Data/"},{"name":"Computing","slug":"Computing","link":"/tags/Computing/"},{"name":"Network Structure","slug":"Network-Structure","link":"/tags/Network-Structure/"},{"name":"Deeplarning","slug":"Deeplarning","link":"/tags/Deeplarning/"}],"categories":[{"name":"DEMO","slug":"DEMO","link":"/categories/DEMO/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Deeplearning","slug":"Deeplearning","link":"/categories/Deeplearning/"},{"name":"C&#x2F;C++","slug":"C-C","link":"/categories/C-C/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"Face","slug":"Face","link":"/categories/Face/"},{"name":"Computing","slug":"Computing","link":"/categories/Computing/"},{"name":"Data","slug":"Data","link":"/categories/Data/"},{"name":"Network Structure","slug":"Network-Structure","link":"/categories/Network-Structure/"}]}