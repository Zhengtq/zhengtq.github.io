<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>&lt; Deeplearning &gt; TF实操Game of Noise - Billy&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Billy&#039;s Blog"><meta name="msapplication-TileImage" content="/img/1.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Billy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="前言 来自于2020年1月份的一篇arxiv文章.文章的主要思想是通过给CNN网络(以分类模型举例)的输入图加入噪声来使得模型更加的鲁棒． 与之前手动加入噪声不同的是，该文章采用对抗网络的思想，通过一个噪声生成器来生成噪声,并尽量使你的分类模型(判别模型)做出错误的分类． 而你的分类模型的目的是尽量能够不被加入的图片噪声干扰，依然能做做出正确的输出.最终经过数轮的迭代训练,达到使得你的分类模型能够"><meta property="og:type" content="blog"><meta property="og:title" content="&lt; Deeplearning &gt; TF实操Game of Noise"><meta property="og:url" content="https://zhengtq.github.io/2020/03/20/advnoise/"><meta property="og:site_name" content="Billy&#039;s Blog"><meta property="og:description" content="前言 来自于2020年1月份的一篇arxiv文章.文章的主要思想是通过给CNN网络(以分类模型举例)的输入图加入噪声来使得模型更加的鲁棒． 与之前手动加入噪声不同的是，该文章采用对抗网络的思想，通过一个噪声生成器来生成噪声,并尽量使你的分类模型(判别模型)做出错误的分类． 而你的分类模型的目的是尽量能够不被加入的图片噪声干扰，依然能做做出正确的输出.最终经过数轮的迭代训练,达到使得你的分类模型能够"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhengtq.github.io/img/og_image.png"><meta property="article:published_time" content="2020-03-20T08:05:23.000Z"><meta property="article:modified_time" content="2021-03-11T11:13:19.694Z"><meta property="article:author" content="Billy"><meta property="article:tag" content="Deeplearning"><meta property="article:tag" content="Tensorflow"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhengtq.github.io/2020/03/20/advnoise/"},"headline":"< Deeplearning > TF实操Game of Noise","image":["https://zhengtq.github.io/img/og_image.png"],"datePublished":"2020-03-20T08:05:23.000Z","dateModified":"2021-03-11T11:13:19.694Z","author":{"@type":"Person","name":"Billy"},"description":"前言 来自于2020年1月份的一篇arxiv文章.文章的主要思想是通过给CNN网络(以分类模型举例)的输入图加入噪声来使得模型更加的鲁棒． 与之前手动加入噪声不同的是，该文章采用对抗网络的思想，通过一个噪声生成器来生成噪声,并尽量使你的分类模型(判别模型)做出错误的分类． 而你的分类模型的目的是尽量能够不被加入的图片噪声干扰，依然能做做出正确的输出.最终经过数轮的迭代训练,达到使得你的分类模型能够"}</script><link rel="canonical" href="https://zhengtq.github.io/2020/03/20/advnoise/"><link rel="icon" href="/img/1.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-03-20T08:05:23.000Z" title="3/20/2020, 4:05:23 PM">2020-03-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-11T11:13:19.694Z" title="3/11/2021, 7:13:19 PM">2021-03-11</time></span><span class="level-item"><a class="link-muted" href="/categories/Deeplearning/">Deeplearning</a></span><span class="level-item">13 分钟 read (About 2003 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">&lt; Deeplearning &gt; TF实操Game of Noise</h1><div class="content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p> 来自于2020年1月份的一篇arxiv文章.文章的主要思想是通过给CNN网络(以分类模型举例)的输入图加入噪声来使得模型更加的鲁棒．</p>
<p>与之前手动加入噪声不同的是，该文章采用对抗网络的思想，通过一个噪声生成器来生成噪声,并尽量使你的分类模型(判别模型)做出错误的分类．</p>
<p>而你的分类模型的目的是尽量能够不被加入的图片噪声干扰，依然能做做出正确的输出.最终经过数轮的迭代训练,达到使得你的分类模型能够抵抗各类噪声干扰的目的．<br><span id="more"></span></p>
<h1 id="网络泛化与图像增强"><a href="#网络泛化与图像增强" class="headerlink" title="网络泛化与图像增强"></a>网络泛化与图像增强</h1><p>神经网络需要落地的最大困难往往在于训练数据和实际应用场景有较大的gap．经常是你的模型在训练集上拟合的很好，甚至在你的验证集/测试集上都拟合的很好，但是一部署到实际场景往往会出现各种各样的问题，如误检太多,召回太低等等．这会让你一度觉得你的人工只能简直是人工智障．以上统称模型的泛化能力差.</p>
<p>当然，这里面最大的问题在你的训练\测试数据不够丰富，说就是你的训练集只是你训练素材真实分布的一个小小的sample．你在训练的时候只是用真实分布的一个sample，但你在线下应用的时候却要让模型面对整个真实分布(更加泛的sample)，这个时候模型如果遇到未知的数据，往往会作出错误判断．</p>
<p>当然我们不太可能获得真实分布那种海量的数据．于是人们就通过各种各样的方式来提升模型的泛化能力．有人通过修改模型的结构，比如Hinton老爷子发明的胶囊网络或dropout等等．有人通过调整优化方式或增加正则项来使得模型的最优值尽量落在平滑的极值点．有人通过调整训练方式，如增加多任务，或给网络添加各种先验等等．除了这些方式之外，更多的人是通过数据增强来达到让模型增加泛化能力的目的．</p>
<p>最初的图像增强是通过给图像加入各种手动的扰动来达到的．如随机裁剪，色彩变换等等．但这样的扰动方式有个很大的弊端，就是你必须手动调整各个增加的参数(如裁剪大小，色彩强度等等)．这是一个很主观且繁琐的事情．万一调试不好，反而会使得模型的准确率下降．</p>
<p>于是Google后来提出了AutoAugment，也就是自己定义了一个扰动的空间，通过强化学习的方式自动选择最优的扰动和参数．而本篇文章要说的Game of Noise，则是另外一种思路．</p>
<p>我们可以这样理解Game of Noise，假如说把扰动分类为两种，一种扰动是改变图像像素的位置，但是不改变图像像素的值(我们姑且成为方位扰动)，如速记裁剪，图像旋转，图像flip，仿射变换等等．另外一种扰动是在图像原始的像素值增加后减去某一个值，但是不改变像素的位置(我们姑且成为像素扰动)，如亮度增强，对比度增强，以及加入噪声等等，今天我们讨论的Game of Noise本质上是属于一种像素扰动．而Game of Noise可以通过对抗学习的方式去学习出最优的像素扰动,从而取代手动设置的像素扰动．</p>
<h1 id="Game-of-Noise"><a href="#Game-of-Noise" class="headerlink" title="Game of Noise"></a>Game of Noise</h1><p>Game of Noise 这篇文章通过实验发现了几个有趣的现象。比如如果给图像加入高斯扰动，一个精心选择的噪声方差可以对模型效果有很好的提升（相对于随意噪声方差来说），比如通过对抗学习的方法来给图像加入噪声可以在免去手动调参的同时给模型加入最适合的扰动。而本文主要讨论和实验的是后者，也就是通多对抗学习的方式给模型加入噪声扰动。<br>具体的实验方式与普通的对抗思想差异不大，由生成器生成对抗噪声，把噪声加入到训练图片上，再把加入噪声的图片送到你的判别器去判别和训练，这里的判别器就是你自己的需要扰动的模型，通常是一个已经训练好的模型，在对抗噪声图片上进行fintue．生成器的目的是尽量让加入噪声的图片让判别器误判，判别器的目的是尽量对加入噪声的图片正确区分。通过这样的对抗训练，最终让你的判别模型学习到抗未知噪声干扰的能力。</p>
<h1 id="TF实现"><a href="#TF实现" class="headerlink" title="TF实现"></a>TF实现</h1><p>首先我们需要定义一个生成器:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">noise_generator</span>(<span class="params">x</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, </span><br><span class="line">            activation_fn=tf.nn.relu):</span><br><span class="line">        x = slim.conv2d(x, <span class="number">32</span>, (<span class="number">1</span>, <span class="number">1</span>), stride=<span class="number">1</span>, scope=<span class="string">&#x27;Conv1_advnoise&#x27;</span>)</span><br><span class="line">        x = slim.conv2d(x, <span class="number">64</span>, (<span class="number">1</span>, <span class="number">1</span>), stride=<span class="number">1</span>, scope=<span class="string">&#x27;Conv2_advnoise&#x27;</span>)</span><br><span class="line">        x = slim.conv2d(x, <span class="number">3</span>, (<span class="number">1</span>, <span class="number">1</span>), stride=<span class="number">1</span>, activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,scope=<span class="string">&#x27;Conv3_advnoise&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br>原文中没加batch_norm,我后来实验发现加入bn还是效果会好一些,原文中前两层的channel都为20,我把channel数变大之后发现效果会好一些.<br>下一步生成noise图像:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">g_noise_input = tf.random_normal(shape=(BATCH_SIZE_SEP, SIZE, SIZE, CHANNEL), mean=<span class="number">0.0</span>, stddev=<span class="number">0.5</span>, dtype=tf.float32)</span><br><span class="line">out_noise = noise_generator(g_noise_input)</span><br><span class="line">ran_sel = tf.random_uniform((BATCH_SIZE_SEP, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), minval=<span class="number">0</span>,maxval=<span class="number">2</span>,dtype=tf.int32)</span><br><span class="line">ran_sel = tf.cast(ran_sel, tf.float32)</span><br><span class="line">out_noise = out_noise * ran_sel</span><br><span class="line">noise_image = images + out_noise</span><br><span class="line">noise_image = tf.clip_by_value(noise_image, <span class="number">0.0</span>, <span class="number">255.0</span>)</span><br></pre></td></tr></table></figure>
<p>generator的输入是和输入图同样大小的高斯噪声(为了后面的相加操作)．给噪声加入ran_sel随机选择参数，目的是随机选择需要扰动的图片．<br>获得图片的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_logits = model.inference(noise_image,  num_classes=LABEL_NUM)</span><br></pre></td></tr></table></figure>
<p>定义生成器的loss：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reverse_label = tf.cast((<span class="number">1</span>-labels), tf.float32)</span><br><span class="line">loss_softmax_sep_fake = LOSS_FUN(reverse_label, out_logits, ran_sel)</span><br><span class="line">tmp_noise = tf.reshape(out_noise, (BATCH_SIZE_SEP, <span class="number">280</span>*<span class="number">280</span>*<span class="number">3</span>))</span><br><span class="line">ord_loss = tf.norm(tmp_noise, <span class="built_in">ord</span>=<span class="number">2</span>, axis=-<span class="number">1</span>)</span><br><span class="line">ord_loss = tf.sqrt(tf.<span class="built_in">pow</span>(ord_loss - <span class="number">2000</span>, <span class="number">2</span>))/<span class="number">200</span></span><br><span class="line">ord_loss = tf.reduce_mean(ord_loss)</span><br></pre></td></tr></table></figure><br>按照普通Gan中生成器的训练方式，需要把label取反，然后和输出logtis求交叉熵loss(最后要乘以ran_sel)．另外如果你要控制噪声的强度(防止noise的值过大)，需要对噪声加上一个2范数的限制，上面的ord_loss就是起到这样的一个作用.<br>定义判别器的Loss：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_softmax_sep = LOSS_FUN(label, out_logits, ran_sel)</span><br></pre></td></tr></table></figure>
<p>判别器的loss很简单，就是没有reverse的label和输出logits的交叉熵．<br>最后定义训练过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % N ==<span class="number">0</span>:</span><br><span class="line">       sess.run(tf.variables_initializer(G_variable))</span><br><span class="line"><span class="keyword">if</span> ord_loss_out &gt; <span class="number">0.1</span>:</span><br><span class="line">     _, ord_loss_out = sess.run([train_op_ord, ord_loss])</span><br><span class="line">_, G_loss_out = sess.run([train_op_G, train_loss_G])</span><br><span class="line">_, D_loss_out = sess.run([train_op_D, train_loss_D])</span><br></pre></td></tr></table></figure>
<p>训练就是普通的Gan的训练方式，也就是G和D交替优化．这里有个需要说明的trick是，最好在一定的迭代次数之后再次初始化G模型的变量，这样做是为了方式G生成的noise过于单一．</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>&lt; Deeplearning &gt; TF实操Game of Noise</p><p><a href="https://zhengtq.github.io/2020/03/20/advnoise/">https://zhengtq.github.io/2020/03/20/advnoise/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Billy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-03-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-03-11</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Deeplearning/">Deeplearning</a><a class="link-muted mr-2" rel="tag" href="/tags/Tensorflow/">Tensorflow</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/07/30/pri-knowledge-1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">&lt; Deeplearning &gt; 给模型加入先验知识</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/01/17/advAS/"><span class="level-item">&lt; Antispoofing &gt;　如何科学的攻破活体识别系统</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>app_id</code> or <code>app_key</code> for Valine. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Billy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>