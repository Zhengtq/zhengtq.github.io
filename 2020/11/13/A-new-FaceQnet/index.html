<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>&lt; Deeplearning &gt; 博采众长，一个更加全面的人脸质量评价库 - Billy&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Billy&#039;s Blog"><meta name="msapplication-TileImage" content="/img/1.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Billy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="开始今天给大家推荐一个效果很好速度又快(逃))的人脸质量评价库，这个人脸质量评价库是我自己训的，为了方便表示，我给它起个名字，就BFQ(很像BBQ). 其实网上有很多开源的人脸质量算法，其中的很多甚至开源了模型出来，这给需要用人脸质量评价算法来过滤人脸的人们提供了很好的工具，但是很多脸质量算法的效果并不好，或是不能满足自己的需要，或是只能满足一部分的需求，所以我们往往需要多个质量算法库联合去判断一"><meta property="og:type" content="blog"><meta property="og:title" content="&lt; Deeplearning &gt; 博采众长，一个更加全面的人脸质量评价库"><meta property="og:url" content="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/"><meta property="og:site_name" content="Billy&#039;s Blog"><meta property="og:description" content="开始今天给大家推荐一个效果很好速度又快(逃))的人脸质量评价库，这个人脸质量评价库是我自己训的，为了方便表示，我给它起个名字，就BFQ(很像BBQ). 其实网上有很多开源的人脸质量算法，其中的很多甚至开源了模型出来，这给需要用人脸质量评价算法来过滤人脸的人们提供了很好的工具，但是很多脸质量算法的效果并不好，或是不能满足自己的需要，或是只能满足一部分的需求，所以我们往往需要多个质量算法库联合去判断一"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/sum_nonorm.jpg"><meta property="og:image" content="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/sum_norm.jpg"><meta property="og:image" content="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/all_norm.jpg"><meta property="og:image" content="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/flops.png"><meta property="article:published_time" content="2020-11-13T01:26:40.000Z"><meta property="article:modified_time" content="2021-03-13T02:31:09.000Z"><meta property="article:author" content="Billy"><meta property="article:tag" content="Face"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2020/11/13/A-new-FaceQnet/sum_nonorm.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/"},"headline":"< Deeplearning > 博采众长，一个更加全面的人脸质量评价库","image":["https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/sum_nonorm.jpg","https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/sum_norm.jpg","https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/all_norm.jpg","https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/flops.png"],"datePublished":"2020-11-13T01:26:40.000Z","dateModified":"2021-03-13T02:31:09.000Z","author":{"@type":"Person","name":"Billy"},"description":"开始今天给大家推荐一个效果很好速度又快(逃))的人脸质量评价库，这个人脸质量评价库是我自己训的，为了方便表示，我给它起个名字，就BFQ(很像BBQ). 其实网上有很多开源的人脸质量算法，其中的很多甚至开源了模型出来，这给需要用人脸质量评价算法来过滤人脸的人们提供了很好的工具，但是很多脸质量算法的效果并不好，或是不能满足自己的需要，或是只能满足一部分的需求，所以我们往往需要多个质量算法库联合去判断一"}</script><link rel="canonical" href="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/"><link rel="icon" href="/img/1.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-13T01:26:40.000Z" title="11/13/2020, 9:26:40 AM">2020-11-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-13T02:31:09.000Z" title="3/13/2021, 10:31:09 AM">2021-03-13</time></span><span class="level-item"><a class="link-muted" href="/categories/Face/">Face</a></span><span class="level-item">14 分钟 read (About 2118 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">&lt; Deeplearning &gt; 博采众长，一个更加全面的人脸质量评价库</h1><div class="content"><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>今天给大家推荐一个效果很好速度又快(逃))的人脸质量评价库，这个人脸质量评价库是我自己训的，为了方便表示，我给它起个名字，就BFQ(很像BBQ).</p>
<p>其实网上有很多开源的人脸质量算法，其中的很多甚至开源了模型出来，这给需要用人脸质量评价算法来过滤人脸的人们提供了很好的工具，但是很多脸质量算法的效果并不好，或是不能满足自己的需要，或是只能满足一部分的需求，所以我们往往需要多个质量算法库联合去判断一个人脸的质量。</p>
<span id="more"></span>

<h2 id="选择基础算法"><a href="#选择基础算法" class="headerlink" title="选择基础算法"></a>选择基础算法</h2><p>我在网上找了一下几个觉得还不错的(人脸)质量评估算算法：</p>
<ul>
<li>拉布拉斯二阶梯度</li>
<li>一阶梯度</li>
<li>MotionBlurNet(mbn)</li>
<li>FaceQualityNet(fqn)</li>
<li>PaQ-2-PiQ</li>
</ul>
<p>这5中算法各有所长，比如fqn是针对人脸的一种质量评价，对于人脸本身的性质，如遮挡和角度具有一定的敏感性；mbn是针对运动模型的一个质量评价算法；PaQ-2-PiQ是针对全场景的一种质评价；而基于一阶/二阶的梯度算法则是用传统算法去求解梯度去衡量一张图片的清晰度。</p>
<p>假如我们想基于分类去训练一个人脸质量评价的网络，我们的目的是训练一个输出为0/1的二分类器，0代表质量不好，1代表质量好，我们可以是先通过上面五种质量评价算法去给我们的每一个人脸样本打上置信度，然后基于这五个置信度分别设定不同的阈值，联合去判断一个样本的质量标签(0或者1)，然后基于这个标签去训练一个质量评价的分类网络。</p>
<p>当然上面的做法是有明显的缺点的，那就是没有给不同质量的人脸划分程度，质量最好的和质量次好的都往相同的目标去优化，同样的，质量最差的和质量次差的也是都向同样的目标去优化，这样显然是不合理的。</p>
<p>更加合理的做法是把这个任务当做回归的问题来看待，我们的目标是训练一个人脸质量评价归回网络，而不是一个分类网络。</p>
<h2 id="标签设置"><a href="#标签设置" class="headerlink" title="标签设置"></a>标签设置</h2><p>所以现在问题来了，怎么给每一张图打上对应的标签。有一个办法，就是通过上面的5个图像(人脸)质量评价算法给每一张图片打的置信度综合去给每一张图片的质量标签，这样下来，图片的质量标签就集合了上面5个图像(人脸)质量算法的特点，更加的全面。</p>
<p>通过给我们的训练图片打上标签，我们发现上面每一个质量评价算法的输出值的范围的差别很大。统计在训练集上的每一个质量评价库的输出值范围如下：</p>
<ul>
<li>拉布拉斯二阶梯度 (0-1427)</li>
<li>一阶梯度 (0-23)</li>
<li>MotionBlurNet(mbn) (0-36)</li>
<li>FaceQualityNet(fqn) (0.3-0.81)</li>
<li>PaQ-2-PiQ (25-85)</li>
</ul>
<p>具体的分布如下图所示：<br><img src="/2020/11/13/A-new-FaceQnet/sum_nonorm.jpg"></p>
<h2 id="调整分布"><a href="#调整分布" class="headerlink" title="调整分布"></a>调整分布</h2><p>大家可以看到，不同算法的输出值的范围的差异还是很大的，我们我们要做的就是统计出每一个算法输出的最大值max和最小值min，然后用对应的最大值和最小值归一化每一个算法的输出。具体的方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val &#x3D; (val - min)&#x2F;(max - min)</span><br></pre></td></tr></table></figure>
<p>这里需要说明一点，那就是这个min和max不是所有样本的统计值，从上面的直方图我们也可以看到，很多算法的min,max值是一种奇异值的存在，它们绝大部分的输出分布和min, max相差的较远，于是从我从上面的直方图大致估计了一下它们新的min,max，这个min,max所代表的的范围比实际的范围要小，所以也用这样的min,max去归一化输出值也会更加的精准。</p>
<p>归一化之后直方图如下：<br><img src="/2020/11/13/A-new-FaceQnet/sum_norm.jpg"></p>
<p>好，把各个算法的输出都归一化到同一个尺度后，我们就要想想怎么去组合每一个样本的5个质量评价的结果，然后去生成它自己的质量标签。可以直接通过给不同输出结果加权重的方式来生成一个新的回归标签，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bfq_score &#x3D; w1 * lp_blur_socre + w2 * g_sharp + w3 * MotionNet_score + w4 * FaceQnet_score + w5 * Paq-P2Q</span><br></pre></td></tr></table></figure>

<p>w1到w5的这些权重就需要自己手动设置了，你可以以算法的重要性来设定权值，把比较符合自己task的算法输出的权值增大。</p>
<p>好，经过了上面的操作，我来看一下训练集的bfq_score的归一化分布情况：<br><img src="/2020/11/13/A-new-FaceQnet/all_norm.jpg"></p>
<p>通过上面的样本label分布图中，我们可以看到，数据呈现了明显长尾分布。所以我对分布较少的区域的做了不同程度的曾广，从而让不同label的分布的差异不至于这么大。</p>
<p>后面就开始训练了，这里我用了普通的L2_LOSS做归回，然后训练了大概50个epoc。然后用tensorflow自带的工具计算了一个模型的计算量，如下图：</p>
<p><img src="/2020/11/13/A-new-FaceQnet/flops.png"></p>
<p>计算量大概有13M(乘法加法算两次)，应该还有降低的空间，这个后面再优化。</p>
<h2 id="NCNN封装"><a href="#NCNN封装" class="headerlink" title="NCNN封装"></a>NCNN封装</h2><p>为了更加灵活的使用，我打算用NCNN把bfq模型封装一下．这里，我在原始NCNN的基础上，拆分出了一个极简的实现，对bfq的ncnn封装都是基于这个极简的ncnn实现，代码会在最后放出来．</p>
<p>首先，由于我是使用tensorflow训练的，需要把TF使用的网络结构转换成NCNN的格式．</p>
<p>在网上找了一下，发现没有合适又好用的转换程序，于是自己写了一套，最后转换的结果如下所示（展示部分）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">7767517</span><br><span class="line">182 195</span><br><span class="line">Input                layer_0 0 1 blob_0 0&#x3D;80 1&#x3D;80 2&#x3D;3</span><br><span class="line">Convolution          layer_2 1 1 blob_0 blob_1 0&#x3D;24 1&#x3D;3 2&#x3D;1 3&#x3D;2 4&#x3D;0 14&#x3D;0 15&#x3D;1 16&#x3D;1 7&#x3D;1 5&#x3D;0 6&#x3D;648</span><br><span class="line">BatchNorm            layer_7 1 1 blob_1 blob_2 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">ReLU                 layer_8 1 1 blob_2 blob_3 0&#x3D;0.0</span><br><span class="line">Pooling              layer_9 1 1 blob_3 blob_4 0&#x3D;0 1&#x3D;3 2&#x3D;2 3&#x3D;0 13&#x3D;0 14&#x3D;1 15&#x3D;1 4&#x3D;0 5&#x3D;0</span><br><span class="line">Convolution          layer_11 1 1 blob_4 blob_5 0&#x3D;24 1&#x3D;1 2&#x3D;1 3&#x3D;1 4&#x3D;0 14&#x3D;0 15&#x3D;0 16&#x3D;0 7&#x3D;1 5&#x3D;0 6&#x3D;576</span><br><span class="line">BatchNorm            layer_16 1 1 blob_5 blob_6 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">ReLU                 layer_17 1 1 blob_6 blob_7 0&#x3D;0.0</span><br><span class="line">ConvolutionDepthWise layer_19 1 1 blob_7 blob_8 0&#x3D;24 1&#x3D;3 2&#x3D;1 3&#x3D;2 4&#x3D;0 14&#x3D;0 15&#x3D;1 16&#x3D;1 7&#x3D;24 5&#x3D;0 6&#x3D;216</span><br><span class="line">BatchNorm            layer_24 1 1 blob_8 blob_9 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">Convolution          layer_26 1 1 blob_9 blob_10 0&#x3D;24 1&#x3D;1 2&#x3D;1 3&#x3D;1 4&#x3D;0 14&#x3D;0 15&#x3D;0 16&#x3D;0 7&#x3D;1 5&#x3D;0 6&#x3D;576</span><br><span class="line">BatchNorm            layer_31 1 1 blob_10 blob_11 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">ReLU                 layer_32 1 1 blob_11 blob_12 0&#x3D;0.0</span><br><span class="line">ConvolutionDepthWise layer_34 1 1 blob_4 blob_13 0&#x3D;24 1&#x3D;3 2&#x3D;1 3&#x3D;2 4&#x3D;0 14&#x3D;0 15&#x3D;1 16&#x3D;1 7&#x3D;24 5&#x3D;0 6&#x3D;216</span><br><span class="line">BatchNorm            layer_39 1 1 blob_13 blob_14 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">Convolution          layer_41 1 1 blob_14 blob_15 0&#x3D;24 1&#x3D;1 2&#x3D;1 3&#x3D;1 4&#x3D;0 14&#x3D;0 15&#x3D;0 16&#x3D;0 7&#x3D;1 5&#x3D;0 6&#x3D;576</span><br><span class="line">BatchNorm            layer_46 1 1 blob_15 blob_16 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">ReLU                 layer_47 1 1 blob_16 blob_17 0&#x3D;0.0</span><br><span class="line">ShuffleTwo           layer_48 2 1 blob_17 blob_12 blob_18</span><br><span class="line">Split                layer_52 1 2 blob_18 blob_19 blob_20</span><br><span class="line">Convolution          layer_54 1 1 blob_19 blob_21 0&#x3D;24 1&#x3D;1 2&#x3D;1 3&#x3D;1 4&#x3D;0 14&#x3D;0 15&#x3D;0 16&#x3D;0 7&#x3D;1 5&#x3D;0 6&#x3D;576</span><br><span class="line">BatchNorm            layer_59 1 1 blob_21 blob_22 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">ReLU                 layer_60 1 1 blob_22 blob_23 0&#x3D;0.0</span><br><span class="line">ConvolutionDepthWise layer_62 1 1 blob_23 blob_24 0&#x3D;24 1&#x3D;3 2&#x3D;1 3&#x3D;1 4&#x3D;1 14&#x3D;1 15&#x3D;1 16&#x3D;1 7&#x3D;24 5&#x3D;0 6&#x3D;216</span><br><span class="line">BatchNorm            layer_67 1 1 blob_24 blob_25 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">Convolution          layer_69 1 1 blob_25 blob_26 0&#x3D;24 1&#x3D;1 2&#x3D;1 3&#x3D;1 4&#x3D;0 14&#x3D;0 15&#x3D;0 16&#x3D;0 7&#x3D;1 5&#x3D;0 6&#x3D;576</span><br><span class="line">BatchNorm            layer_74 1 1 blob_26 blob_27 0&#x3D;24 1&#x3D;1e-3</span><br><span class="line">ReLU                 layer_75 1 1 blob_27 blob_28 0&#x3D;0.0</span><br><span class="line">ShuffleTwo           layer_76 2 1 blob_28 blob_20 blob_29</span><br><span class="line">Split                layer_80 1 2 blob_29 blob_30 blob_31</span><br><span class="line">......</span><br></pre></td></tr></table></figure>



<p>这里需要说明一下，上面的ncnn结构中有个ShuffleTwo的操作，这是在原始的ncnn中是没有的，这里我为了简化操作，自己实现了一个ShuffleTwo的操作．</p>
<p>其次，我需要把TF的模型权重转换成ncnn能够使用的二进制格式．这里需要注意一下，ncnn使用的默认权重格式是(out_c,in_c,h,w)，而TF的默认权重格式是(h, w, in_c, out_c)，所以需要在转换权重的时候，对权重加上transpose的操作．</p>
<h2 id="Pybind11-封装"><a href="#Pybind11-封装" class="headerlink" title="Pybind11 封装"></a>Pybind11 封装</h2><p>为了更方便的调用，决定用Pybind11把c++代码封装成python的库．</p>
<p>由于要编译python接口，所以在CMakeList中添加了一个按钮PY_WRAP，用来选择是编译python接口还是普通的c++接口．</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(PY_WRAP <span class="keyword">ON</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;</span>)</span><br><span class="line"><span class="keyword">add_subdirectory</span>(src)</span><br><span class="line"><span class="keyword">if</span>(PY_WRAP)</span><br><span class="line">    <span class="keyword">add_subdirectory</span>(python)</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">    <span class="keyword">add_subdirectory</span>(examples)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>我把代码开源到了github上，暂时没有放二进制的网络权重，后面会放出来．</p>
<p>代码地址：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Zhengtq/BFQ">BFQ</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>&lt; Deeplearning &gt; 博采众长，一个更加全面的人脸质量评价库</p><p><a href="https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/">https://zhengtq.github.io/2020/11/13/A-new-FaceQnet/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Billy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-11-13</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-03-13</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Face/">Face</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/12/08/ncnn-lession-1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">&lt; NCNN-Lession-1 &gt;　数据读取类DataReader</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/07/30/pri-knowledge-1/"><span class="level-item">&lt; Deeplearning &gt; 给模型加入先验知识</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "BGzxkVpRtr5PoQUppRDqiC1V-gzGzoHsz",
            appKey: "K9tU9mVpjknHh8SNOWrqXqDV",
            
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "zh-cn",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Billy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>