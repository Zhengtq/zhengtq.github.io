<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>&lt; Tensorflow &gt;Tensorflow2.4 最佳实践 - Billy&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Billy&#039;s Blog"><meta name="msapplication-TileImage" content="/img/1.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Billy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．"><meta property="og:type" content="blog"><meta property="og:title" content="&lt; Tensorflow &gt;Tensorflow2.4 最佳实践"><meta property="og:url" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/"><meta property="og:site_name" content="Billy&#039;s Blog"><meta property="og:description" content="开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/before.png"><meta property="og:image" content="http://yoursite.com/2021/01/27/tf2-x-best-practice/before1.png"><meta property="article:published_time" content="2021-01-27T00:38:00.000Z"><meta property="article:modified_time" content="2021-03-11T02:42:09.046Z"><meta property="article:author" content="Billy"><meta property="article:tag" content="Tensorflow"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2021/01/27/tf2-x-best-practice/before.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2021/01/27/tf2-x-best-practice/"},"headline":"Billy's Blog","image":["http://yoursite.com/2021/01/27/tf2-x-best-practice/before.png","http://yoursite.com/2021/01/27/tf2-x-best-practice/before1.png"],"datePublished":"2021-01-27T00:38:00.000Z","dateModified":"2021-03-11T02:42:09.046Z","author":{"@type":"Person","name":"Billy"},"description":"开始最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．"}</script><link rel="canonical" href="http://yoursite.com/2021/01/27/tf2-x-best-practice/"><link rel="icon" href="/img/1.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-27T00:38:00.000Z" title="1/27/2021, 8:38:00 AM">2021-01-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-11T02:42:09.046Z" title="3/11/2021, 10:42:09 AM">2021-03-11</time></span><span class="level-item"><a class="link-muted" href="/categories/Tensorflow/">Tensorflow</a></span><span class="level-item">15 分钟 read (About 2205 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">&lt; Tensorflow &gt;Tensorflow2.4 最佳实践</h1><div class="content"><h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><p>最近想尝试一下用Transformer做图片分类的效果，于是就在网上找找有没有比较好的例子．发现keras官方有个例子，于是就clone下来看看．本以为multi-head-attention这个模块需要自己来实现，竟然发现tf.keras中已经实现了multi-head-attention的接口，发现是真的方便（tensorflow的最新版本tf2.4才有的一个接口）．</p>
<span id="more"></span>
<p>跑了一个官方给的cifar的例子，效果还行．于是就打算在自己的数据上跑跑看效果，在这个过程中，发现官方给你例子还远远达不到训练速度最优化的程度．于是就把官方例子就改了一下，最终达到了一个满意的训练速度，本篇就是记录一下tf2.4下的训练性能调优全过程．最终调优后的代码会共享出来．</p>
<h1 id="单卡到多卡"><a href="#单卡到多卡" class="headerlink" title="单卡到多卡"></a>单卡到多卡</h1><p>官方给的vit(Vision Transformer)的例子是基于单卡的，但是现在训练大型网络已经离不开多卡，于是把官方例子改成多卡训练的版本（这里主要说明一下单机多卡的设置）．</p>
<p><strong>首先根据你手上卡的数量来建立一个strategy的对象</strong>．</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">physical_devices = tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)          </span><br><span class="line"><span class="keyword">for</span> ind, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(physical_devices):                      </span><br><span class="line">    tf.config.experimental.set_memory_growth(item, <span class="literal">True</span>)           </span><br><span class="line">                                                                   </span><br><span class="line">TRAIN_GPUS = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]                                          </span><br><span class="line">devices = [<span class="string">&quot;/gpu:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> TRAIN_GPUS]             </span><br><span class="line">strategy = tf.distribute.MirroredStrategy(devices)                                 </span><br></pre></td></tr></table></figure>
<p>这里tf.config.experimental.set_memory_growth的作用是限制显存的使用．</p>
<p><strong>然后我们要把数据做一个副本化的包装：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)   </span><br><span class="line">test_dist_dataset = strategy.experimental_distribute_dataset(test_daset)          </span><br></pre></td></tr></table></figure>
<p><strong>然后我们需要对train_step和test_step做一个封装：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope()</span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">distributed_train_step</span>(<span class="params">dataset_inputs</span>):</span>                           </span><br><span class="line">           per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))                           </span><br><span class="line">           <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=<span class="literal">None</span>)               </span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">distributed_test_step</span>(<span class="params">dataset_inputs</span>):</span>                           </span><br><span class="line">           <span class="keyword">return</span> strategy.run(test_step, args=(dataset_inputs,))           </span><br></pre></td></tr></table></figure>
<p><strong>最后在train_step，test_step，和计算loss和accuracy加上strategy的scope</strong>（这里只是拿train_step和test_step举例）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope():                                                       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">inputs</span>):</span>                                                  </span><br><span class="line">        images, labels = inputs                                              </span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:                                      </span><br><span class="line">            predictions = model(images, training=<span class="literal">True</span>)                       </span><br><span class="line">            loss = compute_loss(labels, predictions)                         </span><br><span class="line">                                                                             </span><br><span class="line">        gradients = tape.gradient(loss, model.trainable_variables)           </span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, model.trainable_variables)) </span><br><span class="line">        compute_acc(labels, predictions, train_accuracy)                     </span><br><span class="line">        <span class="keyword">return</span> loss                                                          </span><br><span class="line">                                                                             </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_step</span>(<span class="params">inputs</span>):</span>                                                   </span><br><span class="line">        images, labels = inputs                                              </span><br><span class="line">        predictions = model(images, training=<span class="literal">False</span>)                          </span><br><span class="line">        compute_acc(labels, predictions, test_accuracy)                      </span><br><span class="line">                                                                             </span><br></pre></td></tr></table></figure>
<p>经过了上述的步骤，成功的在tf2.4下，把单卡训练转换到了单机多卡训练．</p>
<h1 id="eager-mode-到-static-graph-mode"><a href="#eager-mode-到-static-graph-mode" class="headerlink" title="eager mode 到 static graph mode"></a>eager mode 到 static graph mode</h1><p>由于tf2.x模式的执行方式是急切执行（eager mode），eager mode的好处在在于方便debug，但是如果拿来训练就不太好了，因为eager mode会托慢速度．所以我们需要在调试好网络之后把执行模式切换为静态图的模式．而这个步骤非常简单，<strong>加上一个tf.function的修饰符就好</strong>．</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> strategy.scope():                                                   </span><br><span class="line"><span class="meta">    @tf.function                                                         </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">distributed_train_step</span>(<span class="params">dataset_inputs</span>):</span>                           </span><br><span class="line">        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))         </span><br><span class="line">        <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=<span class="literal">None</span>)             </span><br><span class="line"><span class="meta">    @tf.function                                                         </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">distributed_test_step</span>(<span class="params">dataset_inputs</span>):</span>                           </span><br><span class="line">        <span class="keyword">return</span> strategy.run(test_step, args=(dataset_inputs,))           </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="调整tf-data数据流顺序"><a href="#调整tf-data数据流顺序" class="headerlink" title="调整tf.data数据流顺序"></a>调整tf.data数据流顺序</h1><p>tf.data的数据处理顺序非常重要，改变顺序可能会极度的托慢训练速度．</p>
<p>这里，我的数据处理流程如下：</p>
<ol>
<li>读取所有图片的路径和对应的label．</li>
<li>把图片路径给parse成图片．</li>
</ol>
<p>流程其实很简单，但这里要涉及到几点：</p>
<ol>
<li>如何shuffle.</li>
<li>哪里设定epoch</li>
<li>哪里设定batch</li>
<li>哪里设定预取prefetch．</li>
</ol>
<p>先来上code，再解释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  image_roots, labels = generate_fileroots_labels(file_root)                   </span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((image_roots, labels))           </span><br><span class="line">  dataset = dataset.repeat(<span class="number">100</span>).shuffle(buffer_size=<span class="number">2000</span>) </span><br><span class="line"><span class="comment">#  dataset = dataset.map(_parse_data, num_parallel_calls=tf.data.experimental.AUTOTUNE) </span></span><br><span class="line">  dataset = dataset.<span class="built_in">map</span>(_parse_data, num_parallel_calls=<span class="number">16</span>)                     </span><br><span class="line">  dataset = dataset.batch(batch_size)                                           </span><br><span class="line">  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)                     </span><br></pre></td></tr></table></figure>
<p>首先，repeat表示需要把数据重复多少次，也就是设定的epoc，shuffle代表在多少的buffer中打乱数据．<strong>这两者需要放到map前面</strong>，因为在map前，数据流处理的都是图片路径和label的轻量化数据，这对于repeat和shuffle是有利的．</p>
<p>而batch和prefetch就需要放到map的后面．<strong>这里需要注意，要先设定batch，再prefetch（不然会慢）</strong>．</p>
<h1 id="大杀器－－tf-profiler"><a href="#大杀器－－tf-profiler" class="headerlink" title="大杀器－－tf.profiler"></a>大杀器－－tf.profiler</h1><p>当我们觉得已经把加速做到极致了之后，我们需要用tensorflow自带的性能检测工具tf.profiler来检查一些性能还有那些可以榨取的空间．</p>
<p>用tf.profiler之前，你需要先按照官方的教程<a target="_blank" rel="noopener" href="https://github.com/tensorflow/profiler">安装</a>．这里有个小坑，因为tf.profiler需要依赖libcupti这个库，而libcupti这个库不在cuda的主库目录里，而是在extras/CUPTI/lib64里面，这个需要注意．</p>
<p>然后在你的训练代码中，需要添加如下的code：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t_step, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dist_dataset):</span><br><span class="line">    <span class="keyword">if</span> t_step == <span class="number">500</span>:</span><br><span class="line">          tf.profiler.experimental.start(<span class="string">&#x27;/tmp/&#x27;</span> + datetime.datetime.now().strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>))</span><br><span class="line">    <span class="keyword">if</span> t_step == <span class="number">600</span>:</span><br><span class="line">          tf.profiler.experimental.stop()</span><br><span class="line">    <span class="keyword">with</span> tf.profiler.experimental.Trace(<span class="string">&#x27;Train&#x27;</span>, step_num=t_step, _r=<span class="number">1</span>):</span><br><span class="line">        step_loss = distributed_train_step(x)</span><br></pre></td></tr></table></figure>
<p>这个代码段的意思是，在训练的第200步到第300步需要记录你的训练profile．</p>
<p>这里说明两点：</p>
<p>第一，<strong>我们不需要要整个训练过程都记录profile</strong>，因为记录profile仅仅是为了调优，只需要记录某些步的profile就可以提供你来调优即可．</p>
<p>第二，<strong>不从第０步就开始记录是因为我们需要先让训练达到稳定之后记录才会比较准确</strong>（示例中是从500步到600步开始记录profile）．</p>
<p>好了，我们看一下，都记录了一些什么东西：</p>
<p><img src="/2021/01/27/tf2-x-best-practice/before.png" alt=""></p>
<p>这里我们可以看到耗时主要在3个方面：</p>
<ol>
<li><strong>Kernel Launch Time</strong>  </li>
<li><strong>Host Compute Time</strong></li>
<li><strong>Device Compute Time</strong></li>
</ol>
<p>我们来一个一个解决，首先来解决kernel lanuch time，这个在右边的建议(Recommendation for Next Step)有说明：</p>
<blockquote>
<ul>
<li>14.3 % of the total step time sampled is spent on ‘Kernel Launch’. It could be due to CPU contention with tf.data. In this case, you may try to set the environment variable TF_GPU_THREAD_MODE=gpu_private.</li>
</ul>
</blockquote>
<p>也就是说可以通过设定TF_GPU_THREAD_MODE=gpu_private来解决．也就是说要在之前训练程序的前面加上下面一句命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TF_GPU_THREAD_MODE=gpu_private</span><br></pre></td></tr></table></figure>
<p>然而我这样执行之后发现作用好像不是很大，知道怎么解决的同学可以交流．</p>
<p>我们现在来解决第二个主要耗时的点，也就是Host Compute Time耗时过高．</p>
<p>我们知道，tensorflow的多卡策略是ps-worker的方式，这个ps可以认为是host，主要是负责更新参数和处理数据流，按理说这部分的耗时不应该很高才对．于是我往下看具体的host的耗时的页面：</p>
<p><img src="/2021/01/27/tf2-x-best-practice/before1.png" alt=""></p>
<p>我们看到，host的耗时，很大的程度和２个op有关：</p>
<ol>
<li>stridedSlide</li>
<li>cast</li>
</ol>
<p>其中stridedSlide占了绝对的大头，经过查阅资料发现，stridedSlide耗时比较高主要和tf.distribute.MirroredStrategy这个对象有关．</p>
<p>因为我在我的工程里面用到了tf.data.Dataset.from_tensor_slices这个对象，这个对象用到了stridedSlide这个操作，而tf.distribute.MirroredStrategy对stridedSlide的操作支持的不好．</p>
<p>网上的建议是把tf.distribute.MirroredStrategy换成tf.distribute.experimental.MultiWorkerMirroredStrategy．</p>
<p>也就是如下实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  TRAIN_GPUS = [0,1,2,3]</span></span><br><span class="line"><span class="comment">#  devices = [&quot;/gpu:&#123;&#125;&quot;.format(i) for i in TRAIN_GPUS]</span></span><br><span class="line"><span class="comment">#  strategy = tf.distribute.MirroredStrategy(devices)</span></span><br><span class="line">tf.config.set_visible_devices(physical_devices[<span class="number">0</span>:<span class="number">8</span>], <span class="string">&#x27;GPU&#x27;</span>) </span><br><span class="line">strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()</span><br></pre></td></tr></table></figure>
<p>更换了之后，果然<strong>Host Compute Time</strong> 得到了下降．</p>
<p>关于cast这个操作，是因为我在dataset的parse_function里面用到了tf.cast的操作，我把这个操作放到了网络里面，这部分的耗时也消除了（其实是分给worker了）．</p>
<p>关于<strong>Device Compute Time</strong>这一部分，我发现这个部分主要的耗时用到了矩阵操作Einsum上，这一部分的操作也是MultiHeadAttention的主要操作，于是也就没有修改这个部分．</p>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>当然，说了这么多，不如大家来下面一行代码来的方便…..</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch as tf</span><br></pre></td></tr></table></figure>
<p>不过话说回来，tensorflow和pytorch不是二选一的问题，而是大家最好都会用，这样才能更好在深度学习里探(lian)索(dan)．</p>
<p>最后把示例代码放到<a target="_blank" rel="noopener" href="https://github.com/Zhengtq/tensorflow2.x-best-practice">这里</a>，大家自行取用．</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>&lt; Tensorflow &gt;Tensorflow2.4 最佳实践</p><p><a href="http://yoursite.com/2021/01/27/tf2-x-best-practice/">http://yoursite.com/2021/01/27/tf2-x-best-practice/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Billy</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-01-27</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-03-11</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Tensorflow/">Tensorflow</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/12/28/ncnn-lesson-start/"><span class="level-item">&lt; NCNN-Lession-Start &gt;　Start</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Billy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Billy</p><p class="is-size-6 is-block">To Make It Simple</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>HangZhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">45</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Zhengtq" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/Zhengtq" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DEMO/"><span class="level-start"><span class="level-item">DEMO</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Deeplearning/"><span class="level-start"><span class="level-item">Deeplearning</span></span><span class="level-end"><span class="level-item tag">31</span></span></a></li><li><a class="level is-mobile" href="/categories/Deeplearning%E5%BC%80%E5%A7%8B/"><span class="level-start"><span class="level-item">Deeplearning开始</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Other/"><span class="level-start"><span class="level-item">Other</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tensorflow/"><span class="level-start"><span class="level-item">Tensorflow</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/python/"><span class="level-start"><span class="level-item">python</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-27T00:38:00.000Z">2021-01-27</time></p><p class="title"><a href="/2021/01/27/tf2-x-best-practice/">&lt; Tensorflow &gt;Tensorflow2.4 最佳实践</a></p><p class="categories"><a href="/categories/Tensorflow/">Tensorflow</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-28T05:54:05.000Z">2020-12-28</time></p><p class="title"><a href="/2020/12/28/ncnn-lesson-start/">&lt; NCNN-Lession-Start &gt;　Start</a></p><p class="categories"><a href="/categories/Deeplearning/">Deeplearning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-21T03:10:11.000Z">2020-12-21</time></p><p class="title"><a href="/2020/12/21/ncnn-lesson-10/">&lt; NCNN-Lession-10 &gt;　Forward Net</a></p><p class="categories"><a href="/categories/Deeplearning/">Deeplearning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-21T03:10:11.000Z">2020-12-21</time></p><p class="title"><a href="/2020/12/21/ncnn-lesson-9/">&lt; NCNN-Lession-9 &gt;　Load Image</a></p><p class="categories"><a href="/categories/Deeplearning/">Deeplearning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-18T02:19:11.000Z">2020-12-18</time></p><p class="title"><a href="/2020/12/18/ncnn-lesson-8/">&lt; NCNN-Lession-8 &gt;　读取网络的权重信息</a></p><p class="categories"><a href="/categories/Deeplearning/">Deeplearning</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">十一月 2018</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/06/"><span class="level-start"><span class="level-item">六月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Antispoofing/"><span class="tag">Antispoofing</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-C/"><span class="tag">C/C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DEMO/"><span class="tag">DEMO</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deeplearning/"><span class="tag">Deeplearning</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NCNN/"><span class="tag">NCNN</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Other/"><span class="tag">Other</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Billy&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Billy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Zhengtq"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>